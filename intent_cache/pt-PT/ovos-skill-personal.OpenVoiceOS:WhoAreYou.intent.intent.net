FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -2.74491985269422422711e-01) (1, -1.03879602693079560460e-02) (2, -4.40447809241770252275e-02) (3, -9.57751976393221571282e-02) (4, -6.32710488580225383082e-02) (5, 2.51785953843480125869e+00) (6, -6.46346417107630244470e-02) (7, 2.43075337082537062949e-01) (8, 5.57707215500035324851e-01) (9, 2.74432247024944842018e-01) (10, -2.37296944288262423584e-01) (0, 9.12849697644517288353e-01) (1, -1.81080786581526605172e-02) (2, -3.73586615617762310415e-02) (3, 1.34842159156312087903e-02) (4, -6.18812976475248965880e-02) (5, -3.03906347853438418838e+00) (6, 6.82189168019056046965e-01) (7, 6.51886477620981064085e-01) (8, -3.08765816366769030310e-01) (9, 3.94715291100705134575e-01) (10, 2.46557006202954814844e-01) (0, 4.26352314227860684781e-01) (1, 3.21796975644580762754e-01) (2, 2.07776128860465869597e-01) (3, 2.13926367076389178923e-01) (4, 3.46912331493846815000e-01) (5, 9.66838847509343146669e+00) (6, 9.33386327461240422609e-02) (7, -9.56569847198861511473e-02) (8, 2.40249695365201354491e+00) (9, 1.64851174821371682100e-01) (10, 7.55076560446018008044e-02) (0, 3.51337606992876083645e-01) (1, -9.62641550298224124571e-02) (2, -2.81242368813048176923e-02) (3, 1.58672468666543123317e-02) (4, 4.77624373440255350909e-02) (5, -1.21009479791617504674e+01) (6, 5.37343053157363970129e-01) (7, 5.93914051553194211053e-01) (8, -6.64132387251698541597e-01) (9, 3.71002976282877039438e-01) (10, 2.78943100534696175608e-01) (0, 3.32574663888129395950e-01) (1, -6.12231804128180179259e-02) (2, 4.17838381533135599932e-02) (3, -5.73211660678873760610e-02) (4, 2.20025466684807893825e-02) (5, -1.20942610905265919286e+01) (6, 7.12893893952404100745e-01) (7, 1.06295349321080512262e+00) (8, -3.70865680047724022295e-01) (9, 4.17001489981455142342e-01) (10, 2.49917694889325719965e-01) (0, -6.57824351010306340726e-01) (1, -7.25416696472861999290e-02) (2, -2.36207113306114935281e-01) (3, -8.05585241718986266868e-02) (4, -1.06378332416603840760e-01) (5, 1.59539167440752316374e+00) (6, -1.30736431272693548467e-01) (7, 3.33746015210983071331e-01) (8, -1.64028674060110568611e-01) (9, -5.92868656119727277520e-01) (10, -2.29426327587942208153e-01) (0, 3.97427714937966580777e-01) (1, 2.64667689872203348855e-01) (2, 2.89332076979098740122e-01) (3, 1.63830101561961677392e-01) (4, 2.81020447683749619028e-01) (5, 9.51315635851342200624e+00) (6, -4.98327149315836437360e-02) (7, -1.12536289963378277612e-01) (8, 2.93664886459142460140e+00) (9, 1.12948254813665979746e-01) (10, 9.81888476082080630114e-02) (0, 1.15971605239871844439e+00) (1, -1.71073988092374298930e-02) (2, -2.75637894881200287700e-02) (3, 9.05722811805773075911e-02) (4, -2.52037957323026154399e-02) (5, -2.16204643129478579766e+00) (6, 7.81792701763868058684e-01) (7, 5.08873462231538509215e-01) (8, -3.55793896210301191507e-02) (9, 5.64227692887025455981e-01) (10, 2.07710453830976110590e-01) (0, -2.80803379543657305462e-01) (1, -3.33409788154123767900e-02) (2, -1.87957914613245402613e-02) (3, -1.35470088019227830056e-01) (4, -9.62269039891718580559e-02) (5, 2.17263364846921103180e+00) (6, 8.96269164780235938483e-02) (7, 3.61932985306175680762e-01) (8, 5.39187650036969445821e-01) (9, 2.97773968233456143295e-01) (10, -1.36395683316239385574e-01) (0, -3.25387457461570606121e-01) (1, -3.57985081218241199541e-02) (2, -8.50217001579759990015e-02) (3, -1.11789430870866651757e-01) (4, -1.28065230391977823848e-02) (5, 2.17122945244526999176e+00) (6, -4.55465607544326342437e-02) (7, 2.27765351355465300864e-01) (8, 6.37646133852162400046e-01) (9, 2.73654456986774929916e-01) (10, -1.61339727965840395996e-01) (11, 5.10970235714517495573e-01) (12, -7.80117002928731595457e-02) (13, 4.55410847187858958129e-01) (14, -1.26843503711259686373e-01) (15, -1.12855714550177232969e-01) (16, 3.78269298026924161160e-01) (17, 4.30264750243049998168e-01) (18, -1.11704851462634466297e-01) (19, 3.87235744424631433969e-01) (20, 4.21722219941488785100e-01) (21, 2.09763221057407245329e-01) 
