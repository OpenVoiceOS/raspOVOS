FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=19 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (19, 6, 5.00000000000000000000e-01) (19, 6, 5.00000000000000000000e-01) (19, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.42861967436649778262e-01) (1, 1.38793828239562913041e+01) (2, -2.63575830017138823536e+00) (3, -2.05416535372998865228e+00) (4, -8.70815986727287660862e-01) (5, -2.48391557990469719064e-01) (6, 6.91275507285282042602e+00) (7, 8.96152324221402230009e-01) (8, 1.01591088906871256370e+01) (9, -2.45767603747810969850e+00) (10, -3.58954848560433292803e+00) (11, -2.00595831528584067627e-01) (12, -8.47089982402550067953e-01) (13, -1.11693903041822739652e+00) (14, -1.67562526542897188842e-02) (15, -4.66906102204262402067e-02) (16, 2.36625593297344824495e-02) (17, -9.06438771161165723900e-02) (18, -3.50467332715726775927e-01) (0, -1.11587514992614611486e+00) (1, 1.20482860267320579517e+00) (2, 2.25750291570092942450e+00) (3, -4.58019065752536466696e-02) (4, -3.05393317348016668511e-01) (5, 1.23700343770979562241e-01) (6, 1.50165364330387873792e+00) (7, 1.72182039500477479521e+00) (8, -1.20943747221048079898e-02) (9, 2.30719259104445528763e+00) (10, 2.20786205333975260601e+00) (11, -9.77091563579652322424e-02) (12, -4.19267018028040949762e-01) (13, -3.30688668131604468492e-01) (14, 1.71302703315785370686e-01) (15, -7.85737882483351190999e-01) (16, 2.39678687949351487685e-01) (17, -2.00966147830141633746e-01) (18, -2.13809170501222212124e+00) (0, -1.21869290139337382861e+00) (1, 1.24660686165252632129e+00) (2, 2.44668098016645174653e+00) (3, 7.08692011341541389102e-02) (4, -3.58734393126024342369e-01) (5, -2.85217456984532199971e-02) (6, 1.15336662146557089592e+00) (7, 1.64209872068169238091e+00) (8, -1.53478655940156721860e-01) (9, 2.41846099219039034622e+00) (10, 2.61788546417409362377e+00) (11, -3.87974515568242306340e-01) (12, -2.52058920152922583124e-01) (13, -3.40804803224764585323e-01) (14, 3.42621033186486179112e-01) (15, -9.37158073413717485067e-01) (16, 2.43229947586230510481e-01) (17, -6.14509155353437780400e-02) (18, -2.08441597896895780551e+00) (19, 6.24568033326045712883e-01) (20, 6.48241899022646919093e+01) (21, 1.50183631806291572275e+02) (22, -8.50784281120854468661e-01) 
