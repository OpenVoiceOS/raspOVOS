FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.19532776867676071575e+00) (1, 3.88735790129193958808e-01) (2, 2.23139088924417222648e-01) (3, 3.89870513554105457832e-01) (4, 3.60792357798108753730e-01) (5, -2.37477944684963304667e+00) (6, -1.68998076289153731366e+00) (7, 1.37498720630283854938e+00) (8, 2.39141440508951070854e+01) (9, -4.04106315066475152431e-01) (10, 5.87092474737705982690e-02) (0, 7.26014031634597989395e-01) (1, 1.06003920952324159899e+00) (2, 1.13588848183397561620e+00) (3, 9.59744008579295093675e-01) (4, 9.62481143274348194261e-01) (5, -1.66814852388380052695e+00) (6, -5.06959899572316974314e+00) (7, 4.90191239726738114957e-01) (8, -7.83483448726868925149e+00) (9, -1.27887796665157682519e+00) (10, 7.13272247006014659121e-01) (0, 4.72784196603627526745e-01) (1, 8.68582232050309488436e-01) (2, 9.18215958527932252231e-01) (3, 9.86804058901677216831e-01) (4, 9.34994718186268891635e-01) (5, -4.10937285513760741651e+00) (6, -2.09913233746181537853e+00) (7, 4.69115421425976897307e-01) (8, -1.44394458350014964765e+00) (9, 5.32757668345835044832e-01) (10, 2.55661635012745369622e+00) (0, 1.96028397086165107055e+00) (1, 3.62401305490103631257e-01) (2, 1.98283514731493831773e-01) (3, 3.57053234034148125886e-01) (4, 3.08204783850279717683e-01) (5, -2.04337432503418536811e+00) (6, -1.71632584451890624067e+00) (7, 1.98458225605542248893e+00) (8, 2.38170800997008207389e+01) (9, -3.89958750297684175745e-01) (10, 1.75663417490202811155e-01) (0, 4.25362696516843163952e-01) (1, 9.95686909310231293979e-01) (2, 8.35435545198331186434e-01) (3, 8.26017147056470224520e-01) (4, 8.31583066038499185701e-01) (5, -4.11212431729914840162e+00) (6, -2.15874520767818545153e+00) (7, 4.61920522403397593170e-01) (8, -1.45601763840985576337e+00) (9, 4.16830493658449541261e-01) (10, 2.59593003900169794207e+00) (0, 2.63962842286831478322e-03) (1, -1.88948520389873586556e-02) (2, -5.05029679206688286719e-02) (3, -4.60377008584816338477e-02) (4, 4.71012248727004992355e-02) (5, -5.27622394346414722577e+01) (6, -5.08402261764639540331e-01) (7, 2.81881390110093020540e-02) (8, 1.16754234603399886794e+00) (9, -2.18640632212711918947e-02) (10, -9.27223915881950355722e-02) (0, 7.60498957296622313251e-01) (1, 6.43449965483841923586e-01) (2, 6.30157142497000721804e-01) (3, 7.47682131446299358402e-01) (4, 7.62908376372751995120e-01) (5, -6.07071261200846112871e+00) (6, -6.77694335649862544813e+00) (7, 5.57530388074723548542e-01) (8, -1.59551651210493039557e+00) (9, 2.03952855469179572534e-01) (10, 2.78251891100967041481e+00) (0, -1.40353766972368038113e-01) (1, -1.24704745351645301943e-01) (2, -3.08658450239627929479e-02) (3, 1.19644359356433586244e-02) (4, -1.18321542381617392237e-01) (5, -3.19638223585629210888e-01) (6, -4.33179343349540957320e+00) (7, -1.20262580339799196594e-01) (8, -2.65559783112226610591e-01) (9, -1.16856622471338336688e-02) (10, -1.18552389396679974665e-01) (0, -4.05787594621560721020e-01) (1, -1.09029073406835208537e-01) (2, -1.09610948850293737911e-01) (3, -1.86524541572529109490e-02) (4, 9.61336479626116909014e-03) (5, 1.38757907740002139896e+00) (6, 9.44991460328145960190e-01) (7, -1.97545720109803979447e-01) (8, -5.15283227886915710769e+02) (9, -2.25419080096069007757e-01) (10, -1.96935328422050359709e-01) (0, -4.14333239361570793147e-01) (1, -4.23658624199977296154e-01) (2, -5.61489589420905765849e-01) (3, -4.07584279684176820080e-01) (4, -4.40243795422664063732e-01) (5, 2.78717479441977289412e+00) (6, 2.31608616774096365276e+00) (7, -1.61412380744579225800e-01) (8, 1.67014715422451631710e+00) (9, 1.47921416076102052450e+00) (10, -3.44742560993174462780e-01) (11, 3.76637499068424086435e-01) (12, -4.40892636588866182468e-01) (13, -1.10073316753651034849e-01) (14, 3.32347440753623823984e-01) (15, -8.93679595597042381039e-02) (16, -1.67422342568011556230e-01) (17, -1.44249131078171249909e-01) (18, -9.34787107277345041823e-02) (19, -2.36497743291314260761e-01) (20, 6.13379279225614082449e-01) (21, 4.21829469985615157501e-01) 
