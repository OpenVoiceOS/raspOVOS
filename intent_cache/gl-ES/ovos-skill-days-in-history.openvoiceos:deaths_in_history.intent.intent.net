FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 9.77028230081153220565e-02) (1, -1.92102299465671894074e-01) (2, -8.94572971601642064066e-02) (3, -1.53449059857860864442e-01) (4, -8.23353318710483006448e-02) (5, -1.63510356616269691976e+00) (6, -3.96860892639879114796e+00) (7, -3.52443262045970873153e-01) (8, -9.83016126572072268530e-01) (9, -9.91401452756440482172e-02) (10, 4.40330177055249377682e-02) (0, 5.59307847577603434175e-01) (1, 3.87355067536994501776e-01) (2, 3.05160668448373306738e-01) (3, 3.32296044335528895086e-01) (4, 3.88900556371375605291e-01) (5, 3.31693264729773618082e-01) (6, -6.96653435556970812570e+00) (7, 5.90354429480037135036e-01) (8, 2.37190782726125259927e+00) (9, 9.86848826298143322333e+00) (10, -2.03996617698669141516e-01) (0, 6.10939558262146853451e-01) (1, 7.69303779980391899684e-01) (2, 8.66623405298442284206e-01) (3, 8.32699719747752586940e-01) (4, 8.60878292402476708034e-01) (5, 3.24898569316089680381e-01) (6, -7.37152613672851320104e+00) (7, 5.78304192169614483809e-01) (8, 3.66117374741916279390e+00) (9, 7.50017361111270997043e+00) (10, -2.69512051754486114952e-01) (0, 1.15201222508345191287e-01) (1, 3.17790248201056946264e-01) (2, 3.99363496110602900213e-01) (3, 4.45544772551223322576e-01) (4, 4.24928077266379766730e-01) (5, 5.60090101051661681808e-01) (6, -1.53251430772925001378e+00) (7, 4.45268686359148291309e-01) (8, 1.95392438296969922495e-01) (9, 1.49425820869666026169e+00) (10, 1.82769138365023509074e-01) (0, 9.69124501332097598727e-02) (1, 7.96978375762556062734e-02) (2, 1.43257095366570962047e-02) (3, 4.26855361908528851966e-02) (4, 6.38808659165952391623e-02) (5, 5.51753989148471069015e-01) (6, -3.97822738609429293888e-01) (7, 1.87484300624809530067e-01) (8, 2.44306391397395877840e-01) (9, 3.77923891918512278032e-01) (10, 1.36509622882792003296e-01) (0, 3.20529516373911360638e-01) (1, 3.87827985689803644842e-01) (2, 3.73758897886439844793e-01) (3, 3.23361866877242554175e-01) (4, 3.46961282894774958319e-01) (5, 1.60475656819321660373e-01) (6, -6.90683086419378167164e+00) (7, 3.11810639721122662138e-01) (8, 3.09307129208739650661e-01) (9, 9.76719317549492771491e+00) (10, 3.34505576554903899522e-02) (0, 7.06919731861361949621e-02) (1, 5.99911198032786405210e-02) (2, 9.99756962073733296270e-02) (3, 1.36487527192919511076e-01) (4, 6.43987268222262487782e-02) (5, 8.62981901186292077455e+00) (6, -1.28219135462915923007e+00) (7, 1.34002527486475758733e-01) (8, 1.77503023042826169053e-01) (9, 1.39175544323772049093e-01) (10, 2.72819746631966286721e-01) (0, -1.21554089295718825703e+00) (1, -2.18007398395466456265e-01) (2, -2.94416075198578541805e-01) (3, -3.81600884346890212306e-01) (4, -3.54963874070572504849e-01) (5, -5.03594003960413760801e-01) (6, 6.38389201163697350694e+00) (7, -5.81546082361826144336e-01) (8, -1.19432765245646921315e+00) (9, 8.82198683115355208884e-01) (10, -1.08420791448837552928e-01) (0, -6.36186611660555234593e-01) (1, -2.03522620348858485073e-01) (2, -3.58534684030937800259e-01) (3, -3.77731492369103083462e-01) (4, -2.32597408442425379604e-01) (5, -5.08453265930481013157e-01) (6, 6.53433321099370711948e+00) (7, -2.81116144520223876224e-01) (8, -1.38617738090839393017e+00) (9, 6.55338644031425388015e-01) (10, -2.17539117588241681656e-01) (0, 2.04037905791242590858e-01) (1, 1.01022155758219808752e-01) (2, 1.39495232876616526152e-01) (3, 1.87757672544795028990e-01) (4, 7.92968587840376798459e-02) (5, 4.51081536866277499342e-01) (6, -7.37599271012550783411e+00) (7, 1.06440470985315394081e-01) (8, 7.20390140358550978839e-01) (9, 4.32353640867290389682e-01) (10, 2.46410737773007953999e-01) (11, 1.25649311645049110986e-01) (12, 2.90605874921661677668e-01) (13, 2.65978278107553867127e-01) (14, 3.22611273198397663364e-01) (15, 2.56622486423031537228e-01) (16, 2.66393256815312062802e-01) (17, 1.10154282460221145112e-01) (18, 7.54078091227900704219e-01) (19, 6.52087882092160286973e-01) (20, -8.21388857893592394532e-02) (21, 1.79715266629080994143e-01) 
