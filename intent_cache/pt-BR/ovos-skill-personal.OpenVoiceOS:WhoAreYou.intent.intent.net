FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.96374309610588326791e+00) (1, 5.25848938143191224626e-01) (2, 5.49632949745593069046e-01) (3, 4.90967741227087917100e-01) (4, 5.11563463127550965837e-01) (5, 4.23081884301818078598e+00) (6, 2.44909467895220694933e-01) (7, -2.79014535934472007739e-01) (8, 7.19317475089931268606e-01) (9, -3.02816609918250490008e-01) (10, 3.05263122505696482545e-02) (0, 2.73458277550019879243e+00) (1, 5.57228726969192900143e-01) (2, 5.46226149127911009273e-01) (3, 6.14238021598289662961e-01) (4, 6.36377041862438375119e-01) (5, -4.72435624695124012362e+00) (6, 9.02715731481609495734e-02) (7, -2.44173131194343556727e-01) (8, -6.94244883757652697653e-01) (9, 3.93559508457544893201e-01) (10, 8.88643942181632451316e-01) (0, 2.04695413064224496225e+00) (1, 4.40748879676995108934e-01) (2, 5.33591879522738343766e-01) (3, 5.68199066436228750199e-01) (4, 5.65661309277949331253e-01) (5, 4.33022602416194146713e+00) (6, 1.64841542740057883654e-01) (7, -2.76545197457814140307e-01) (8, 5.51219323406124228626e-01) (9, -3.02624474345817018328e-01) (10, 1.24831276196912088761e-01) (0, 2.05301926087600961068e+00) (1, 4.88722531115946712266e-01) (2, 4.75316440677580720475e-01) (3, 4.54600324845251924089e-01) (4, 4.65667439615664369157e-01) (5, 4.25993766404307550033e+00) (6, -7.58288002377979220014e-02) (7, -2.16847182006382949160e-01) (8, 5.55908560868138890143e-01) (9, -3.93876562892569948016e-01) (10, 8.81850739903004554598e-02) (0, 1.31990354717524693839e+00) (1, 3.51224398342517951654e-01) (2, 3.93967481432108024286e-01) (3, 4.95035654035953620600e-01) (4, 4.55601235953716376947e-01) (5, -4.97125462208587443058e+00) (6, 2.88714191735394254401e-01) (7, -1.65860255536548178990e-01) (8, 5.82197674196208714292e-02) (9, 5.65921814851256588241e-01) (10, 2.49243397795070265044e-01) (0, 1.90478206973535790780e+00) (1, 5.44133899724421499222e-01) (2, 4.84766116356787624131e-01) (3, 4.05770322298941554795e-01) (4, 4.41010868167815039964e-01) (5, 4.27185668564952081283e+00) (6, -9.27569552129737795454e-02) (7, -2.41311621607088938601e-01) (8, 4.62132535990113779700e-01) (9, -4.63933932661189374347e-01) (10, 1.22938456196263590225e-01) (0, 3.70388099460198105817e+00) (1, 6.18769673333595449094e-01) (2, 5.55251599923799910030e-01) (3, 6.00214747415016236509e-01) (4, 6.81989183352421091122e-01) (5, -4.73735323345960956942e+00) (6, 1.73634431241106945709e-01) (7, -2.25861071851482381190e-01) (8, -1.88874820368861784381e+00) (9, 2.38910185951420606099e-01) (10, 2.70852381003759135059e-01) (0, 1.25130913284233358240e+00) (1, 4.21723786832203462183e-01) (2, 3.84682756008019044458e-01) (3, 3.99716477931847169458e-01) (4, 4.13569774569382264673e-01) (5, -5.01993614946204935734e+00) (6, 2.05777003288872301079e-01) (7, -2.19472204622737476765e-01) (8, 1.50091751722618516052e-01) (9, 5.40812583379240985337e-01) (10, 2.99016465031016975828e-01) (0, 3.60991843698813141827e+00) (1, 6.84319933579395578427e-01) (2, 5.37612748966644682369e-01) (3, 5.18171196506451048336e-01) (4, 5.30611289368103422603e-01) (5, -4.77945652401747089755e+00) (6, 1.97814865885329094208e-01) (7, -2.47296328898897160853e-01) (8, -1.89609811949109663409e+00) (9, 1.28374623728529729805e-01) (10, 3.47446048272512186816e-01) (0, -1.23691133692168930835e+00) (1, -2.37866476561597595119e-01) (2, -2.64323696877530767146e-01) (3, -3.85795332874826146785e-01) (4, -3.67990576114228917781e-01) (5, 2.34575346749235835730e+00) (6, -1.94319800539503650949e-01) (7, 9.02772587496485146019e-01) (8, 5.90654616832866130594e-01) (9, 7.74311479224503051988e-01) (10, 1.23294567041940833496e-01) (11, 3.07618647075711681982e-01) (12, -1.92450490458739187360e-01) (13, 3.92268738902627422949e-01) (14, 3.71524742997704937597e-01) (15, -4.95670933948211822617e-02) (16, 3.99826741970597698828e-01) (17, -3.47364024237011903562e-01) (18, -8.18195530422260453030e-02) (19, -3.33195076301907533445e-01) (20, 8.13258417838261582489e-01) (21, 2.13728271897173582872e-01) 
