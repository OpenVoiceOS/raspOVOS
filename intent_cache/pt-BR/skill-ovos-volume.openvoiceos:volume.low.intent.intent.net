FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.31436403885123596069e+01) (1, 1.19817159640762907480e-01) (2, 6.90199691235758167052e-02) (3, 1.38446129429314224168e-01) (4, 1.62084638643238632127e-01) (5, 1.17779767313407682239e-01) (6, -1.45537584265333541644e+00) (7, -6.08117858613929040956e+00) (8, 5.66205425977436060947e-01) (9, 4.49514589461799007886e-01) (10, 1.22757407647925198546e+00) (0, 8.00769143830801499728e-02) (1, -1.25014683586834006446e-02) (2, -3.03124277276275791437e-02) (3, 9.62182270007850420912e-02) (4, 1.16694985480857091331e-01) (5, 8.35270067363123480675e-01) (6, 1.31744174830693727696e-01) (7, 7.69530672739284438144e-01) (8, -6.54697703658260277138e+00) (9, 7.11966656766846339721e-01) (10, -1.45175707248330149168e-01) (0, 3.39355589302530180618e+00) (1, 3.22559740265505823231e-01) (2, 2.81029540916579334553e-01) (3, 2.26103234907286815281e-01) (4, 2.87443574389117328938e-01) (5, 1.53595446516928877401e+01) (6, -7.02360848772421664243e-01) (7, 1.03332346836141635293e+01) (8, -6.03167571530379298395e+00) (9, 9.87919342824822699356e+00) (10, 2.33770259976563532511e-01) (0, 1.26278262687828685129e-01) (1, 1.76562653499366672971e-02) (2, -1.75366638583419887087e-02) (3, 6.73713968711616290053e-02) (4, -3.77868650836227504275e-02) (5, 7.96215895235950044118e-01) (6, 5.11655198261923779834e-02) (7, 6.87777158733414051284e-01) (8, -7.91003531157295824983e+00) (9, 8.79575230799630292111e-01) (10, -4.43942441617250424524e-02) (0, -4.54525587868548963666e-01) (1, 3.15680267769622296958e-02) (2, -3.62382543724251229844e-02) (3, 7.66444268185424160134e-02) (4, -2.44841886203957040391e-02) (5, 1.32785582727056206132e-01) (6, -3.35732278883343893128e-02) (7, 7.26879482697923989809e-02) (8, 1.53484924266447797514e+00) (9, 1.26122220149265551470e-01) (10, -1.23102485336629416390e-01) (0, 4.58741344190553082538e-02) (1, -2.34723477048156825564e-02) (2, 1.03326140494895149513e-01) (3, -3.36575148982284702570e-02) (4, 1.05400516243529487892e-01) (5, 6.80925988683109939537e-01) (6, 1.27093248029312128056e-01) (7, 7.23923362615441878809e-01) (8, -6.61109720631159714088e+00) (9, 8.05695184193566338315e-01) (10, 4.29809273208379624687e-02) (0, 3.71360072027708612574e-02) (1, -1.67470327002285056250e-02) (2, -4.89331422728775042308e-02) (3, 1.19098796101165027572e-01) (4, 4.66220887022735577809e-02) (5, 1.99876835029542321287e+00) (6, 1.03473727890162431331e-01) (7, 5.73543725409375593216e-01) (8, -6.97147228194375578880e+00) (9, 6.30801664669177641898e-01) (10, -2.94515766483287477762e-01) (0, 2.13617097878726944771e+01) (1, 4.55398653475634074272e-01) (2, 4.35036090044371104302e-01) (3, 3.38289652792803263726e-01) (4, 3.51563271908155894341e-01) (5, -2.33030043679091836140e+00) (6, -2.70277317051031862327e-01) (7, -4.23161722825862884889e+00) (8, -7.77065619505729276995e-01) (9, -2.92474189522875627212e-01) (10, 7.03666088964605584977e-01) (0, -6.50592334271400618562e-01) (1, -6.20723707392105243652e-03) (2, -1.05496281811736883349e-01) (3, 6.75437878279459141817e-02) (4, -6.29195068569410320691e-02) (5, 8.35102738796288796008e-02) (6, -9.02044283486426456964e-02) (7, 1.42654011173196593187e-01) (8, 1.47373058775772824980e+00) (9, 6.07794362213481195734e-02) (10, -1.88587078820917908306e-01) (0, 2.14517172345789894905e+01) (1, 4.03714579540499718391e-01) (2, 4.28684738475092919074e-01) (3, 5.03300850290068102488e-01) (4, 3.55826643306025536262e-01) (5, -2.34636675703141595051e+00) (6, 1.90707921295622917501e-01) (7, -4.95062173158338403312e+00) (8, -1.20066907001792211318e+00) (9, 1.89926202291928802879e-01) (10, 1.06991952708214288492e+00) (11, -1.23663415117448480252e-01) (12, -1.24093053637477260254e-01) (13, 5.88302385152462470330e-01) (14, -1.50421833394500104042e-01) (15, 7.10381587614978160516e-01) (16, -1.43783567248317090126e-01) (17, -1.47308043454839665554e-01) (18, -1.35609857927658256083e-01) (19, 7.58790419469313692780e-01) (20, -1.43322102915146049051e-01) (21, 3.29066982649600991984e-01) 
