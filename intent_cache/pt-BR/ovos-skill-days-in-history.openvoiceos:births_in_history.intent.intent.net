FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.37165003478342184540e-01) (1, 5.13588828768264393787e-01) (2, 4.65867636527549144887e-01) (3, 3.72379028971921322011e-01) (4, 3.51930593873988506459e-01) (5, -1.33105385851945845488e+00) (6, -2.18601209152226205745e+00) (7, 6.77439855513832900336e-01) (8, 1.85641905705497545220e+00) (9, 2.07009571657360513797e+00) (10, 4.47098995075713823155e-01) (0, -4.55764064033882920768e-01) (1, 3.37489408307396154818e-01) (2, 2.97647953831278067049e-01) (3, 4.03202430211626161594e-01) (4, 4.72487904929243196506e-01) (5, 7.07205789821258257177e-01) (6, 1.04392612885015578783e+00) (7, 4.06227634167356410622e-01) (8, 2.99574923821788896561e+00) (9, -1.73134072775363700991e+00) (10, 2.15264480010469078097e-01) (0, 2.73302047908121237274e-01) (1, 4.70870641793738720082e-01) (2, 4.60640517797004100942e-01) (3, 4.29331382538806316518e-01) (4, 3.78951871569882747792e-01) (5, -1.33836898011054961088e+00) (6, -2.11667993891720485067e+00) (7, 4.42842301018464279405e-01) (8, 1.82616205553577226617e+00) (9, 1.61416053029519068751e+00) (10, 4.33647672162543962315e-01) (0, -1.62669908260279383327e+00) (1, -3.66723553545978464552e-01) (2, -4.09227007337120085584e-01) (3, -4.20303092606571226941e-01) (4, -3.02746744163540026928e-01) (5, 1.22896953026565447509e+00) (6, 5.25701771392313688125e+00) (7, 5.83228961179698343620e-02) (8, 1.94878045619880507777e-02) (9, 6.60014699974275087513e-01) (10, -6.77414428605656482318e-01) (0, -3.78183992048161332633e-01) (1, 3.04004893504701834139e-01) (2, 3.73100914858900178928e-01) (3, 3.90140660904966352085e-01) (4, 3.47099588119112234530e-01) (5, 7.19722362892738054541e-01) (6, 1.13280373598712080785e+00) (7, 4.16106988711912928469e-01) (8, 2.97605054386955369949e+00) (9, -1.63792735988616700027e+00) (10, 2.25297387105772378701e-01) (0, -3.82367969890492265250e-01) (1, 2.69526179872410898142e-01) (2, 3.33496440194504750743e-01) (3, 3.88389598570721639170e-01) (4, 4.02835141859906209483e-01) (5, 6.46454188917996219743e-01) (6, 9.93469265552774483119e-01) (7, 3.60863546621750574950e-01) (8, 3.29920713529994635849e+00) (9, -1.70963192855051548946e+00) (10, 4.32664088725446371875e-01) (0, -3.72982492907476370636e-01) (1, -2.57288696104249148267e-01) (2, -9.53729393422213245213e-02) (3, -1.00245782965382668195e-01) (4, -2.19773096823653257825e-01) (5, -2.22833189130747086892e+01) (6, 4.42518308122780990033e+00) (7, -9.83304225914522767038e-02) (8, 3.57313527926394036704e-01) (9, -2.90153520266465070332e-01) (10, -2.40685238097440752647e-02) (0, -1.67668627952509607937e+00) (1, -3.97409750620215884709e-01) (2, -5.03373297402948005619e-01) (3, -3.50193961063712810855e-01) (4, -4.65437448123782737675e-01) (5, 2.11758778593930818701e+00) (6, 5.50256907756108049057e+00) (7, 3.48913051475860158668e-01) (8, 1.94009719685521969668e-01) (9, 6.20941745978305648102e-01) (10, -1.73640120746185133704e+00) (0, -4.54317928691761796500e-01) (1, 3.24180770657914285593e-01) (2, 4.31601088486569417491e-01) (3, 2.79888961456673746042e-01) (4, 3.95635034881489766612e-01) (5, 5.09838445699274500633e-01) (6, 9.95382142516389900599e-01) (7, 4.11375878535731320795e-01) (8, 3.27713957727959304123e+00) (9, -1.86876854179932960776e+00) (10, 3.46832538518896338342e-01) (0, 3.24634148687343082607e-01) (1, 5.08453283048327930871e-01) (2, 4.91115111447032348657e-01) (3, 3.50539389587100402856e-01) (4, 4.07205160296615020776e-01) (5, -1.03935337660305937568e+00) (6, -1.09339292224174555734e+00) (7, 4.75143414027917099229e-01) (8, 1.78840657363698762872e+00) (9, 1.50578709959722556455e+00) (10, 4.70681398859247646094e-01) (11, 4.59996426924698220695e-01) (12, -2.14152376202908917291e-01) (13, 4.69896266683571206535e-01) (14, 6.24490885687558394679e-01) (15, -2.15044165996877117975e-01) (16, -2.20958459852242106169e-01) (17, -3.52777992531736528381e-01) (18, 7.48966550923016893826e-01) (19, -1.92343006877271155419e-01) (20, 4.37675158008568154777e-01) (21, 5.03756111258058525415e-01) 
