FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.08877139922865717736e+00) (1, 6.30092026565924889780e-01) (2, 6.39411235128299004771e-01) (3, 5.69865634565011269785e-01) (4, 6.89875492726222283579e-01) (5, 1.47168470290475816853e+00) (6, 1.19854828212105246621e-01) (7, 1.89419247634616567666e+01) (8, -5.95637475681812578188e+00) (9, 1.81360903982415710978e+00) (10, -3.02381390016667681020e-02) (0, 1.68481006429369306598e-01) (1, 1.56382490619593805814e-01) (2, 2.15472561046057942136e-01) (3, 3.08620755299502669278e-01) (4, 2.07133096381598713620e-01) (5, 8.99507707903765663104e-01) (6, 1.55087199114522289456e-01) (7, -4.49560783910877614922e-01) (8, -1.17154747528994374051e+01) (9, 5.32137260079517671052e-01) (10, 2.64109724031399817346e-01) (0, 2.15115739789494364587e+00) (1, 6.19820782814875848032e-01) (2, 6.68375844035998589732e-01) (3, 5.20318018172637231089e-01) (4, 7.00140276823893792368e-01) (5, 1.45076825586133928425e+00) (6, 1.38447178397022724328e-01) (7, 1.88816047064867689187e+01) (8, -5.06174489248314962708e+00) (9, 1.87526000146165405802e+00) (10, -1.24194276188236876757e-01) (0, -2.82981642818676482065e+00) (1, -5.23838364923442600940e-01) (2, -5.62321060741390099658e-01) (3, -6.96167239392246117724e-01) (4, -7.18218514168704857958e-01) (5, 8.11352822914237159502e-01) (6, -6.92063561631396512164e-02) (7, 2.00585170723163264128e+00) (8, 4.45406657419865137371e+00) (9, 9.84795990413984828615e-02) (10, 2.73171830902341587766e-02) (0, 6.61754069647185438896e+00) (1, 3.48534773155428689861e-01) (2, 4.92256778343893697247e-01) (3, 3.38234062358118869884e-01) (4, 3.64481533929087442303e-01) (5, -8.43632090686486091347e-02) (6, 5.56871138509284713258e-01) (7, -3.30779314807860425063e+00) (8, -4.12417114369827508824e+00) (9, 6.25685578874694836582e-02) (10, 7.48225032499567177524e-01) (0, 9.35388835909665816715e-01) (1, 1.07048005252230710171e-01) (2, 1.84635036080229714139e-01) (3, 5.30871305755262826165e-02) (4, 1.31260194270956948026e-01) (5, -1.57492180545424287352e+00) (6, -1.34760423332278528319e-01) (7, 5.96043640366264448893e-02) (8, -1.61774681348644655543e+00) (9, -2.79002617848506662135e-01) (10, 2.79132723051549958004e-01) (0, 2.30851471608111646905e-01) (1, 1.78075639398750623643e-01) (2, 1.42981761934217771470e-01) (3, 1.44363650919851621568e-01) (4, 1.54977967830356916368e-01) (5, -6.24520772701652182590e-01) (6, 1.28641783061428260915e-01) (7, -1.71795958022473715943e+00) (8, 1.29310731106753866726e+01) (9, -5.17276883169983242583e-02) (10, 2.69210251380670395616e-01) (0, 8.36322200024422035547e-01) (1, 1.15391770998381978930e-01) (2, 1.99741447965049168101e-01) (3, 1.58284733692550083628e-01) (4, 2.78009201086992438490e-02) (5, 4.09150392354899028469e-01) (6, 4.74973206372271472553e-01) (7, -1.08239509940323275217e-01) (8, -1.27390335142327515427e+01) (9, 1.11164537011933828303e+00) (10, 2.77674742502040050596e-01) (0, 8.55392219476196619121e-02) (1, 1.52876002581772169053e-01) (2, 2.09292916448768934190e-01) (3, 1.55836408825573285997e-01) (4, 1.46732790263351758897e-01) (5, -3.78140503513131054358e-01) (6, 1.84510341647072029225e-01) (7, -1.70893647263644599121e+00) (8, 1.06936543328439572775e+01) (9, -2.92530451929252314353e-03) (10, 1.42002099554998068021e-01) (0, 2.07695292320736735192e+00) (1, 6.44908347998515374400e-01) (2, 6.12657526408091790415e-01) (3, 5.56195964936867959238e-01) (4, 6.81697422538653619029e-01) (5, 1.34834088501506776936e+00) (6, 1.94849734995209272759e-01) (7, 1.88761055111792757089e+01) (8, -5.03974404323617086732e+00) (9, 1.82161561999812637858e+00) (10, -1.42333400104862839441e-01) (11, 3.18392643978884837264e-01) (12, -9.06238630404605921553e-02) (13, 3.36984539678385874861e-01) (14, 4.73935026299162176500e-01) (15, -3.66425729929901800030e-01) (16, -2.74147908935049311463e-01) (17, 1.61099094625914623924e-01) (18, -2.76033850451616047383e-01) (19, 1.62622865017855694481e-01) (20, 2.79375406256011149519e-01) (21, 1.93381573422371194626e-01) 
