FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.93533999919081084462e-02) (1, -1.49808213291096805481e-02) (2, -2.12744476174594890697e-03) (3, 4.58593234243464056843e-02) (4, -8.79853055057454452870e-02) (5, -5.53772660391130822077e-02) (6, -2.96735581808646897706e-01) (7, 2.15601183494991383016e-01) (8, 1.04617338184797736833e-01) (9, -2.74598468569431597430e-01) (10, 1.29161594246801555985e-02) (0, -4.35266597845541225453e-01) (1, -3.14799709696251150959e-01) (2, -4.23058962900120072437e-01) (3, -4.16448770541626267505e-01) (4, -3.43079938072639756275e-01) (5, 2.83102990514052577087e-01) (6, 4.17505467747195346817e+00) (7, 2.12720307925723939313e-01) (8, 5.19792267700980414880e-01) (9, 6.50149275562949657825e-01) (10, -2.03972648933257505455e-01) (0, 8.70011707741899509294e-01) (1, 3.65200331542218481484e-01) (2, 3.13208026919091497842e-01) (3, 2.89525067541325842324e-01) (4, 3.80931144687855993691e-01) (5, 3.42883550879838516767e-01) (6, -1.29442753492867157483e+00) (7, 4.31297367020693711570e-01) (8, 1.76419226926869121552e-02) (9, 1.07206715080206521407e+02) (10, 2.65110304824897335951e-01) (0, 8.29347776223106403215e-01) (1, 3.93877631160939489785e-01) (2, 3.03642040226186071816e-01) (3, 2.75444241348231533273e-01) (4, 2.67475428167308071359e-01) (5, 3.94537926969411423261e-01) (6, -1.28925526737248108411e+00) (7, 5.59049343272295606866e-01) (8, -7.96929427879743967900e-02) (9, 1.07349706403230555907e+02) (10, 3.13246121875831173842e-01) (0, 2.08922446738860845272e-01) (1, 6.06379048712418766698e-01) (2, 6.28501416928933354100e-01) (3, 5.36272520013020947971e-01) (4, 7.15387666411565037450e-01) (5, 7.84301635428316240572e-02) (6, 1.25142895467457959668e+00) (7, 3.14121408616074626607e-01) (8, 4.03684621768216678195e-01) (9, -1.59821548850240957584e+00) (10, 8.97850593068820090892e-01) (0, 1.21944718974455312832e+00) (1, 3.06064665742981356633e-01) (2, 3.12295414932835024846e-01) (3, 4.03198227711784762395e-01) (4, 4.14391547866928500188e-01) (5, 3.20668769350060611778e-01) (6, -8.83506088134561728964e+00) (7, 4.46862337692824296287e-01) (8, 4.03768031400746491011e-02) (9, 1.07332361302589305296e+02) (10, 1.87290200160851794120e-01) (0, -1.10431413620629331751e-01) (1, -6.85719240423562220732e-02) (2, -4.19551346417786769072e-02) (3, -6.82605419274689845244e-02) (4, 4.49272405389425991129e-02) (5, 1.11205003463681131870e-01) (6, -4.37895663269121215855e-01) (7, -4.98564865647508764779e-02) (8, 3.24452962975813274937e-01) (9, -3.77679457930198969606e-02) (10, 1.17214897710402596309e-01) (0, 8.43302468172530406143e-02) (1, 5.63112361613083534095e-01) (2, 7.18083350244331786172e-01) (3, 5.95513062808085136268e-01) (4, 6.86999796214867064492e-01) (5, 1.90817459355199975368e-01) (6, 2.58956966516747533191e+00) (7, 3.39041357732000614433e-01) (8, 5.41580870801858793584e-01) (9, -1.51392698825302352006e+00) (10, 8.10363588371497467477e-01) (0, 2.13484279104860820431e-02) (1, 5.91883370466397717991e-01) (2, 5.46125890500710919895e-01) (3, 5.77618514932559445896e-01) (4, 7.17638233490155430516e-01) (5, 7.72210721476919298434e-02) (6, 9.08074502708824171648e-01) (7, 2.95565171395310466451e-01) (8, 2.97692440183401518805e-01) (9, -1.51888934315147627885e+00) (10, 9.55402140953761191966e-01) (0, 1.58254676531932592098e-01) (1, 6.37221516854928227147e-01) (2, 6.43459358997510166844e-01) (3, 6.07755930907891483983e-01) (4, 7.15942004509137364110e-01) (5, 6.98408101304418965638e-02) (6, 1.23952510265458104577e+00) (7, 1.90162984526338757485e-01) (8, 2.86484393955707905555e-01) (9, -1.58871502221527349796e+00) (10, 8.12722695508223846872e-01) (11, -3.22840921157375615458e-01) (12, 1.60715673101217437768e+00) (13, 4.30767574477172565661e-01) (14, 3.88156935212892129083e-01) (15, -2.05106849623011705530e-01) (16, 7.97343910658001808756e-01) (17, -8.38256641523579371089e-01) (18, -1.16157951605605269663e-01) (19, -1.32316264164733077280e-01) (20, -1.07812645685958038988e-01) (21, 2.61302589176642230395e-01) 
