FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.57910298892050704467e+00) (1, 3.81254928402080750516e-01) (2, 4.31850945792570384274e-01) (3, 3.56045460544004654935e-01) (4, 4.20143021486177659085e-01) (5, -5.98462356822986674132e+00) (6, 4.99188393093365689879e+01) (7, 1.68077875664692015789e+01) (8, 8.51302837285152658175e-01) (9, 1.13333197742442409739e-01) (10, -3.22379723563243208995e-02) (0, 6.27298291027498855499e+00) (1, 5.02240514026482398613e-01) (2, 5.12725202010472114189e-01) (3, 4.51558602617581128502e-01) (4, 5.71689454482395942314e-01) (5, -4.07790070747018673636e-01) (6, -3.81112821904043741483e+00) (7, -2.67743611432830785901e+00) (8, 6.78207657954080889162e-01) (9, 4.20647829169209119815e-01) (10, 4.72190484436970647764e-01) (0, 2.17698400778844419756e-01) (1, -1.42466472695371125512e-02) (2, -3.87145439396401752630e-02) (3, -5.06577426801225194630e-02) (4, -2.05033989856740449242e-02) (5, 2.24759728828403226863e-01) (6, -1.84301570773944511927e+00) (7, -1.65995003818048014743e+00) (8, 5.94101787334387898554e-02) (9, 2.08426907752202600355e-01) (10, 1.84427016980859947992e-01) (0, -3.81016514892118018931e-01) (1, 5.18504044031307190221e-02) (2, -6.72741608703926602386e-02) (3, 6.16183837865993122573e-02) (4, 8.11155250286265672166e-02) (5, 3.38956274225607216977e+01) (6, -2.18845314448733313517e-01) (7, -6.62540723581288570898e-02) (8, -1.15385819837504344232e-01) (9, 5.45455291933473820909e-02) (10, -3.67000706078191712978e-02) (0, 9.45324974436141940437e-01) (1, 7.94355098800055524355e-03) (2, 1.51661957546037368116e-01) (3, 3.12724237040869776708e-03) (4, 1.25449712320130968290e-01) (5, -4.69361772328318149761e+00) (6, -1.35151165993376753960e+00) (7, -8.76723525261230873795e-01) (8, 1.39618373328231099029e-01) (9, -9.77113740614990455846e-02) (10, 8.95482276055554332617e-02) (0, 2.28047419237542847714e-01) (1, 6.02623211552847093397e-02) (2, 2.82949941565740596683e-02) (3, 1.18358081798766889836e-01) (4, 6.50889264752614882248e-02) (5, -4.59988575550423028915e+00) (6, -5.59832566969433287341e-01) (7, 8.65126914767327725997e-01) (8, 3.16495135048629416818e-01) (9, 7.46136895506399611122e-01) (10, -8.42526052051880032512e-03) (0, 1.68765911771133009234e-01) (1, 6.99921687423330496358e-02) (2, -2.84526358068841814242e-02) (3, -2.50604758680719255648e-02) (4, -2.05355818093675389358e-02) (5, -4.63611095520363214462e+00) (6, -7.88293495143566058658e-02) (7, 9.12913881825681561288e-01) (8, 2.51394778976638422208e-01) (9, 3.51065998173998972121e-01) (10, 4.57893141334956149668e-02) (0, -4.00111131066815894908e-01) (1, -3.00231540585353985640e-02) (2, 4.25357928489849129727e-02) (3, 5.36805054163143127721e-02) (4, 1.00810041641399231876e-02) (5, 3.40196812360778153561e+01) (6, -3.10968927210230727809e-01) (7, -1.77119732369897198154e-01) (8, -2.08029045314130356381e-02) (9, -4.58034053378644848142e-02) (10, 7.74821706531862025580e-02) (0, 9.77046705779010760118e-01) (1, 1.27650765547163841385e-01) (2, 1.12500360080607333946e-01) (3, 1.19238531062014499473e-01) (4, 1.31239896485217000244e-01) (5, -4.91980565447155804293e+00) (6, 9.70212978304013162756e+00) (7, 2.93218074824851626659e+00) (8, 2.26059143492467362613e-01) (9, 1.09062195284905660841e+00) (10, 7.62739452592103295281e-01) (0, 2.44764113858345737640e+01) (1, 3.20959995479992932221e-01) (2, 4.41168802769593304536e-01) (3, 3.85457287521771496674e-01) (4, 4.29104941876343792817e-01) (5, -3.72379068984885486238e-01) (6, -7.15433513275187848990e+00) (7, -1.29378806286709635565e+00) (8, 6.91153737594448092096e-01) (9, -2.94995130133392902128e-01) (10, 2.01657186367125351723e+00) (11, 3.58937362665323111965e-01) (12, -2.93526320392068669485e-02) (13, -1.51185674866374741843e-01) (14, 5.79299765477508499245e-01) (15, 2.00340189976646876335e-02) (16, -1.44933865685062779338e-01) (17, -8.41238013957207458882e-02) (18, 5.15979246341781383478e-01) (19, 2.06455713701987869646e-01) (20, -8.07789938967052090213e-01) (21, 4.25790603470945383524e-01) 
