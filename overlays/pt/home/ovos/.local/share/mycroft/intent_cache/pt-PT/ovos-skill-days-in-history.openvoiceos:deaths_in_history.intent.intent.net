FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.73884640178259086696e-01) (1, 1.86233018591361321192e-01) (2, 1.63483110322909630518e-01) (3, 2.89447103693442731043e-01) (4, 2.13136487706857002955e-01) (5, 7.84363332737793572669e-01) (6, -6.94526296687302391319e+00) (7, 1.12263644997937173109e+00) (8, 9.93650035038790013786e-01) (9, -6.93961509587934233245e-01) (10, 3.89056843466598967218e-01) (0, -5.56587618533220029882e-01) (1, -1.24853518506131411225e-02) (2, 4.02795556286730457862e-02) (3, 1.01232263544954875778e-02) (4, 6.33613202074923137497e-02) (5, -1.13988416916251344979e+00) (6, 3.80860251331482757564e+00) (7, -3.59898833288181674295e-01) (8, -1.90351933678657325721e+00) (9, 2.37823365578560608213e+00) (10, -9.93699657379569556426e-01) (0, 2.97707663806804967432e-01) (1, 3.02508300773129679140e-01) (2, 3.51347497335896707948e-01) (3, 3.49961242310033060487e-01) (4, 3.45867476097569681581e-01) (5, 6.56989287861944326607e-01) (6, -6.41913824989539172350e+00) (7, 9.02441517011935423476e-01) (8, 4.99547850634877443898e-01) (9, -1.05785130371602664745e+00) (10, 5.01288309078930072182e-01) (0, 1.15234757913631913340e-01) (1, 7.89766117468304607030e-02) (2, 2.16921541048569171961e-01) (3, 2.22921553207916706141e-01) (4, 1.79384368611854999598e-01) (5, 1.02727604159699903441e+00) (6, -4.12419099320205173598e+00) (7, 1.85559965780073032882e+00) (8, 1.01255426538490778121e+00) (9, 4.10676052244183242834e+00) (10, 1.05126620346673735340e-01) (0, 1.05605684320464421333e+00) (1, 2.14758964580041067371e-01) (2, 2.71618480187397870473e-01) (3, 3.57287736934166821889e-01) (4, 2.66368562680226295125e-01) (5, 1.92925982134742080021e+00) (6, -5.62502782337927165202e+00) (7, 1.46455855812901813984e+00) (8, 1.13559407434140635651e+00) (9, 3.12253322657858145561e+00) (10, 5.55173430209396467916e-01) (0, 5.84806669139501411614e-01) (1, 2.22031437966914130255e-01) (2, 2.08211233083100272223e-01) (3, 2.63293297205061782584e-01) (4, 2.03779787931532813117e-01) (5, 3.84855624771076210155e-01) (6, -5.26574195474003126094e+00) (7, 4.71587344833731325622e-01) (8, 3.67532800971863193862e-01) (9, 5.84973120624545828150e+00) (10, 2.09389466657053407994e-01) (0, 4.23538583898183607079e-01) (1, 1.82525248441786719367e-01) (2, 2.80994274828524515097e-01) (3, 3.13411929819674528463e-01) (4, 2.71351919684500564323e-01) (5, 4.12685279220249889853e-01) (6, -4.12165037027797520608e+00) (7, 3.48381787368177930908e-01) (8, 4.96454863438087423688e-01) (9, 5.33647001326963721368e+00) (10, 8.38120429355015128525e-02) (0, 5.50469794452138505569e-01) (1, 3.51416947203722018767e-01) (2, 4.51991801965083928039e-01) (3, 3.07506067561950702238e-01) (4, 4.08006882178630736480e-01) (5, -5.82852480464690209105e-02) (6, -6.02060991501378328650e+00) (7, 7.83098305887515944157e-01) (8, 4.09642888909099722738e-01) (9, -8.91241957108543481070e-01) (10, 5.45421860497711463722e-01) (0, 1.04749385317050580646e+00) (1, 4.48751444976860147662e-01) (2, 4.42534330349498850055e-01) (3, 5.03105710309081910481e-01) (4, 6.21062773328357575764e-01) (5, 2.23542438439707513709e+00) (6, -5.37373970887892937753e+00) (7, 2.26093960922491277188e+00) (8, 1.23332220096868527826e+00) (9, 3.41919339742073047361e+00) (10, -3.23461359681210036676e-01) (0, 7.51950310829647694533e-01) (1, 5.44693264227443574299e-01) (2, 6.09523418407970307697e-01) (3, 4.46312512319618326373e-01) (4, 4.82268209617668253131e-01) (5, 2.07920790455918069739e+00) (6, -5.42958482048743462656e+00) (7, 1.18912579272293905674e+00) (8, 1.15152829666418088372e+00) (9, 3.29114020844479071570e+00) (10, -2.16174433322271819513e-01) (11, -7.31249838858035294820e-01) (12, 1.96181927603934491700e-01) (13, -2.86899495893510769751e-01) (14, 5.87356027165876981577e-01) (15, 1.06726629322008603751e-01) (16, 1.13330265498297680371e-01) (17, 1.39576395964758903645e-01) (18, -3.15691631310972165014e-01) (19, 1.27385654232769984784e-01) (20, 1.24724321744709987225e-01) (21, 3.29645346255065918584e-01) 
