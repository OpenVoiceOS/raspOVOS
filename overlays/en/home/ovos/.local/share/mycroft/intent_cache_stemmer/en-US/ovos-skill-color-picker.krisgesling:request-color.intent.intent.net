FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 5.77926403199651828047e-01) (1, 4.80091256144804412465e-01) (2, 4.88923785332006866078e-01) (3, 5.55000760647816226978e-01) (4, 5.82876623663467818837e-01) (5, 2.83158855213674932882e-02) (6, 1.11187703982531882474e+00) (7, 6.01821017273115543844e+00) (8, 3.33208345496595148560e+00) (9, 1.70928766073875260112e+00) (10, 5.90820972190988169537e-01) (0, -3.94332070535441892023e-01) (1, -1.08490602700771265887e-01) (2, -1.35287018148959964359e-01) (3, -5.75483903575552679932e-02) (4, -2.36832216668738243515e-02) (5, 5.02619079021381143235e-01) (6, 3.58920267236093992747e-01) (7, 5.63361337624769809018e-01) (8, 1.20204675930528370742e+00) (9, 2.86011610976873387191e-01) (10, -2.11716392152013488381e-01) (0, 1.97725330198108051327e+00) (1, 5.87537096787043910950e-01) (2, 7.16921644259043921998e-01) (3, 7.03334035325595130494e-01) (4, 7.17928396511622657350e-01) (5, -5.05756060406134660967e-02) (6, 5.08693182057889603520e-01) (7, 2.69757254023726134040e-01) (8, -5.12934007402211555871e+00) (9, -6.50323076100951247280e-02) (10, 7.64741068931834466760e-01) (0, -4.86381822593792645382e-01) (1, -2.29400536615440325816e-01) (2, -1.03606014151164937953e-01) (3, -1.78189723391124682506e-01) (4, -2.14969063747713046153e-01) (5, -8.37707274118762668103e-01) (6, 1.98250607248066479382e-01) (7, -4.91016135265397224430e-01) (8, 6.86292270265948101127e+00) (9, -1.14471115412147564361e-01) (10, -4.36872589477503292521e-01) (0, 1.21245561175491833872e+00) (1, 5.49175718393297040976e-01) (2, 6.05790191855401727317e-01) (3, 5.48627303507299268759e-01) (4, 6.15693309869737359641e-01) (5, 1.34534824978650302540e+00) (6, 2.81627910491839938967e-01) (7, 3.77330658003114660648e-01) (8, -4.84782387286201288390e+00) (9, -7.22391707261791637951e-02) (10, 6.08173582714567007024e-01) (0, -7.95561449721404301272e+00) (1, -5.95589149062727862827e-01) (2, -6.25922519033049407078e-01) (3, -6.28161209886168303562e-01) (4, -6.00410602306221785618e-01) (5, 2.65777954838112673297e-01) (6, 2.99017113030368597570e-01) (7, -3.40711848428175922976e-01) (8, 1.03504351929443032532e+01) (9, 3.56697240835731732567e-01) (10, -3.40609475822184371019e-01) (0, 3.62866500021244542395e-01) (1, -1.21043929891618454175e-01) (2, -2.04517798559697872030e-01) (3, -1.18559310274156295972e-01) (4, -2.05625930862458949910e-01) (5, 4.55609181847303867929e-01) (6, -2.24941705091540045114e-02) (7, 2.28817226619875468963e-01) (8, 2.02890598170958447177e-01) (9, -1.95859811260468391625e-01) (10, 3.70455942977759689061e-01) (0, 1.25141640387918973154e+00) (1, 5.89859017696351739524e-01) (2, 6.34400361742944451926e-01) (3, 5.39483697679014051474e-01) (4, 5.50710932877035097555e-01) (5, 1.99454042422891220099e-01) (6, 3.10018311177143401913e-01) (7, 4.03615570765472553294e-01) (8, -4.42628322525741779714e+00) (9, -1.43309610434698475601e-01) (10, 4.03700727241583401383e-01) (0, 3.78926541061613386674e-01) (1, -1.76257591318241174649e-01) (2, -2.07156102728000751645e-01) (3, -9.47344187608871596273e-02) (4, -1.67188744615665491056e-01) (5, 4.54798892432573209632e-01) (6, 6.46990312840660997118e-01) (7, 5.70495988347609950253e-01) (8, -7.86309473765576711912e+01) (9, 1.53696118107108370410e+00) (10, 4.18986023971537280275e-01) (0, -4.37386608194930304183e-01) (1, -1.90966793801848194745e-01) (2, -1.47468732204500935223e-01) (3, -1.11326643731657820569e-01) (4, -2.48564550187651472912e-01) (5, -3.43929786701635986645e-01) (6, -6.29258302130511371075e-01) (7, -4.87129893799590263370e-01) (8, 7.12922351713029467390e+00) (9, -1.41274339689547667742e-01) (10, -4.12558111628112067493e-01) (11, -1.89094898400222227730e-01) (12, 3.79425699127551308010e-01) (13, 4.22680985784630147695e-01) (14, 3.80156154049273720119e-01) (15, 4.15120062089065877675e-01) (16, 4.49082756354943823940e-01) (17, -2.45883004513386071999e-01) (18, 4.90514252936939454397e-01) (19, -1.57121873025448083316e-01) (20, 3.33117867124911537502e-01) (21, -1.32895908427080183811e-02) 
