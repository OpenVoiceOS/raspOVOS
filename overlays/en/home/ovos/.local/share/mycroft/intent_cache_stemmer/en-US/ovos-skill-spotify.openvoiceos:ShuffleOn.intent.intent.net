FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 8.01576352359554045179e-01) (1, 8.24903387371843099807e-01) (2, 8.44493757728879468338e-01) (3, 8.08558072690313101027e-01) (4, 9.31754957680051343338e-01) (5, 1.56644534599935036923e+00) (6, 6.50372527349213913617e-01) (7, 1.13078004711633650636e+00) (8, -5.61016663256356817868e+00) (9, 1.22063983983655877097e+00) (10, 7.46290279070581652476e-01) (0, -6.23772132096356068942e-01) (1, -3.17253111268065823847e-01) (2, -2.24764695728801117136e-01) (3, -2.81110784376643552118e-01) (4, -2.72605976427577278987e-01) (5, 7.71646039017975793328e-01) (6, 2.15420741737162213525e+00) (7, 2.01264347874144183814e-01) (8, 1.21618730157035015438e+00) (9, 1.06532432931041709523e+00) (10, -6.31522417014494186205e-01) (0, 7.29414651805735392287e-01) (1, 6.24570706544462894705e-01) (2, 5.71522416232172703054e-01) (3, 7.39452855346743209708e-01) (4, 7.42880658863131149161e-01) (5, 1.59242142272149544979e+00) (6, 7.81848580577995111440e-01) (7, 8.35754923359769752444e-01) (8, -4.52741967922130061908e+00) (9, 2.41085597373471438587e+00) (10, 7.93995359286197688320e-01) (0, 1.37205526318709214451e+00) (1, 3.42710211003204978653e-01) (2, 2.51083674902102149673e-01) (3, 3.26265907907864249893e-01) (4, 3.92903327072045005508e-01) (5, -2.37846466127514089006e-01) (6, -1.77185845529079322169e+00) (7, 7.83739645851719068936e-01) (8, 3.45894211342867130554e+00) (9, -1.54306167338963318159e+00) (10, 2.96489590433840266570e-01) (0, -1.58810825343503325691e-02) (1, -8.67594519754913140464e-02) (2, -1.42875317682888774984e-01) (3, -2.01019460460212691377e-02) (4, -1.79999714103012075327e-03) (5, 4.56369082734337572216e-01) (6, -5.43478892376980837753e+00) (7, 1.00320121199775469378e+00) (8, 1.03184801324367159125e+00) (9, 1.75714953404748241983e-01) (10, -1.26778640227913103056e-01) (0, 1.90245180005528657929e+00) (1, 3.21770771468063987442e-01) (2, 3.92653807127854026504e-01) (3, 3.55717985952278770156e-01) (4, 4.04578148448845542617e-01) (5, -2.32597486941407555205e-01) (6, -1.79879360800027732203e+00) (7, 8.24944679511654066495e-01) (8, 2.48906813187858233505e+00) (9, -4.03210447668068183713e-01) (10, 3.96120043126348908569e-01) (0, -1.60376672196055647390e+00) (1, -3.03170753543786475337e-01) (2, -2.19560829823426589824e-01) (3, -2.96727953379563758052e-01) (4, -2.66228197043351544337e-01) (5, 6.53195205518025789893e-01) (6, 2.13341692239319424829e+00) (7, 3.86947646587105120730e-01) (8, 8.43191488271527012088e-01) (9, 1.01327364044274692567e+00) (10, -6.45966827815428024095e-01) (0, -4.79968847031964629402e-02) (1, -4.48153677656978263744e-02) (2, -6.84155586436122620242e-02) (3, -8.44676259234279080967e-02) (4, -1.04835166342529623185e-01) (5, 1.94444811372936843252e-01) (6, -2.98976758406413578584e+00) (7, 3.55469339146219698300e-01) (8, 1.53307908174841145410e+00) (9, 7.80709120783527193987e-03) (10, -3.10912026342660269274e-02) (0, 1.76219130480983010578e+00) (1, 3.76449002945801414199e-01) (2, 3.20356986546894706436e-01) (3, 3.92431750023743308731e-01) (4, 3.86456399286171592422e-01) (5, -2.14539604155610436065e-01) (6, -2.33944469868570537052e-02) (7, 9.15946167780029574246e-01) (8, 2.64804517103692660385e+00) (9, -5.47559240399830171597e-01) (10, 2.63093535019924162910e-01) (0, -1.50996456417108815273e+00) (1, -2.65043778245858563380e-01) (2, -2.22632584994248733379e-01) (3, -2.60904086654595745998e-01) (4, -2.00874818389825210430e-01) (5, 7.19295013888270218239e-01) (6, 2.09159122556721310815e+00) (7, 4.90674778669090649874e-01) (8, 8.20176914143526891721e-01) (9, 1.22583334084265760389e+00) (10, -1.14906374354283014050e+00) (11, -2.88527698657079501032e-01) (12, 5.66866635043651867498e-01) (13, -1.94463310414231455026e-01) (14, 4.95643275878012656843e-01) (15, -1.63695680302134161099e-01) (16, 5.25742950937331254835e-01) (17, 4.94197434974050109346e-01) (18, -1.54016588370338158853e-01) (19, 4.33949492500205091794e-01) (20, 4.91389362838543064527e-01) (21, 3.79063046584184437648e-01) 
