FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 8.07806922112933173397e-02) (1, -3.67901880080509502413e-01) (2, -3.14404938156414404116e-01) (3, -3.24702698523807786746e-01) (4, -4.81686953241634796541e-01) (5, -6.21704400587518718480e-02) (6, 1.46999999880790710449e+03) (7, -5.98853972901553510688e-01) (8, -2.45381476574602563900e+00) (9, 1.20328008249668538809e+00) (10, -2.17047525919076028655e-01) (0, 2.41499164935148308286e-01) (1, 3.78172336208983328465e-01) (2, 3.28442422735854056004e-01) (3, 3.05376796114607718113e-01) (4, 2.36461399185820514424e-01) (5, 2.39108579576273849732e-01) (6, -4.47065710669458393767e+00) (7, 3.58057225322133660939e+02) (8, 4.68220431315660562177e+02) (9, 4.81549660838498496673e+01) (10, -6.99370441286746857834e-02) (0, 1.35069478989826787796e+01) (1, 9.97456689599471535423e-01) (2, 1.00108234328361089815e+00) (3, 1.18410947662921461898e+00) (4, 1.12845828873248588131e+00) (5, -8.39873068187982685373e-01) (6, 1.54292474177725336126e+00) (7, -1.20335690102856940342e+00) (8, 3.25574636199240430301e+00) (9, -3.86348934397999599355e+00) (10, 1.03232693185597512731e+00) (0, 3.08890058979152504204e-01) (1, 2.31001613041002702709e-02) (2, 3.85472563406068988168e-02) (3, 2.70609048505861850276e-02) (4, -5.03208364047217065335e-03) (5, -1.32618306249071810088e-01) (6, 1.46999999880790710449e+03) (7, 1.23126684690630330832e-01) (8, 8.70840954376494169598e-02) (9, 2.22089990889155430853e-01) (10, -2.10953152766475637847e-01) (0, 5.70957706411477916575e-01) (1, 3.54461279642205306928e-01) (2, 3.19510546457390853803e-01) (3, 2.11158853900055926145e-01) (4, 3.83253154289345809858e-01) (5, 1.41434506268499765591e-01) (6, -4.54841289987812924522e+00) (7, 6.25569892725957515722e+02) (8, 8.49688916754112888441e+00) (9, 4.76368918299118249138e+01) (10, 3.03372800337650405655e-02) (0, 2.99912373296280909685e+00) (1, 1.15077426427645557183e+00) (2, 1.24104225778622501153e+00) (3, 1.15801112124366634148e+00) (4, 1.16161684834999912042e+00) (5, -6.73254449926317249941e-02) (6, 1.82090822723283451978e+00) (7, -2.00279634364880165798e-01) (8, 4.17335989714911459458e+00) (9, -4.01554749811158817607e+00) (10, 9.07277693042012223223e-01) (0, 5.14437050233910952990e-01) (1, 8.48495767852595617065e-02) (2, 6.27623455068401364665e-02) (3, 8.89855654379657079467e-02) (4, 7.68298165342143346557e-02) (5, 1.39079999079704271026e+03) (6, -4.00340935763675975778e+00) (7, -2.79970154303240348348e-01) (8, 1.74986367099616429988e-01) (9, -4.04322730310890721128e-01) (10, 3.65098798524036288748e-02) (0, 6.01525248404535073377e+00) (1, 9.45117759502867915700e-01) (2, 9.47839754618148067067e-01) (3, 1.05356169769857332952e+00) (4, 1.06123665998075389361e+00) (5, -5.34393658004701926911e-02) (6, 1.82523744630276762990e+00) (7, -4.56623822950270041865e-01) (8, 4.18671387822678564561e+00) (9, -3.90625110142466347440e+00) (10, 8.83174856194706925372e-01) (0, 3.04091832153277010775e+01) (1, 5.64959759618502088685e-01) (2, 6.02407368625861083800e-01) (3, 6.79044659044008902526e-01) (4, 7.24307129885416456361e-01) (5, -4.73837664463492735933e+00) (6, 9.82985141893110481170e-01) (7, -4.13975764175416038881e+00) (8, 2.96678536787037128164e+00) (9, -3.50537457906029592536e+00) (10, 8.51818913060241711399e-01) (0, 3.77147085522129207891e-02) (1, -1.79751677841941071012e-01) (2, -2.80997185439864072265e-01) (3, -2.53076827556887540283e-01) (4, -2.27426423163214874723e-01) (5, -6.73935087899734386330e-02) (6, 1.46999999880790710449e+03) (7, -1.21334797638072489079e+00) (8, -1.51955174420996885765e+00) (9, 4.26274636701532871896e-01) (10, -1.72750986521362476234e-01) (11, 6.87494724203792428874e-01) (12, 5.85418634746162402926e-01) (13, -1.48093514171346279928e-01) (14, 2.98963095105324261347e-01) (15, 5.47317796084968444248e-01) (16, -1.56181512645467396316e-01) (17, 7.23576029944231202684e-02) (18, -1.32135724893792744217e-01) (19, -1.28977199479802739912e-01) (20, 4.12690443263547424024e-01) (21, 3.83098484173613529347e-01) 
