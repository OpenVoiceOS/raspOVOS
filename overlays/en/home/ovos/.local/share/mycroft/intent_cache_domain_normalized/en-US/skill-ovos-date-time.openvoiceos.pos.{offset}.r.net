FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=8 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 6, 5.00000000000000000000e-01) (8, 6, 5.00000000000000000000e-01) (8, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 7.27290130508957965993e-01) (1, -1.80680740080020463267e+00) (2, 1.21910341658589549407e-01) (3, 5.05387538219316589938e-01) (4, 5.45883924644037676899e-01) (5, -1.77200089595458543101e+00) (6, -1.78703960976741349498e+00) (7, 3.28910203691515412583e-01) (0, -8.84325621352251900475e-01) (1, 3.11905399445471775977e+00) (2, 6.14241978033985211449e-02) (3, -6.03933881877630418522e-01) (4, -6.57036091996250126712e-01) (5, 3.05880274359164250342e+00) (6, 3.22818004820523274390e+00) (7, -9.68258912470806865080e-01) (0, -9.83962867005787833463e-01) (1, 5.40548713765515209673e+00) (2, 4.24151935341817276637e-01) (3, -1.06030049656249913959e+00) (4, -1.97480508418890920996e-01) (5, 5.31736605874670864580e+00) (6, 5.29427466801775814531e+00) (7, -2.97757792919385266828e+00) (8, -2.07507758968278555045e+00) (9, 4.12014936612072801836e+00) (10, 6.51647042575285695420e+00) (11, -2.51627700534767839713e+00) 
