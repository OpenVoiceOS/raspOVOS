FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 8.03827591848679690045e-01) (1, 5.45399298236531659967e-01) (2, 6.37163085208100499202e-01) (3, 5.46402124033135816461e-01) (4, 7.15505001411599339534e-01) (5, 1.62331924682332512377e-01) (6, 4.42425914512770801501e-01) (7, -2.67937340597374262785e+00) (8, 1.05604302727332477474e+00) (9, 8.89875796798368146057e-01) (0, 2.98529657940990789111e-01) (1, 4.57632299593087599199e-01) (2, 4.12033144465085432451e-01) (3, 3.38096667161580488603e-01) (4, 4.76515617302056715410e-01) (5, -7.78593899149799817394e-01) (6, -2.65818736671114885795e-01) (7, 6.29989434283104721146e+00) (8, -2.70199082015658342826e-01) (9, 2.56726862225697127773e-01) (0, -5.39847259984809271782e-02) (1, -1.52348612772378710911e-01) (2, -1.58250262366685656712e-01) (3, -1.79545239882621554539e-01) (4, -1.24034085648212222264e-01) (5, 5.24473158350207468992e-01) (6, -1.31767955167396561000e-01) (7, -5.37795375390971663876e+00) (8, 5.72754393241313386476e-01) (9, -1.30948570613558273878e-01) (0, -5.00690361269894523844e-01) (1, -3.95560734079986944800e-02) (2, -9.72810335563331629594e-02) (3, -8.05828891800552671310e-02) (4, -3.25631819771438971167e-02) (5, 5.30803670948433614640e-01) (6, 2.87351524826870319274e-01) (7, 2.55637274486275423158e+00) (8, 2.68212432738573869351e-01) (9, -3.48083176154965479032e-01) (0, 4.01439758373898086807e-02) (1, -1.57882226036939410374e-01) (2, -5.03932066455820698581e-03) (3, -1.50793058203611163304e-01) (4, -1.14887037651691253926e-01) (5, 5.24659244051196238523e-01) (6, -1.24803249282939926479e-01) (7, -5.31417913093293670102e+00) (8, 8.27125532607014823938e-02) (9, -5.60378266142207495659e-02) (0, 5.82593419853797644947e-01) (1, 6.06263150737447076288e-01) (2, 6.30153154299420537043e-01) (3, 6.03279655144376092402e-01) (4, 6.30217437908810795832e-01) (5, 9.36318278454266628152e-01) (6, 7.76298822078605565977e-01) (7, -2.62381707886917414640e+00) (8, 6.26940291021470974542e-01) (9, 8.24917790654798199768e-01) (0, 2.28533092360622747119e-01) (1, 4.75485142400857374589e-01) (2, 3.29559575846787855546e-01) (3, 4.04202591708299086015e-01) (4, 3.76738783052560255449e-01) (5, -6.04292677183056348156e-01) (6, -2.77903012155199968802e-01) (7, 6.41221999253955665665e+00) (8, -2.85284779189730608451e-01) (9, 1.77725340637718914882e-01) (0, -3.10601634689751493479e-01) (1, -2.85867155034417230741e-01) (2, -2.17567779559964258329e-01) (3, -1.91963859338635522978e-01) (4, -2.33160682458752710478e-01) (5, 4.70387012329684162104e-01) (6, 3.83252783116684314635e-01) (7, 2.39072217596026170838e+00) (8, 2.42643709059984946652e-01) (9, -2.09724470045654365347e-01) (0, 2.56569064628012044604e-01) (1, 4.39229037693139479082e-01) (2, 3.59086278906461164873e-01) (3, 4.38426982691880628984e-01) (4, 3.03099449804421827714e-01) (5, -6.33231119651699536632e-01) (6, -3.34120518683100664603e-01) (7, 6.30611752595630470353e+00) (8, -3.46882193504477465140e-01) (9, 1.47316585692917556560e-01) (0, -3.60455868550721036936e-01) (1, -2.58693762559765894071e-01) (2, -3.13210182328099329130e-01) (3, -2.05820180911892969267e-01) (4, -3.49592641432160455839e-01) (5, 5.67769427415715011165e-01) (6, 3.73253552612648364928e-01) (7, 2.38190588158817995179e+00) (8, 2.27236124452383780881e-01) (9, -2.82560898926345893667e-01) (10, -2.59012788970345575468e-01) (11, 3.98788798680898115556e-01) (12, -1.81059652345332888768e-01) (13, 6.30853806898755253840e-01) (14, -1.33144349457920863067e-01) (15, -2.10720800299519617216e-01) (16, 4.41603329113122389238e-01) (17, 6.07068349883717828241e-01) (18, 4.81381829908486769121e-01) (19, 6.11417492196721368281e-01) (20, 3.59891470602151319902e-01) 
