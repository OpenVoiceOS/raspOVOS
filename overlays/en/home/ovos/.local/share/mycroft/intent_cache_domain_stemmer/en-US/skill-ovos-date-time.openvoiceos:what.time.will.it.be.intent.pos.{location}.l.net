FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=15 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (15, 6, 5.00000000000000000000e-01) (15, 6, 5.00000000000000000000e-01) (15, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -7.30866803052779943073e+01) (1, 2.26367529211893137031e+00) (2, 9.67851622346337059355e+00) (3, -3.00193175959188485180e+00) (4, 1.75673573731293473088e+01) (5, -8.24924393266300959304e-01) (6, 7.32607486081986863935e-02) (7, 1.29052518380187364322e+00) (8, -2.08954200689273594804e+00) (9, 2.06702096174284477570e+00) (10, 1.60988348165580923421e+01) (11, 1.62007686948385902781e+01) (12, -3.16039104539458826082e+00) (13, -6.12955877445237273804e+00) (14, 2.45203167680224787262e+00) (0, -7.76544617545329263208e+00) (1, 6.68748787321434945419e-01) (2, -7.79638296034184108407e-01) (3, -4.86855928226785639179e-01) (4, -1.23484104193236854385e+00) (5, -1.64775908634530310337e+00) (6, 1.32454958262705840832e-01) (7, 1.95489856255418231745e-01) (8, -9.72160003690247020991e-01) (9, -4.13142045762334431114e-01) (10, -1.22474058187791023933e+00) (11, -1.23012239432234338210e+00) (12, -2.08386629949004076323e+00) (13, 2.79296697369709034930e+00) (14, 1.18376273310539659178e+00) (0, -7.30165368307944362414e+01) (1, 7.79933111314925930913e+00) (2, 3.18941985202224032037e+00) (3, -1.67461560413507654088e-01) (4, 5.77260175245828310864e+00) (5, -6.44215251076033945665e-01) (6, 4.85172242498266470534e+00) (7, 1.28922683858326259099e+01) (8, 3.48756881204427600096e+01) (9, 3.54926954237366221179e+00) (10, 4.00090278845268088048e+00) (11, 4.52835876215377908238e+00) (12, 8.22375135881017849115e-01) (13, 3.24687544702976405020e+00) (14, 9.27978166811812998915e-01) (15, 4.86207393626047235102e+00) (16, 2.13814648282725308093e+01) (17, 4.39427644096303193777e+00) (18, -8.67830776571440032363e-01) 
