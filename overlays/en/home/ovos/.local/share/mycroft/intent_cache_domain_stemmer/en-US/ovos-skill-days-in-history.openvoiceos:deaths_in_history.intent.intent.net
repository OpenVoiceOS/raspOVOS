FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.88581723317480332014e+00) (1, 4.63156331911713015081e-01) (2, 4.80207212850481401922e-01) (3, 4.17064078733354937079e-01) (4, 4.29798611089617144110e-01) (5, 1.09455936331029524133e+00) (6, 5.07915137787126180768e+00) (7, -7.75738629869243245807e-01) (8, 7.95198166124628169626e+00) (9, -6.50026278430681880849e+00) (10, 6.77531781938545019095e-01) (0, 1.78279248251166033690e+00) (1, 3.20938176102393724065e-01) (2, 4.27146016247027859869e-01) (3, 3.67681017703765489202e-01) (4, 3.42995917982810594182e-01) (5, 2.96674420556446327701e-01) (6, 5.73973546398176193151e+00) (7, 8.05957819193956925830e-01) (8, 6.40399307450356136684e+00) (9, -4.91266494148530608044e+00) (10, 3.43423378407010371216e-01) (0, -9.20335901316428506291e-01) (1, -1.52020866765761653028e-01) (2, -2.51140127672934698744e-01) (3, -2.59176860346579829297e-01) (4, -3.13222202255511672497e-01) (5, 6.56032607507355675081e-02) (6, 8.63355556022357149004e+01) (7, -2.10415569861742435265e+00) (8, -1.04525975520105229855e+00) (9, 7.10131142761915440387e-02) (10, -2.13563915667687037603e-01) (0, 5.06788590994459053896e-01) (1, 5.03654976903135542798e-01) (2, 4.06748966841633363689e-01) (3, 5.25487413226301547375e-01) (4, 4.55544320462877905253e-01) (5, -3.81782132402033902441e-01) (6, -4.07265077653162066440e+00) (7, 3.66829854303238089841e+00) (8, 1.55746803257597776771e+00) (9, 9.15709851653164719210e+00) (10, -7.12029529495870807526e-03) (0, 3.82362142547941452619e+00) (1, 4.45497569593331588234e-01) (2, 4.08990868332526458229e-01) (3, 4.25400980713507903541e-01) (4, 4.66717054101369155372e-01) (5, 1.21263033431643330040e+00) (6, 5.11537022967599508405e+00) (7, -3.83947271066586104382e-01) (8, 8.22784076503404548930e+00) (9, -6.75442802138250275590e+00) (10, 7.24662636561610451480e-01) (0, 1.06156644750133777322e+00) (1, 1.60958203668572746858e-01) (2, 1.46848376090504967317e-01) (3, 1.24824348087289177522e-01) (4, 1.20369131858327260698e-01) (5, -1.45162962375832149675e+00) (6, -1.53635657359961319557e+00) (7, 7.24451765852647922017e-01) (8, 4.90993873277915882092e-01) (9, -1.01704264251096490775e-03) (10, 1.47378250997823811996e-01) (0, 1.05970388950184046806e+00) (1, 3.48543917687714810416e-01) (2, 3.60087664725363965079e-01) (3, 3.82643932404101605460e-01) (4, 4.44044524731219414360e-01) (5, 1.98487562558075397723e-01) (6, 5.80166027915967941198e+00) (7, 8.33220872977503468348e-01) (8, 6.65334293987604930010e+00) (9, -5.15551397721740212177e+00) (10, 3.18811597305158511695e-01) (0, -8.87914193209433877385e-01) (1, -5.18645784720534019918e-01) (2, -4.59595805868262097249e-01) (3, -3.94335574373358532796e-01) (4, -5.30537600531929776082e-01) (5, 9.74187588752594008090e-02) (6, 3.99655648512389589655e+01) (7, -1.13159038783589058208e-01) (8, -2.79484588154036872609e+00) (9, 1.16921024397206152479e+00) (10, -6.16774353589455356928e-02) (0, 4.32000911793943342953e-01) (1, -5.90920946408143021866e-02) (2, -4.53659682381978429655e-02) (3, -7.06990486789574879012e-02) (4, 8.36188742112287802799e-02) (5, -8.87627813067797949431e-02) (6, -6.42327787153908702500e-01) (7, -2.39753108257126264224e-02) (8, 1.57836458327444917327e-01) (9, -6.90708708786134384372e-02) (10, 7.96787305105166876462e-02) (0, 8.02158491005875551316e-01) (1, 1.17712831936858375315e+00) (2, 1.03320840813301240146e+00) (3, 1.09027068011544403525e+00) (4, 1.06027261607430633994e+00) (5, -5.13539655601812361496e-01) (6, -4.68794249454016220824e+00) (7, 4.90243194404481119619e+00) (8, 1.43255970609376426239e+00) (9, 1.21204359944994966014e+01) (10, -3.93527628170168286559e-01) (11, -1.16081072423701234242e-01) (12, -4.11226174793651380601e-02) (13, 5.55551064055977539269e-01) (14, 6.54987460747733174493e-01) (15, -1.68430155549292553951e-01) (16, -2.24513229491471594246e-02) (17, -3.77756676636150298676e-02) (18, 6.32654676006961902424e-01) (19, -2.60619692173051198392e-01) (20, 5.86033773288777593002e-01) (21, 2.97608538570415748215e-01) 
