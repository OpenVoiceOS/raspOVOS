FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.68568545359957799867e-01) (1, 2.49366665497928463591e-04) (2, -5.37583498114193591988e-02) (3, -1.24088717454517785677e-02) (4, -1.33259382640704027168e-01) (5, -9.39155167415451491708e-02) (6, 1.06133870157099199916e-01) (7, 3.58775832476210077004e+00) (8, -3.56432354723450126044e-02) (9, -1.11773501676928135873e-01) (0, 2.53399534913504830003e-02) (1, 2.44216223372324986451e-01) (2, 1.83430333746775697801e-01) (3, 2.13574682487353395555e-01) (4, 1.51035566939219545457e-01) (5, -7.99589652558005292704e-01) (6, -8.64227569436398335156e-01) (7, 8.92270288514904663657e+00) (8, 3.60665859334253352531e-03) (9, -1.20746419933830933385e-02) (0, 7.66415122167755757943e-01) (1, 5.21778700945066176153e-01) (2, 5.13892272559093199469e-01) (3, 5.81350160834477991401e-01) (4, 6.47406442162679285346e-01) (5, -7.31432777888837010183e-02) (6, 3.13523412288976399687e-01) (7, -5.03597300337600284337e+00) (8, 5.83447955528755279886e-01) (9, 5.66587649475383314801e-01) (0, 1.38970597264079981370e-01) (1, 1.22818558093814769738e-01) (2, 1.76141621439742344923e-02) (3, 4.21932363777445945674e-02) (4, 6.73906715481566581660e-02) (5, 5.85735889529208630933e-01) (6, 5.19763836169504500617e-02) (7, -1.97372047779557480141e+00) (8, 2.94568520649249043775e-01) (9, 7.08776825896017675932e-02) (0, -5.96832815836166208356e-01) (1, -1.45241755860417542934e-01) (2, -1.86994563656418949504e-01) (3, -1.02042812722294970462e-01) (4, -1.78565561639636188884e-01) (5, -6.14218914389887182814e-01) (6, -1.67197950478703283306e-01) (7, 2.43647251944135057045e+00) (8, 3.34683192731192041824e-02) (9, -2.09752221142666284148e-01) (0, 4.90499510261023785596e-02) (1, 1.80793156041010927293e-01) (2, 1.99806218160494875047e-01) (3, 2.45063086165293736451e-01) (4, 1.07420907480820726487e-01) (5, -7.19596821804678876688e-01) (6, -3.98800304824903473211e-01) (7, 8.95852718028239891623e+00) (8, 2.55237707649924733255e-02) (9, 8.22132154199088360791e-02) (0, 6.83783068152869488721e-02) (1, 1.10881057692870210740e-01) (2, 1.58295755399569582078e-01) (3, 2.39256446374758735551e-01) (4, 1.69494991315707277391e-01) (5, 3.89378160052030797278e-01) (6, -1.10601939183109357345e-01) (7, 9.01413407447747871970e+00) (8, -1.95342238420935010446e-01) (9, -6.47356362250840061678e-02) (0, -1.65538662452090368227e-01) (1, -1.02357384359225145332e-01) (2, -4.13923976772869600671e-02) (3, -1.19919982349261156074e-01) (4, 3.12209980732356534583e-02) (5, 1.26813735775678726458e-01) (6, 1.17401547106600237513e-01) (7, 3.65779939266037423806e+00) (8, -3.86543578182693900458e-02) (9, 1.86631116204848096496e-01) (0, -1.67140365747010105624e-02) (1, 1.72289415137456963789e-01) (2, 1.21703228788065040411e-01) (3, 2.56643210217228565084e-02) (4, 1.70454009310888360273e-01) (5, -1.66128754745049150854e-01) (6, 1.10138469222850268747e-02) (7, 9.06625999036245033835e+00) (8, -1.88700913811416959298e-01) (9, -9.62329666886679695181e-03) (0, 1.88534291546039872722e-01) (1, 4.96663694187930368495e-02) (2, 1.71428950588398326671e-01) (3, 9.99315549537471287112e-02) (4, 4.20243312522700571132e-02) (5, 2.52893375804853159927e-01) (6, 1.49008330378940456740e-01) (7, -7.16315093770294453179e+00) (8, 2.63295060950859582949e-01) (9, 1.88706747896170312639e-01) (10, 7.03322818924163373211e-01) (11, 2.64351587440686042196e-01) (12, -1.73757070083010778383e-01) (13, -7.77025439480071300480e-02) (14, 1.43046315877133317640e-01) (15, 2.40309144999699408896e-01) (16, 2.62633761813381050310e-01) (17, 6.32445929993365796307e-01) (18, 2.27688442613796948599e-01) (19, -1.07597429940960567785e-01) (20, 9.18067604801443143403e-02) 
