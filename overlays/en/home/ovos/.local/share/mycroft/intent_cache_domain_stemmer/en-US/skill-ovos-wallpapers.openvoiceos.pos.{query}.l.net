FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=13 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (13, 6, 5.00000000000000000000e-01) (13, 6, 5.00000000000000000000e-01) (13, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -9.35614251007905073365e-01) (1, 5.88351614763464603186e+00) (2, 2.09610857033780684233e-02) (3, -3.10041467632053202585e-01) (4, -5.24988961640600271252e-01) (5, 5.74340889593090686560e+00) (6, 1.54152972992663755969e-01) (7, -1.52842435027358836130e-01) (8, -3.27450591112373179392e-01) (9, -9.36188899388184614381e-01) (10, -1.08641088049637302149e+00) (11, -1.40876295949218577341e-01) (12, -1.06768110807763560999e+00) (0, 2.23872246320538814679e-01) (1, 4.08737140680244193103e-01) (2, 1.37971675152547879062e-01) (3, 1.34570301348932219732e-01) (4, -1.09630703678055274608e-02) (5, 3.32279498661449179675e-01) (6, 1.09426861758001370273e-01) (7, -1.67210783545684249507e-01) (8, -1.50319654230784804927e-01) (9, 1.24740252371125501241e-01) (10, -4.85234034053046020540e-02) (11, -1.80991891507815749751e-01) (12, 1.93544397230439424451e-01) (0, 1.55771785081747116131e+00) (1, -5.76799532761301669836e+00) (2, 2.87139229264548156983e-01) (3, 6.42384097353513205952e-01) (4, 9.84787220102850024972e-01) (5, -5.88657391806568774939e+00) (6, 2.77985613581230972535e-01) (7, 6.52663708106572593159e-01) (8, 6.14649210498626197285e-01) (9, 1.43525332064750732464e+00) (10, 1.54324304909828247112e+00) (11, 6.09701667354400123067e-01) (12, 1.61707041264894546551e+00) (13, 3.14660199883787683461e+00) (14, 1.18913592870526593770e-01) (15, -2.90425690427047422659e+00) (16, -9.89726372470726922792e-01) 
