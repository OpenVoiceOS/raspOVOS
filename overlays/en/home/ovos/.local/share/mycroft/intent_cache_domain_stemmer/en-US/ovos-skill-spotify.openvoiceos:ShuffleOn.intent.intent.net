FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 7.10665326322187929087e-02) (1, 5.81315761807949405587e-04) (2, -4.09313858985867126683e-02) (3, 1.23219202315810710813e-01) (4, 1.17048154628280193235e-01) (5, 2.18029970731671240980e-01) (6, -5.46018885328857228600e-01) (7, 2.78892080667568043051e-01) (8, -2.72904518394481432342e-01) (9, -1.10380847332356468726e+00) (10, 5.22475852821354183608e-03) (0, 1.08500559231149110162e-01) (1, -1.46262840867008928059e-02) (2, -8.44098343848845127091e-03) (3, 6.75297407865557974826e-02) (4, 1.21080289638045818235e-01) (5, 2.49814178823487526016e-01) (6, -3.92359776972149965069e-01) (7, 3.41198000493598774252e-01) (8, -2.89576397332838020304e-01) (9, -1.22864195284722343970e+00) (10, 1.38926522359657609457e-02) (0, 6.66016380990614720314e-02) (1, -5.04751996636356933812e-02) (2, 4.31636153221163984539e-02) (3, 6.44311635255847281467e-02) (4, 6.80257035851512259494e-02) (5, 2.46985363271172486677e-01) (6, -2.51445761500214748363e-01) (7, 3.31936749997688129721e-01) (8, -3.13597741870891344451e-01) (9, -1.23129505989430443336e+00) (10, 1.14108579002361334997e-01) (0, -3.91586004569239642947e-01) (1, 5.06186220066811368157e-02) (2, 1.40556725876595338398e-02) (3, -1.23868104515764548834e-01) (4, -1.16164368091318442877e-01) (5, 1.46959546772022497940e-01) (6, 1.36074295070905670890e+00) (7, -2.80247368910032934597e-01) (8, 1.79842529533520228968e-01) (9, 5.93052816993186238115e-01) (10, -2.21304701239416201641e-01) (0, -4.04682083799548175662e-01) (1, 1.67892479255169618790e-02) (2, 2.47051321818798769181e-02) (3, -7.70018383011847884489e-02) (4, 5.24347477509945550378e-02) (5, 1.37995400322333650056e-01) (6, 6.44891595988928645333e-01) (7, -1.58019481437905368049e-01) (8, 2.46867172911906462840e-01) (9, 9.94532349009467631085e-01) (10, -2.47685734115122679899e-01) (0, 1.08548609588180489105e+00) (1, 9.28128049604066474032e-01) (2, 9.83551264635690314364e-01) (3, 1.02692624264157639224e+00) (4, 9.04687584272988898348e-01) (5, 5.76045571972306769126e-01) (6, 1.90562077111429006848e+00) (7, 3.10266074788653689609e+00) (8, -7.56763455463295198200e+00) (9, 2.00567014376019825406e+00) (10, 9.93069033133519796230e-01) (0, 1.04695051873448052859e+00) (1, 9.58084856530283079934e-01) (2, 9.78048322161291228127e-01) (3, 1.10479584838971911509e+00) (4, 1.12134726848230203977e+00) (5, 7.43388604138544906164e-01) (6, 2.27554260926248552721e+00) (7, 2.70164083056545401007e+00) (8, -7.64014111313368182721e+00) (9, 2.14623463014459003873e+00) (10, 1.03113689120184170989e+00) (0, -4.12855907156176593631e-01) (1, 6.35154541013074891431e-02) (2, -4.73125321890581745227e-03) (3, 5.75477476594282374989e-02) (4, -1.85929690125154339908e-02) (5, 2.08135484832155021051e-01) (6, 6.87931732866489342726e-01) (7, -2.90885934817297964194e-01) (8, 2.30018374559465887952e-01) (9, 8.69147346867903958234e-01) (10, -3.25840186260222319792e-01) (0, -1.94031002309533157080e-01) (1, 1.03933225429061415479e-01) (2, 9.42762149930033710454e-02) (3, 2.06105694771149859992e-03) (4, -2.21335636019673147323e-02) (5, 2.89628980570271421513e-01) (6, -6.02105121810495802492e+00) (7, 7.01507925029239975956e-01) (8, -4.32264463570802892622e-01) (9, 4.00327956009253860259e-01) (10, -2.24447751511680088088e-02) (0, 1.27453244043957703013e+00) (1, 4.70153926769642294126e-01) (2, 3.24043235639480953214e-01) (3, 3.09179126182941854672e-01) (4, 4.97747673550991476255e-01) (5, 6.98245755482845842366e-02) (6, -6.42649278097679577115e+00) (7, 2.00818903852976831104e+01) (8, 4.93831439049775511307e+00) (9, -5.83686167189810500844e+00) (10, -1.57647171968699262301e-01) (11, -1.82585898765269677435e-01) (12, -2.19255454816762423986e-01) (13, -1.23444736573619914810e-01) (14, 2.99215853992118641091e-01) (15, 6.97470741575467290119e-01) (16, -2.48630045493526602440e-01) (17, -2.31718158861084128075e-01) (18, 6.24298470899349200458e-01) (19, -6.04531526171749544751e-01) (20, 6.14365776075540326318e-01) (21, 4.79982561351581182851e-01) 
