FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.72525399326180661763e-01) (1, 3.47760265680803537514e-01) (2, 3.11046017321600198891e-01) (3, 3.59207993361009836342e-01) (4, 3.09900326642050027992e-01) (5, -8.86345810279109858065e-01) (6, -4.03779168004017297022e+00) (7, 3.88678703587894580629e-01) (8, 3.63242739963846539286e-01) (9, 1.48828737023344359613e+01) (10, 2.00926341861201535055e-01) (0, 1.43419557203374581356e+00) (1, 3.05170535535742804001e-01) (2, 2.97572236002614065598e-01) (3, 3.62087893725563825242e-01) (4, 3.85004113854100049608e-01) (5, -3.46222712851092184749e-01) (6, -8.18200082012730121050e+01) (7, -2.44539776240258022177e-01) (8, 2.98304530955989910534e-01) (9, 8.76406757660111113140e-01) (10, 8.55625365050988531679e-03) (0, 2.59725499139427551398e-01) (1, 3.75569289980973164411e-01) (2, 4.19780819176281960736e-01) (3, 4.40878136514271767865e-01) (4, 3.96641379653061898480e-01) (5, -2.51405815425622181092e-01) (6, -1.73512909027693629227e+00) (7, 4.57993448061253860537e-01) (8, 1.82239345254787410422e-01) (9, 3.05582088035682675198e+00) (10, 2.49678187817050256658e-01) (0, 1.81649033873175824727e-01) (1, 3.27355014386661646864e-01) (2, 2.64928227516897207838e-01) (3, 3.96575192215927241346e-01) (4, 3.63252038362510798475e-01) (5, -9.64953280076244035435e-01) (6, -3.97438718700628745850e+00) (7, 4.24310941379909545290e-01) (8, 3.68484275567846630839e-01) (9, 1.49768280047049842096e+01) (10, 2.02014163881255398580e-01) (0, 1.50822477926336007137e+00) (1, 3.19859305297560569947e-01) (2, 2.35841611301131182099e-01) (3, 3.07694593345351097291e-01) (4, 4.20778037881083588623e-01) (5, -1.79208884881031149661e-01) (6, -8.17222599217014789019e+01) (7, -1.35745419460509497700e-01) (8, 1.27331824390037939843e+00) (9, 1.23447790532188461654e-01) (10, 1.88956152439475610905e-01) (0, 1.86278128441667467641e-01) (1, 4.42311702726925881635e-01) (2, 3.55205869732941659223e-01) (3, 2.90389543830002871960e-01) (4, 3.40457370935524972211e-01) (5, -4.07955831679808755830e-01) (6, -1.69488156053183436356e+00) (7, 5.84717776757125773379e-01) (8, 2.20747626366504634543e-01) (9, 3.10519203347305294827e+00) (10, 2.28856743126822803625e-01) (0, 2.56139542120496110922e-01) (1, -6.79752423904251973363e-02) (2, -1.84610117926390177034e-01) (3, -1.71710383846552377962e-01) (4, -1.68673720910341745638e-01) (5, 9.12033688594238500791e-01) (6, 1.17172699924450363085e+00) (7, -5.29330400800456304933e-02) (8, -3.07302449498205620948e-01) (9, 7.81949258718791506695e-02) (10, -3.47861573039997429291e-01) (0, 2.78197136467960154382e-02) (1, 5.23072373401614720834e-01) (2, 6.23743754159899799205e-01) (3, 6.73947882186861857434e-01) (4, 5.21739367675277287972e-01) (5, 9.22951279695461090924e-01) (6, 1.37651577155782156581e+00) (7, 3.81853353867056988236e-01) (8, 2.13435049061218018451e+00) (9, -3.53587082575685407448e+00) (10, 9.29622600110821539765e-01) (0, -6.81518964149273875908e-02) (1, 5.76054012144900595693e-01) (2, 5.54556706662274634390e-01) (3, 5.77877169216967856435e-01) (4, 5.14447187746859824209e-01) (5, 8.70397316668071296952e-01) (6, 1.36350427504373961085e+00) (7, 5.47939476260665303542e-01) (8, 2.12961664502063507953e+00) (9, -3.54110403161174680520e+00) (10, 9.04355729254536444550e-01) (0, 5.53150531596210137675e-02) (1, 5.88127276548249700028e-01) (2, 4.99565145620210482313e-01) (3, 5.58765343078477361161e-01) (4, 5.78716686614854314286e-01) (5, 9.10886960480250862382e-01) (6, 1.38228759105993681544e+00) (7, 4.08884209515134044999e-01) (8, 2.00310270194450179559e+00) (9, -3.17852471671171299406e+00) (10, 9.50589824798499294367e-01) (11, 3.95470205783901573859e-01) (12, -5.51452406911704051673e-01) (13, 3.56921356260834099494e-01) (14, 3.87850139379558922492e-01) (15, -5.03380515842291931250e-01) (16, 3.82557768464146019660e-01) (17, 5.92908140110025572156e-01) (18, -1.72278666384998085936e-01) (19, -1.59040921815219588042e-01) (20, -1.64601953216376967193e-01) (21, 3.98404288851609555966e-01) 
