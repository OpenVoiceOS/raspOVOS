FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -2.81725765377413950308e-01) (1, -3.51345596213450295231e-01) (2, -4.19231464464773995182e-01) (3, -4.47664899129977089665e-01) (4, -4.27705665428747994206e-01) (5, 6.62445238863186958156e-01) (6, 1.06817803726465754721e+02) (7, 1.09585242220004674962e+00) (8, 4.34399178051691148283e-01) (9, -1.66102791245459013858e+00) (10, 2.38336482121302067894e-01) (0, -8.51079934683732330924e-01) (1, -2.39577286241920511811e-01) (2, -2.09476884750993769257e-01) (3, -1.57551013229759201417e-01) (4, -1.15438655315311430871e-01) (5, -2.47613190657208576129e-01) (6, 5.44437892714887960999e-01) (7, -8.90157353195650258293e-02) (8, -9.63099142588685619426e-02) (9, -3.48336459588586011549e+00) (10, -1.31931808279774048787e-01) (0, 3.75632577881113538520e-01) (1, 3.73621171862374090722e-01) (2, 3.15315833837281012109e-01) (3, 3.72977218181381964257e-01) (4, 2.79928712696324133447e-01) (5, 3.67019313140261038697e-01) (6, 5.80902534273017523425e-01) (7, 3.09710919828508013385e-01) (8, 9.66837671519542585230e-02) (9, -1.67267985774110194974e+00) (10, 3.49054505412499560535e+00) (0, 8.31084924765558508142e-01) (1, 4.64190769691825211307e-01) (2, 4.20569475491404831669e-01) (3, 5.21086428065657858433e-01) (4, 3.66719778378367722294e-01) (5, -1.23441824581219328216e-01) (6, -2.91498169332950718413e+00) (7, 1.78284055168943106340e-01) (8, 1.22733797975086461607e-01) (9, 8.93508359322695255855e+00) (10, -7.42806731371326689484e-01) (0, 5.87435762665569849439e-01) (1, 5.94534583163277674522e-01) (2, 6.37734785449521113243e-01) (3, 5.95485880744473505821e-01) (4, 6.71159408760086950707e-01) (5, 4.87918403216384133358e-02) (6, 1.14325042579140800925e+00) (7, 1.71050570053177780627e-01) (8, 3.01280850440743508400e+00) (9, -7.17327632169897011494e+00) (10, 1.34751649709037324421e+00) (0, -3.12415584810764423995e-01) (1, -6.98308261083271819558e-01) (2, -7.69236845867541152444e-01) (3, -8.11250487314369772562e-01) (4, -6.75183521555569488015e-01) (5, 5.45280403404953317725e-01) (6, 6.40893193281710704667e+00) (7, 1.27859926296562242953e+00) (8, 6.35523831881058098858e-01) (9, 1.08464214378085865853e+00) (10, 3.27960821123396584209e-01) (0, -1.24959753275756613178e+00) (1, -2.88008283656254915339e-01) (2, -2.58812200110570100886e-01) (3, -3.78488916736260616602e-01) (4, -3.25589578431264126124e-01) (5, -4.59471770018097358523e-01) (6, 2.56870585721653066358e+00) (7, -1.99233444332498482732e-01) (8, -5.44808529487346773768e-02) (9, -8.40079454130459524208e+00) (10, -8.92762451641312820838e-02) (0, 1.50746299747083922771e-01) (1, 6.78046178992491754833e-01) (2, 7.63917607243758234326e-01) (3, 6.25007485028010512096e-01) (4, 7.42949885304671320263e-01) (5, 1.35427149570472915574e-01) (6, 7.45610515081125746484e-01) (7, 2.54525882993539509513e-01) (8, 1.73266619809090927085e+00) (9, -4.74285281016686610656e+00) (10, 2.10779827887905346628e+00) (0, -9.19212321242898866558e-01) (1, -3.18139305743196265652e-01) (2, -3.19228148492791907742e-01) (3, -2.76871378007152335599e-01) (4, -1.63921943339326608990e-01) (5, -1.95557396402537747582e-01) (6, 4.00390931386375559242e+00) (7, 6.29117664350938715634e-02) (8, -6.67635150423740580949e-02) (9, -8.84794457229111053209e+00) (10, -7.73611737856378239853e-02) (0, 1.00378119481417682479e+00) (1, 4.60460467278200502594e-01) (2, 3.72627189397055025299e-01) (3, 4.22390347360807771881e-01) (4, 4.03257524608808870514e-01) (5, 2.34085583549875561604e-02) (6, -3.03675210868674172460e+00) (7, 2.14735759969072043107e-01) (8, 2.14999418802761271818e-01) (9, 8.99792522469906330684e+00) (10, -7.44651060397984121408e-01) (11, 6.50679527249638955944e-01) (12, -2.04202647507508161517e-01) (13, -9.46030170141388265792e-02) (14, 4.09044090055025344466e-01) (15, -2.28532538753621561334e-01) (16, 6.54572112884824197643e-01) (17, -1.36215365254571868503e-01) (18, -2.19338671308629495904e-01) (19, -2.05940029346635772312e-01) (20, 4.13801196359194045638e-01) (21, 4.71252266161082400409e-01) 
