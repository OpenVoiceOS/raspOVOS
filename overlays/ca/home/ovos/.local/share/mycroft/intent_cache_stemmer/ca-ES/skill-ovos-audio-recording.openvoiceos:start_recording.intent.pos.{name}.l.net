FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=17 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -7.90496896468372844957e-01) (1, -1.05077871007680254678e+01) (2, -1.21193130244604458312e+03) (3, -6.08105932615568711697e+00) (4, -2.83939464760041104796e+00) (5, 1.41977577177390532626e+01) (6, -2.27262445910709161012e+01) (7, -6.46323520002740337986e+02) (8, 6.40447625400456410638e-01) (9, -1.11468169444822137848e+02) (10, 7.02755805686945045352e+00) (11, -1.22042717302715163896e+03) (12, 2.57031112888409074557e-01) (13, 7.89458673241834052448e-01) (14, -9.67539801477292940035e+02) (15, -1.12417124282817564307e+01) (16, -2.18350276896793893400e+00) (0, -7.78662563156038967094e+00) (1, 1.01582440702376786135e+01) (2, -8.31196482552885917983e+00) (3, 3.14021092276899160822e+00) (4, -1.21629249941511030642e+00) (5, -2.48840455719839113158e+01) (6, 1.15393468047237512941e+00) (7, -9.77805643268071789098e-02) (8, 2.61956025888875521090e+00) (9, 1.61169455522526039459e+00) (10, 2.30107595219069160919e+00) (11, 7.90686270502919885672e+00) (12, 3.31626047187698835828e+00) (13, 3.96174353983242522048e+00) (14, 1.66312203121227253355e+00) (15, -8.31166921494983853336e+02) (16, 4.99169487068228834303e-01) (0, -4.96685832676290672794e+00) (1, 2.24746480372177881435e+00) (2, 1.30079353803716433191e+01) (3, 3.28011293625048605538e-01) (4, -6.96391143715182292517e-01) (5, -2.22955752051392970259e+01) (6, 2.08745799200923676242e-01) (7, 2.89966099222755202547e+00) (8, -3.70423646796069627740e+00) (9, -1.36834759895900659465e+00) (10, 9.66180552147723803458e-01) (11, -4.06253853589468505447e+00) (12, -9.33324755435206432352e-01) (13, -3.75896594037281372991e+00) (14, -8.01953623004432070154e-01) (15, 6.29078014375570901962e+01) (16, -2.25389115561263198018e+00) (17, 1.52842558618885551169e+01) (18, 1.48007724056401581691e+01) (19, 2.86899876120855132910e+01) (20, -8.04533361210271458219e+00) 
