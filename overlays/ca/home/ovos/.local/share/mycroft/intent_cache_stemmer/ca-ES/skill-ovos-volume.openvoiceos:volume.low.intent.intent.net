FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.19758775864374769071e+01) (1, 3.71533569481978687143e-01) (2, 3.19834653821597370005e-01) (3, 4.51945808318267139292e-01) (4, 3.47448297378430637217e-01) (5, -8.13284004590473408314e+00) (6, 6.48824579101354492749e-01) (7, -8.02540735187079912194e-01) (8, -8.80882005463434064607e-03) (9, 3.68461990334544187142e-01) (10, 6.58542825523891761641e-01) (0, 4.18633781556618345121e+01) (1, 2.69209654136519926926e-01) (2, 2.43507177635055166443e-01) (3, 3.16101969285827177902e-01) (4, 2.59636067851405638596e-01) (5, -7.50031468459347294697e+00) (6, 1.82646845226232623638e+00) (7, -5.96421197189526575144e-01) (8, -2.71038700001402788065e-01) (9, 3.11195137548990252707e-01) (10, 4.18324250536333597239e-01) (0, 5.39299611024440794793e-01) (1, 3.51404497524932857289e-01) (2, 4.02386615177825923695e-01) (3, 3.72733289500907893910e-01) (4, 4.02270862957672115101e-01) (5, 1.32872784250325707944e+02) (6, -7.30727165746408235947e+00) (7, 5.65109174482604892376e-01) (8, -4.03172945152228123789e-01) (9, -2.66290462544691908242e-01) (10, -2.99322758380243691967e-01) (0, -1.26772529808866085688e+00) (1, -2.00411549627747126445e-01) (2, -8.67022466656162428544e-03) (3, -2.90935326814313090227e-02) (4, -4.32873655557293804397e-02) (5, 4.22572119463304918252e-01) (6, 1.10329344290844622023e+00) (7, 2.37271165612430112724e-01) (8, 3.95464309248812484476e-02) (9, -5.45897336084687767599e-02) (10, -2.38575664601087172878e-01) (0, 4.19139035378229749540e+01) (1, 3.28454815652037324103e-01) (2, 3.44848622437382401618e-01) (3, 3.33279953207636536749e-01) (4, 4.12287981118822755011e-01) (5, -8.18749522528610462757e+00) (6, 5.25826006215364349927e-01) (7, -9.74825760311976208250e-01) (8, 1.44433641328708062268e-01) (9, 5.02082530145493866058e-01) (10, 8.74368295256176986108e-01) (0, -2.23612498446388752171e-01) (1, -4.71628475554272241083e-03) (2, -1.36509455922916439485e-01) (3, -6.11421095169333919661e-02) (4, -1.50918238047435787630e-01) (5, 3.48699069630999447522e-01) (6, 1.00952296871296631764e+00) (7, 3.93699612926346975694e-01) (8, 8.84590820741898986590e-02) (9, -1.12564391421435372465e-01) (10, 1.69304298299715744180e-02) (0, -1.58186061405644375066e-02) (1, -2.97737526057655459144e-02) (2, -3.82878328023845901362e-02) (3, -7.79195161817962389517e-02) (4, -3.65860941170150916202e-02) (5, 5.30583645361592193268e-01) (6, -5.76812401026372523205e+00) (7, 5.19050927237266424008e-01) (8, -1.45663960051636252879e-02) (9, -1.26106141804605553780e-01) (10, 9.28393435110264481080e-02) (0, 1.22821824527278040257e-02) (1, -5.19298123477394332759e-02) (2, -1.80742772577697947967e-02) (3, -6.49415882585937659366e-02) (4, -1.78042086122924998748e-02) (5, 4.99214018839528039795e-01) (6, -5.44773605956306905540e+00) (7, 5.20960138515228154965e-01) (8, -2.10803128336561826361e-02) (9, -9.97555881634333736774e-03) (10, 5.28323632110767604786e-02) (0, 1.99870810673028198590e-01) (1, 7.82110006231170745705e-02) (2, 1.87144840570102277688e-01) (3, 1.78068424077640119485e-01) (4, 1.77628184171329084329e-01) (5, 8.86203266982395376772e+00) (6, -7.74324994710041347723e-01) (7, 1.32221902890522757801e+00) (8, 1.38003008302505530969e-01) (9, -1.75735437575039905866e-02) (10, 1.00715691306033694463e-01) (0, -1.82285327712661177069e-01) (1, -6.75718935168532763846e-02) (2, -1.88254492319373800679e-02) (3, -7.52068014777450000174e-02) (4, 8.00506007306293031200e-03) (5, 1.49818738013713120827e-01) (6, 3.17021447933042876954e+00) (7, 9.71930089001313557429e-02) (8, -1.18631729848006309269e-01) (9, -1.40682504643070110939e-01) (10, -2.32228571603278755031e-02) (11, -8.77066866738118178226e-02) (12, -1.37596280990294361768e-01) (13, 5.00779606541943311804e-01) (14, 3.62302290903254664567e-01) (15, -1.36067384598902607618e-01) (16, 4.04857855661441334849e-01) (17, -4.17417266091371641501e-01) (18, -3.85846347233320341452e-01) (19, 9.56130407505647282596e-03) (20, 4.05274332652301949587e-01) (21, 3.41238181914625904323e-01) 
