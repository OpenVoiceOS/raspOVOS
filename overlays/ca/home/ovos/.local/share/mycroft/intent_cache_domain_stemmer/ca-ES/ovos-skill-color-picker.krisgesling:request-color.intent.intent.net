FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -8.11243643379672835225e-02) (1, -1.35749828557063328960e-01) (2, -2.49065351227809150814e-01) (3, -8.92122355499756075625e-02) (4, -1.15590584019709757824e-01) (5, -1.80422227287251379924e-01) (6, -1.58751018274285643495e-01) (7, 1.62731984812386626382e-02) (8, 3.33694792257819639048e+00) (9, -2.46683220460847923983e-01) (10, -2.66236390670944386705e-02) (0, 3.08695306240676703435e-01) (1, 3.25148952479026365214e-01) (2, 2.61998486990592360790e-01) (3, 2.63343279535434182659e-01) (4, 4.36408435279986905986e-01) (5, 6.04462834809582627571e-01) (6, 3.23344600142575799850e+00) (7, 1.86957159413764978373e-01) (8, -3.13009607153667568369e+00) (9, -7.50152972965302949904e-02) (10, 4.61070167600928504381e-03) (0, -1.30865885640999539408e-02) (1, -2.15409792284353945879e-01) (2, -8.63476761382662716393e-02) (3, -1.69603898445948308993e-01) (4, -1.39870896260126748034e-01) (5, -1.58737547446696602993e-01) (6, -7.22669977103455041245e-01) (7, -1.97698098163851948561e-01) (8, 6.85913673902769027713e-01) (9, -2.11988181150158938593e+00) (10, 2.49268928988463367435e-01) (0, -3.86050533154858876994e-02) (1, -6.14248192326747044500e-02) (2, 2.62955898030079356087e-02) (3, -1.05594725759478688665e-01) (4, -3.04094976203166320050e-02) (5, 8.14749484809359936222e-01) (6, 1.19588256433410561330e-01) (7, 2.43941035481885948322e-01) (8, -1.60887176241462814819e-01) (9, 6.61590161486212746667e-01) (10, -2.95530014966389302511e-02) (0, 6.98929274712640880018e-01) (1, 6.01420325282355006102e-01) (2, 5.40015374365587885741e-01) (3, 5.60599205139418299559e-01) (4, 6.19336110595007593993e-01) (5, 9.54016331121934624271e-02) (6, 2.74732034859304752317e-01) (7, 5.27756330253751571036e-01) (8, -3.57972875056470130062e+00) (9, 3.37389247761350430377e-01) (10, -1.34400559196494157055e-01) (0, 7.91265849230785911494e-01) (1, 1.34837150924958426579e+00) (2, 1.49336710178412657157e+00) (3, 1.40368857585944395439e+00) (4, 1.39165579133786398991e+00) (5, 5.76090571221287484072e-01) (6, 6.55607134847982164771e-01) (7, 9.43936672362700623928e-01) (8, 1.62943151796589780211e+00) (9, 5.95206229895211436087e-01) (10, 5.27314578971392577955e+00) (0, -3.54889591752456934337e-01) (1, -9.45860833435489656296e-01) (2, -1.04473419910593201543e+00) (3, -9.79572833090259553757e-01) (4, -9.39117149024440767136e-01) (5, -8.04006772271238401295e-01) (6, -1.94114749831382438217e+00) (7, -6.38276080983246618672e-01) (8, -4.04886941333359473560e+00) (9, -5.83237311684474901696e-01) (10, 2.25928663940736768057e-01) (0, -4.28176646774527824846e-01) (1, -8.07212296715596577812e-01) (2, -8.08004174223760029960e-01) (3, -7.91456479421476011282e-01) (4, -8.48678145399907712942e-01) (5, -5.70636590993171988195e-01) (6, -1.73178296673859977162e+00) (7, -1.11859685277638440382e-01) (8, -3.09029167350912725709e+00) (9, -2.11914005538529043315e-01) (10, -8.89726007645017746839e-01) (0, -3.57204874699828256457e-01) (1, -7.84913468948224668509e-01) (2, -9.39892809680322294241e-01) (3, -7.66528729191640501028e-01) (4, -7.97843340626576802421e-01) (5, -5.48853455161815939611e-01) (6, -1.74073035407628440296e+00) (7, -1.46400427913365521748e-01) (8, -3.04320527818585073732e+00) (9, -2.24446764996355602184e-01) (10, -8.27986607615835978891e-01) (0, 5.86888362172617028545e-02) (1, -1.68606966306963645952e-02) (2, -4.50793832113543097018e-02) (3, 1.08554665543643793016e-03) (4, -1.00641871946076461031e-01) (5, -6.10766316532976305531e-02) (6, -1.83387542397504210712e-01) (7, -1.92829234830368123621e-01) (8, 9.04871985801591804943e+00) (9, -6.78844663090641842196e-01) (10, 1.28256490608803286335e-01) (11, 1.01317502936613745845e+00) (12, 4.19419039242372115872e-01) (13, -4.07357584075040635163e-01) (14, 8.03606166426824808369e-01) (15, 7.34326187180445177916e-01) (16, 1.49466419821438628590e-02) (17, 4.09155423978521615158e-01) (18, 3.31051686522258636902e-01) (19, 4.91305752008076232484e-02) (20, 8.65677580859571693672e-01) (21, 1.40245948200292402941e-01) 
