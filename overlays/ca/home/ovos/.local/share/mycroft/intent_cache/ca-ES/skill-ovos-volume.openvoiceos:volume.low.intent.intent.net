FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.54958546978837841124e+01) (1, 1.19159525101650531909e+00) (2, 1.20308442255128200671e+00) (3, 1.32080143531430560522e+00) (4, 1.15145713677275973730e+00) (5, -8.59368939435457512843e+00) (6, 3.90187150793730852527e+00) (7, -1.00628759754004946103e+01) (8, 7.19670648772503440682e+00) (9, -1.89257922779194132090e+00) (10, 3.32100523791932156215e+00) (0, 1.00227241329907790224e+00) (1, -7.25875051409616556253e-02) (2, -1.12133685657442999650e-01) (3, -2.43039915055646982278e-02) (4, 1.05765497594461424513e-02) (5, 4.38130089317908200108e-01) (6, 1.05238405177176627681e-01) (7, 1.09829468035975352791e+00) (8, 7.91179608583799237609e-02) (9, -6.70306291617114258230e+00) (10, 3.16731130941567240455e-01) (0, -2.14874838179475377564e-02) (1, -1.05409520781105880971e-01) (2, 4.48221423619369133129e-03) (3, -1.68117239237489425607e-02) (4, -6.57698056704421416679e-03) (5, -1.86957289944630178358e-01) (6, -1.35731249745061843814e-01) (7, -1.31549583708874401333e-01) (8, -1.02201265737380808662e-01) (9, 3.32021921031411659797e+00) (10, -9.95800880009598682330e-02) (0, -1.28652490409402684435e+01) (1, -5.35309595285089923067e-01) (2, -4.00969279227884778383e-01) (3, -4.09636018095644482973e-01) (4, -4.52039172170790204408e-01) (5, 1.76680418382608217520e+00) (6, -2.54941045460911723530e-02) (7, 1.19292176321677767525e+00) (8, 5.00443548365620327623e-01) (9, 3.20164598849760029253e+00) (10, 6.34465898903534553099e-01) (0, 4.04201662739840728023e+00) (1, 4.53959349550838886511e-01) (2, 4.04479715981121479285e-01) (3, 3.52677818395729480994e-01) (4, 4.24870956697102009070e-01) (5, 5.64915563074967685964e+00) (6, 2.02158904793483917839e-01) (7, 7.27139125672973989367e+01) (8, -6.18477586935417961156e-02) (9, -1.09062419259917433578e+01) (10, -1.35467577819560580643e+00) (0, -9.76428750033140557818e-01) (1, 5.87847738404602276896e-02) (2, 1.14981418742689564527e-01) (3, 1.97712971164736156515e-01) (4, 1.71059934689554576925e-01) (5, 2.91909472360007027358e-01) (6, 2.39132590268972849978e-01) (7, 5.64155585474596654727e-01) (8, 2.69053773712396138151e-01) (9, -1.37050937645481063498e+00) (10, 1.85464913499421346632e-01) (0, -1.15918376237880393909e+00) (1, 1.13994012567247809953e-01) (2, 6.98135597436088800594e-02) (3, 2.09409232231821423875e-01) (4, 1.03345054182257103936e-01) (5, 9.96408902664716600306e-01) (6, 2.29303592661099564509e-01) (7, 5.66837407059298192813e-01) (8, 4.58436509790181967450e-01) (9, -1.50300779929282524172e+00) (10, 1.68159166824883687941e-01) (0, 3.74325488480756474186e-02) (1, -1.39955961362111195451e-02) (2, -9.20026385442005711912e-02) (3, -5.50802284136998193675e-02) (4, -2.39342683211552428868e-02) (5, -7.63559598548346096791e-01) (6, -1.19782850583352196150e-01) (7, -5.32919393424512111146e-01) (8, -2.32929509307064075418e-01) (9, 6.05598382080076014233e+00) (10, -1.44602671633262097606e-01) (0, 1.40519458890200676926e-01) (1, 3.19607179371469873885e-01) (2, 2.36056174842470350983e-01) (3, 2.96805683282965082626e-01) (4, 2.52785373238676280838e-01) (5, -2.86124517478955642247e+00) (6, 6.74400013372985274174e+00) (7, -6.26720829039703053454e-01) (8, 8.25791733537691285960e+00) (9, 7.63682514839574810850e+00) (10, -3.60575065955063056578e+00) (0, 8.24500578267463729576e-02) (1, -6.62082296061560215961e-02) (2, -2.97823038148924239266e-02) (3, -1.66012730646177865401e-02) (4, -1.31374874358181709111e-01) (5, -7.41908035535849941233e-01) (6, -8.31507970974712240508e-02) (7, -2.76102655640000860249e-01) (8, -2.02390881173295206219e-01) (9, 4.22685162722630103360e+00) (10, -1.02998233341338063518e-01) (11, -2.15142563751277604567e-01) (12, -9.31856127616069104813e-03) (13, 4.84819568308676174251e-01) (14, 4.84379628312654852973e-01) (15, 5.53562959326558745587e-01) (16, 3.73083815669038754148e-01) (17, 3.69051855680169127449e-01) (18, 4.69005661673120688437e-01) (19, -3.80497903339908419285e-02) (20, 4.29345356888345908164e-01) (21, 4.11374737150264457863e-01) 
