FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=19 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (19, 6, 5.00000000000000000000e-01) (19, 6, 5.00000000000000000000e-01) (19, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -4.42142752874705902144e+00) (1, 1.30929191531983279972e+00) (2, 5.28302126919015582751e-02) (3, 6.17269876614071399956e-01) (4, 6.84502269438236110588e-01) (5, 5.84716718725284412272e-01) (6, 8.05260458310424365180e-01) (7, 1.41182697475152618694e+00) (8, 7.32969838783318061726e-02) (9, 6.95166135091191739193e-01) (10, 5.83994787267764881022e-01) (11, -7.19400837644029622631e-01) (12, -4.59696623578516316844e-02) (13, 1.50416336753161128392e+00) (14, -2.25443619635458947226e-02) (15, 4.13347423389585355569e-01) (16, 5.39986781134312643360e-01) (17, -2.07665438669143859185e-02) (18, -7.07303313127435662189e-01) (0, 4.85790553000158809027e+00) (1, -7.53398627080089089425e-01) (2, 4.94978640187937746209e-01) (3, -3.19178141987047692130e-01) (4, -3.38933532463070119611e-01) (5, -2.52591566733700201297e-01) (6, -7.06550024171236135118e-01) (7, -6.80748708163012805628e-01) (8, 6.96867829600670063428e-01) (9, -3.31113112495895534071e-01) (10, -1.87733621887547053042e-01) (11, 1.72653210632539222402e+00) (12, 4.33901911670747819638e-01) (13, -2.38603014315379091670e+00) (14, 3.70468964509581055733e-01) (15, 2.11804245474540131511e-01) (16, -6.79427966226150314988e-01) (17, 5.73620935344170690406e-01) (18, 2.04340785788555390212e+00) (0, -3.79224740270512361562e+00) (1, 8.44776043484942951167e-01) (2, 1.06313145899268046701e-01) (3, 1.00010495546087185836e+00) (4, 3.11096171183492087309e-01) (5, 4.54041368002177203600e-01) (6, 5.05839641436072184888e-01) (7, 6.43260254727942237807e-01) (8, -6.20060028210458955011e-02) (9, 3.34880860322963103126e-01) (10, 4.69126752251864398424e-01) (11, -1.87894470326946005923e-01) (12, -1.16616574406291459431e-01) (13, 6.16958655085141249863e-01) (14, 2.28288937086031512580e-02) (15, 1.73079577450274968431e-01) (16, 2.67640379357553115369e+00) (17, 1.70346416646469961398e-01) (18, -4.43716973248074963987e-01) (19, 3.13832864720287716409e+01) (20, -2.68219984532737409921e+00) (21, 1.30003381058715348217e+01) (22, -3.93086629257467679022e-01) 
