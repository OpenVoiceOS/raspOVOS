FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 5.46049633201446393826e-02) (1, 4.23096353527792623517e-02) (2, -6.29871924999467480699e-02) (3, 1.13456704687763088035e-01) (4, 3.59637597915419271466e-02) (5, -2.90420673224927802791e-02) (6, -7.72094535625362232700e-02) (7, -1.44053648076946622103e-01) (8, -8.06108409874659481709e-01) (9, -2.67053070430867400820e-01) (10, 1.33272352679988093094e+00) (0, 1.61021927158606970032e-01) (1, 2.81362995701080337296e-02) (2, -1.66974617521261553543e-01) (3, -1.24788040648197540161e-01) (4, -1.15101943145489105103e-01) (5, -3.20820971691180498175e-02) (6, 1.01943569490430041569e-01) (7, -2.58801072088251163383e-03) (8, 2.03104732420241695934e+00) (9, -8.30831232000951991346e-02) (10, -4.57595336967461463473e-01) (0, 1.38512991110971417363e+00) (1, 6.59138812998933676379e-02) (2, -1.00569770356015927848e-02) (3, -2.81173036118345098799e-02) (4, 1.09576332507531154503e-01) (5, -4.52243641996196488186e-01) (6, -3.78809341524027854906e-03) (7, 2.84970098642523039789e-01) (8, -1.14526521091367272653e+00) (9, -1.29470649290406941700e-01) (10, 1.10436826782282682125e+00) (0, 1.44567334811461917221e-01) (1, 1.12026705115736632812e-02) (2, 6.56647457450331289985e-02) (3, -5.17877639562188268885e-02) (4, -7.12143519252835255351e-02) (5, -1.92308129460297544400e-01) (6, 1.02575068871765040512e-01) (7, -1.14573508963235953795e-01) (8, 1.80330458081179689778e+00) (9, -1.32163355186937586661e-01) (10, -6.41496871736860652113e-01) (0, 7.28373634084821564016e+00) (1, 9.57487226763702875543e-02) (2, 1.33655801538560520259e-01) (3, 1.27204403404328952876e-01) (4, 5.91283888855911529681e-02) (5, -4.82665545109078664154e-01) (6, -1.40247360940325471734e-01) (7, -4.41682984121405719513e-01) (8, -1.33419452669212557083e+00) (9, -4.46605090983473951294e-01) (10, 1.56279731133493249295e+00) (0, -1.87617862075118074783e-01) (1, -5.47264248275636383534e-02) (2, -1.41129672670243858090e-02) (3, -1.02099607899176936243e-01) (4, 2.24925100660444340916e-02) (5, -2.38394072471670798175e-01) (6, 7.61777224652654821879e-02) (7, 1.42517626543036679543e-01) (8, 2.01807742055451688756e+00) (9, -8.91235584691809634705e-02) (10, -6.34335222560053924035e-01) (0, 2.92489980321633780136e+00) (1, 2.61191598165420146849e-01) (2, 3.30686791825679338164e-01) (3, 2.40216705042508693602e-01) (4, 2.89227916884807145781e-01) (5, 1.25629330537753314267e+00) (6, 1.29450496594840758213e+00) (7, 6.96956010701841877086e-01) (8, -4.97614436542712557099e+00) (9, 3.84297425937848657895e-01) (10, 2.35160889303748277257e+00) (0, 6.09511830729407400042e-01) (1, 2.81278460478937342160e-01) (2, 2.19909413910066964259e-01) (3, 2.11804970359957100978e-01) (4, 4.05498600757276672635e-01) (5, 8.49951320484587657944e-03) (6, 3.64419722769099507786e-01) (7, 1.38273480126658077083e-01) (8, 6.37417396602679486861e-01) (9, 4.02217559562164303499e-02) (10, -2.05241622294106962343e+00) (0, 1.03327760205397245485e-02) (1, -9.71239734520478736890e-02) (2, -1.14321339954896218638e-02) (3, -9.47133489747479032084e-03) (4, -8.00025690426393043531e-02) (5, -2.07723802019769393512e-01) (6, 2.40383977149662780848e-01) (7, -1.48967268116977996639e-01) (8, 2.37258714288698957517e+00) (9, -3.04484254221827621567e-01) (10, -3.74897451037899009929e-01) (0, 1.33492967973640408452e+00) (1, 1.18988529643469266583e-02) (2, 2.56712586483412094629e-02) (3, 7.80505129000597236422e-02) (4, 1.25221755435174053606e-01) (5, -2.94167622823480912064e-01) (6, 6.52122699346742579207e-02) (7, 2.56540423536073758370e-01) (8, -1.24204286950418940627e+00) (9, -1.97217362698865056325e-01) (10, 1.01720921533163899397e+00) (11, -4.24659484941088904986e-01) (12, 4.63244775744710457488e-01) (13, -4.24428378433215292009e-01) (14, 6.55273417611866304533e-01) (15, -3.11263656008181399670e-01) (16, 4.09730261828743724806e-01) (17, 1.49528605992430185800e+00) (18, 2.76854432500894809266e-01) (19, 3.85412845543482651678e-01) (20, -4.08393008055548512036e-01) (21, 2.48807028160478838075e-01) 
