FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=19 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (19, 6, 5.00000000000000000000e-01) (19, 6, 5.00000000000000000000e-01) (19, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.18975013431629648153e+01) (1, 5.53233335359644140539e-01) (2, 3.26333601791722971441e+00) (3, -4.45712702811847805151e+00) (4, 3.71189764160887047950e+00) (5, -2.09675329018664635683e+00) (6, -5.14984681665588528432e+00) (7, 9.64801643928416940987e-01) (8, -3.81747399472259374420e+01) (9, -5.76781795536317343220e+00) (10, 2.31873317550652968677e+00) (11, 1.80608802466652385021e+00) (12, 5.94110918135141741203e+00) (13, -1.92667785111285083666e+00) (14, 2.16155286084373665645e+00) (15, 3.49641302665940179040e+00) (16, 2.08018660068969918342e+00) (17, 1.50000000000000000000e+03) (18, 8.53723151103823324481e-02) (0, -1.91098990748862824773e+01) (1, 1.00348385774496742795e-01) (2, 4.93972147530895799861e-01) (3, -1.03942193738076316123e+00) (4, -1.80632560300552552635e-01) (5, -6.14320706969335161851e-01) (6, -1.63416469244261897131e+00) (7, -5.31332226321278744408e-01) (8, 9.76139285446898652410e-02) (9, -2.61208928429121578496e-01) (10, -5.32010145020437530405e-01) (11, 4.53642860054465357766e-01) (12, 1.89709812122871124274e+00) (13, -6.15767529330771057339e-01) (14, -1.28081075554301371255e-01) (15, 9.88081226095933629061e-01) (16, -7.26459212204356830078e-01) (17, 1.50000000000000000000e+03) (18, 1.73497329040446568449e+00) (0, 1.16043015050042068737e+01) (1, 3.66350217559316737592e+00) (2, -6.02439788517656782396e-01) (3, -2.80397841687128446253e+00) (4, -8.80294132724811273505e-01) (5, -1.26787197144722476594e+01) (6, 1.11185369751044849096e+00) (7, -9.90797513071497348847e+00) (8, 3.50247086304861543482e+00) (9, 7.50099066216105558880e+00) (10, -1.01930502888562433128e+01) (11, -4.11080753318969316368e-01) (12, 4.57908951772888428877e+02) (13, -1.44377830293802773554e+01) (14, -2.31636708436130644984e+00) (15, 1.50000000000000000000e+03) (16, -2.42780036237579821545e+00) (17, 1.50000000000000000000e+03) (18, -2.18436763749715101568e-01) (19, -2.66284479928297947993e+01) (20, 6.23731485822977376188e+01) (21, -5.99835915265286168108e+00) (22, 5.08681645248253766489e-01) 
