FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=17 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -8.43392958352230159846e-01) (1, 1.18832908120425861931e+00) (2, 4.06957935860218178448e+00) (3, -8.66403546214197795017e-01) (4, -1.27306411024806387378e-01) (5, -8.06607452232425276017e-02) (6, 3.48305338400760278184e-01) (7, -8.56850141076469062895e-01) (8, 3.28089509025413254492e-03) (9, 1.74470889991786504147e+00) (10, -9.62763665413010789884e-03) (11, 2.27249664328311418160e+00) (12, 4.93720365487340037691e-02) (13, -2.97208701763821303565e-01) (14, 2.81949935694690534405e+00) (15, 9.92853772664701761386e-01) (16, -2.20568635716843619576e+00) (0, -8.56288070727251549030e-01) (1, 1.33350022545402313590e+00) (2, 2.24450402902113888004e+00) (3, -8.02539572149608693152e-01) (4, -7.39614534943079243723e-02) (5, 1.35630744498188952896e-02) (6, 2.95885012823501214463e-01) (7, -8.24133583692931770415e-01) (8, -5.03953152547043300991e-02) (9, 1.81436565941836858151e+00) (10, -3.03819398550652597368e-01) (11, 2.17989588788960908516e+00) (12, -3.13959972425506966820e-02) (13, 4.82454877379852764552e-01) (14, 1.96877838358858214818e+00) (15, 1.42183480907920634095e+00) (16, -2.20321344241309180489e+00) (0, -3.04169940234877367313e-01) (1, -2.59121860978896112471e+00) (2, 1.67642885918598452122e+01) (3, -4.50514127662184682421e-01) (4, 1.58672773221851352854e-02) (5, -1.63778100246981972443e-01) (6, 3.20816775696317543765e-02) (7, -6.18713669983772668326e-02) (8, 2.84946089816730935545e-04) (9, 7.89811490328659493443e-01) (10, -1.71520543140645553848e+00) (11, 1.22043812646938398814e+01) (12, 4.32828129359295044054e-02) (13, 1.98124139900457546526e-01) (14, 1.37792060041703443574e+01) (15, -3.23878889088774091221e+00) (16, -1.83982485757718849939e+00) (17, 2.02988147928848405854e+01) (18, 8.73792302613489795249e+01) (19, 4.17816125892689349541e+00) (20, -1.41702695925807842237e+00) 
