FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.92483251185501380576e+00) (1, 5.61256625954637614062e-02) (2, 1.07009477287961035641e-01) (3, 1.56984455258084326656e-01) (4, 2.10664249331189185055e-01) (5, -1.58279609872951243510e+00) (6, 6.34342643814012285830e-01) (7, -3.27619255950687859880e-01) (8, 9.33961055246041588207e-02) (9, -4.61950443381988318237e-01) (10, 1.96299403136504568845e-01) (0, 1.62681105226348532433e+00) (1, 1.23577230690024408855e-01) (2, 1.74477384505770688472e-01) (3, 1.22247667131922754802e-01) (4, 1.33790292857192100184e-01) (5, -7.96680954327998014364e-01) (6, 1.07198857649861117869e-01) (7, -2.87812391917479259984e-01) (8, -8.07492611389095532592e-02) (9, -1.32737007276244517451e-01) (10, -4.37098104504339626808e-02) (0, 3.11604150350894748556e+00) (1, 4.48644598308143127685e-01) (2, 3.12524591866073175872e-01) (3, 3.22023337069091364349e-01) (4, 4.65571110430297363525e-01) (5, 1.25039984223875162428e+01) (6, 5.04527892449814663323e-01) (7, -3.52688444768227693515e-01) (8, 3.00004831811257977048e-01) (9, 4.21602360236637496715e-01) (10, -8.88545121409685334513e-01) (0, 1.38668288501319558748e+00) (1, 9.59461338584896294979e-02) (2, 5.68248636787410874027e-02) (3, 3.87742824619289397359e-02) (4, 5.21151132648464340824e-02) (5, -5.63589672208005265652e-01) (6, 4.55608825302967090920e-01) (7, -1.78274827187761192304e-01) (8, 1.85355343493969010327e-01) (9, -1.43513896189589706953e-01) (10, 1.37611061948754248574e-01) (0, 6.54069251551531394284e+00) (1, 3.65683293981723833710e-01) (2, 4.14556971831493370484e-01) (3, 4.01579766614608812958e-01) (4, 4.04939360065631914765e-01) (5, -4.79912281811115093433e+00) (6, 7.11131696411225755483e-01) (7, -9.79101482625303831675e-01) (8, 2.52999081155246552388e-01) (9, -5.08119373637880511474e-01) (10, 3.40088790434355414671e-01) (0, 2.54944808412874479586e+00) (1, 2.70754604880373617171e-01) (2, 3.84826942626993795393e-01) (3, 2.20416239742796532530e-01) (4, 3.13739030901949544905e-01) (5, -2.23872744151062130769e+00) (6, 2.06325543366690733471e-01) (7, -5.82685656383273853365e-01) (8, -2.58372224636325142910e-02) (9, -4.42395716892070078874e-01) (10, 3.88103247851175198191e-02) (0, 2.61522195119941969210e+00) (1, 2.48829047475616987883e-01) (2, 3.44007938448946615217e-01) (3, 2.98474050406973501204e-01) (4, 3.29661621992151765426e-01) (5, -2.24452085279530644257e+00) (6, 2.51602825962325193920e-01) (7, -4.67211178972957164035e-01) (8, -1.94662285521119532948e-03) (9, -4.35224212692565226579e-01) (10, -5.36932266086297682595e-02) (0, -2.97448950550635959900e+00) (1, -2.00049887257264316642e-02) (2, -3.44823757963822544181e-02) (3, 1.94097151202513446000e-02) (4, -9.10968865113900433395e-02) (5, 1.02203698011304688897e+00) (6, -3.28116076157614899866e-01) (7, 1.35471638472193256497e+00) (8, 1.81306088912223217768e-01) (9, 9.28344064993390283291e-01) (10, -1.60285365345789021507e-01) (0, 3.02418658103551685201e+00) (1, 4.06456094865921435666e-01) (2, 4.36766256277683673215e-01) (3, 3.65111146752956861050e-01) (4, 4.19021610801342425656e-01) (5, 1.23485435483488199537e+01) (6, 6.34444247463662125419e-01) (7, -9.74880171411270812776e-01) (8, 2.80651650687524456540e-01) (9, 3.65908175039284133678e-01) (10, -6.99854497774199391102e-01) (0, 2.61159772802675504977e+00) (1, 1.91803111974756829161e-01) (2, 3.76545592491190572737e-01) (3, 3.39403479640047578414e-01) (4, 2.49645914231102522551e-01) (5, -2.09994174771613240083e+00) (6, 2.66461503945609190502e-01) (7, -5.32771773174045115695e-01) (8, -9.80804659195193273558e-02) (9, -4.01217192397898936296e-01) (10, -2.03271313339429653133e-02) (11, -1.55191662377881876150e-02) (12, -1.58746037382287097373e-01) (13, 7.26984287374746518395e-01) (14, -2.06794633753324685888e-02) (15, -1.06098387046460324035e-01) (16, -4.87008976670926962971e-01) (17, -5.73077830406850452505e-01) (18, 3.52710194757897887996e-01) (19, 6.86652402156126440147e-01) (20, -4.64450388104105715836e-01) (21, 4.49181344176023344961e-01) 
