FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=17 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -2.07183847909767848705e+00) (1, 2.21632564548463895093e+00) (2, 4.12445426736602394069e-01) (3, 9.96876876701244918344e-01) (4, 6.13842960843791085601e-01) (5, 7.84924654807153454605e-01) (6, 2.17862457007475684989e+00) (7, 1.06816955994302742461e+00) (8, 8.16458117852063569586e-02) (9, 1.40350647367640068985e+00) (10, 1.55603049370976642685e+00) (11, 1.88657781450402084467e+00) (12, 8.98646834211697109751e-01) (13, -2.82528710370443247868e-02) (14, 9.56163722838827667694e-01) (15, 7.39808043982091656510e-01) (16, -1.92870440524418573780e+00) (0, -1.21484630308768681850e+00) (1, 1.04206018313843817857e+00) (2, 3.71766844060872680977e-01) (3, 6.04337683381591772580e-01) (4, 5.09107757338891464549e-01) (5, 4.82449563382638779707e-01) (6, 1.02421790028402570627e+00) (7, 4.76442790326535692280e-01) (8, -3.40885394391752596932e-02) (9, 8.94090075479565427052e-01) (10, 1.45897610511651754095e+00) (11, 1.26540115734498748168e+00) (12, 7.29810742160752923269e-01) (13, 2.41029932513202205246e-01) (14, 7.35551992680498201160e-01) (15, 5.61810525296701612774e-01) (16, -1.16292168554098807043e+00) (0, -5.58358718671933917932e+00) (1, 6.57273269542287708056e-01) (2, 3.00497987492498341844e-01) (3, 8.76140960947697222672e-01) (4, 6.46499706758221881486e-01) (5, 4.19761364798740022053e+00) (6, 7.63934014116467663413e-01) (7, 8.63606861695647709531e-01) (8, 7.76015035877509196105e-01) (9, 4.98436320397980114905e+00) (10, 3.36715558006226345356e+00) (11, 9.70145658440873304151e-01) (12, 1.06416369777835839727e+00) (13, 7.62950529977716729313e-01) (14, 4.97814038855378282467e-01) (15, 4.21497814994052522053e+00) (16, -6.86660249739545980319e-01) (17, 1.49989301347013341825e+01) (18, 9.93313462400376678829e+00) (19, 6.55046377116436495669e+00) (20, -9.25107898438288001053e-01) 
