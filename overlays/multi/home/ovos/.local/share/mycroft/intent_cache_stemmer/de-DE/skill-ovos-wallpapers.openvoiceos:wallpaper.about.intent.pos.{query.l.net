FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -6.13587512304407667330e-01) (1, 3.43234701231936861765e-02) (2, 5.80901388069119128943e+00) (3, -8.23548750073924001880e-02) (4, -3.31440651551104970007e-01) (5, -1.07346070666143156291e-01) (6, 5.84409931292261752844e+00) (7, 5.19386886565671307414e-02) (8, 1.73323012437937074015e-01) (9, -9.51097899446630812292e-01) (0, -9.52669659028196669226e-01) (1, 1.86365016725540066922e-01) (2, 5.83860165616001758337e+00) (3, -3.03524897487177508504e-02) (4, -3.43764537707187123328e-01) (5, -6.11512565205781233324e-02) (6, 5.80219663669790897131e+00) (7, -3.43880216193338805741e-03) (8, 3.93052841688369403172e-02) (9, -8.86724309394502974158e-01) (0, -4.89294091465934111174e-02) (1, 4.21868354165800429900e-01) (2, 1.03796354107500188313e+00) (3, 1.91955761980345751549e-01) (4, 6.25776584440170813295e-02) (5, 2.18658458536972938324e-01) (6, 8.99881021901379862804e-01) (7, 5.34472633116750694970e-02) (8, -2.96094392546872170469e-02) (9, -3.16026349410703399911e-01) (10, 5.83023797710861835242e+00) (11, 5.86298337568964633704e+00) (12, 2.72848927216640946281e-01) (13, -1.44389018993259665713e+00) 
