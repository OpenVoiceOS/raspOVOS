FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=17 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -4.72660563253235910253e+00) (1, 1.08694643691359149607e+00) (2, 5.95104766136895557693e-01) (3, 1.53051086129330271568e+00) (4, 1.13328525450388140072e+00) (5, -6.55324567139607405686e-01) (6, 1.06939570207351919429e+00) (7, 4.97029723977730319984e-01) (8, 1.88916424354689516107e+00) (9, 4.97649454614032538124e-01) (10, -1.62580798836350542969e-01) (11, 4.60762159997206488526e-01) (12, 2.03355407067771087881e+00) (13, -3.63081125677966881637e-02) (14, -1.03873328438491485670e+00) (15, 2.20831211571828545104e+00) (16, -9.11126504036172901735e-01) (0, -4.69939146303010080175e+00) (1, 5.90296493443354552966e-01) (2, 6.85516481346379658035e-01) (3, 1.17390651148892266598e+00) (4, 1.05913744148174471249e+00) (5, -2.59255101900683015970e-01) (6, 8.95161449485039506513e-01) (7, 3.37295877194092319495e-01) (8, 1.93298204716677002324e+00) (9, 3.34572960247721029603e+00) (10, -2.66595165701508540756e-01) (11, 7.77505959273769864382e-01) (12, 2.19217306846018011868e+00) (13, -1.65167788036102908267e-01) (14, -9.47475775040793610771e-01) (15, 2.00199530732008490475e+00) (16, -1.16489251731957343949e+00) (0, -1.92595050744198026393e+00) (1, 4.09208767984843513332e-01) (2, 3.96093360832869001431e-01) (3, 3.15212780174394913146e+00) (4, 3.53950034747701813842e-01) (5, 9.60568544084099418257e-02) (6, 6.94446486206878987879e-01) (7, 6.30024217547798581762e-01) (8, 1.12977759122891230348e+00) (9, -5.72027357315620488265e-01) (10, 2.43214825649183979950e-01) (11, 3.81960686650691716437e-01) (12, 7.39191539431187871756e-01) (13, 3.76675288749195646965e-01) (14, -1.32983049428629235278e-01) (15, 9.93075831248002249829e-01) (16, -8.78821818456976933120e-01) (17, 3.77786284195427626287e+01) (18, 2.03342417656375928914e+01) (19, 1.81644647211007921328e+01) (20, -1.98359146073468273208e+00) 
