FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=13 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (13, 6, 5.00000000000000000000e-01) (13, 6, 5.00000000000000000000e-01) (13, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.16568115256251569889e-01) (1, 1.27608717902157930979e+00) (2, 1.09502135777612252809e+00) (3, 2.74394811637890145839e-01) (4, 9.48739795683264830473e-01) (5, -1.14528830924087582366e-01) (6, 1.66313425849952700730e+01) (7, -1.45941650340654183182e+00) (8, 7.21027830960392690507e-01) (9, -2.49370392253093875468e+00) (10, -4.51780332229484304918e+00) (11, 1.30613339879553507217e+01) (12, -4.62992472849310998662e-01) (0, 4.57207724345140320565e+00) (1, -4.51481151806074887389e+00) (2, -3.83860483209360170420e+00) (3, -2.78608075587823300623e+00) (4, -2.72628595643100801738e+00) (5, -3.97901316537170979259e+00) (6, -1.87181238115014880741e+00) (7, 1.18306069811033931494e+00) (8, -4.98069730756041018083e+00) (9, 2.97524252762540397654e+00) (10, 3.08897538235465951573e+00) (11, -3.80931782446641920004e+00) (12, 4.64353972566537542832e+00) (0, 4.65149442744664831650e+00) (1, -4.54726022707182675475e+00) (2, -3.89740997741687822398e+00) (3, -2.67527231632544904016e+00) (4, -2.83673174642381731303e+00) (5, -4.25853129179043943253e+00) (6, -1.86266347050847858036e+00) (7, 1.26075898845839229345e+00) (8, -4.86295710113998858048e+00) (9, 3.03126456072076377879e+00) (10, 2.91665173064741545250e-01) (11, -3.98502894215126035604e+00) (12, 4.68850719821862860215e+00) (13, 1.08774610243684621835e+00) (14, -5.91040107229604050332e+00) (15, -5.85270848492039963418e+00) (16, -9.20670920545289872550e-01) 
