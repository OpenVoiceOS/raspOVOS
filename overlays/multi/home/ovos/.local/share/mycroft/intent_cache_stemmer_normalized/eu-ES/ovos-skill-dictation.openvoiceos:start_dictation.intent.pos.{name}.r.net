FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=14 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (14, 6, 5.00000000000000000000e-01) (14, 6, 5.00000000000000000000e-01) (14, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.26509478625034914312e-02) (1, 3.08505899646415704396e+00) (2, 3.26003120618778452489e-01) (3, -7.50834653135047935812e-01) (4, 3.22095599411259936318e+00) (5, 2.40709092440872529650e+00) (6, 1.34459584567177259373e+00) (7, 1.99880036827504081742e+00) (8, 1.35409031738737151151e+00) (9, -4.22524875543966449420e-01) (10, 1.25586570676369113642e+00) (11, 6.47880126969845870910e-01) (12, 1.26424526020642091773e+00) (13, -2.63638109255574581979e+00) (0, -2.39156269135971127904e-01) (1, 1.39079520367708076023e+00) (2, -6.15584578153981487603e-01) (3, 4.95062157726864626728e-01) (4, 1.38319134487830397440e+00) (5, 1.00066877868765491399e+00) (6, 6.58145779683257048376e-01) (7, 8.21909103905769988785e-01) (8, -1.47706825414455844570e-01) (9, 1.34300760646157257838e+00) (10, 2.55441082889585113769e-01) (11, -1.06715144513080725103e-02) (12, 4.02473459930530963469e-01) (13, -5.45079151084324142751e-01) (0, -9.02173370410907365935e-03) (1, 1.60339442895441997017e+00) (2, -7.15709300051353247696e-01) (3, 5.76658424056153195991e-01) (4, 1.24785372253088944206e+00) (5, 1.28174916979664432759e+00) (6, 5.18111707174412017274e-01) (7, 1.02931043154558810038e+00) (8, -1.14799047722064825883e-01) (9, 5.45157379214492365804e-01) (10, 4.70158254826812560534e-01) (11, 4.01019055310367467104e-01) (12, 2.13884114933941910097e-01) (13, -1.15056707525992729657e+00) (14, 3.80907235483748749516e+01) (15, 3.27735641920888021161e+00) (16, 2.26242977708369785717e+00) (17, -1.47312313484437340527e+00) 
