FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.12815299345205843906e+01) (1, 4.89517295312131672791e-01) (2, 5.21631681870664221101e-01) (3, 5.82210192035878759675e-01) (4, 5.98472455095494848543e-01) (5, 2.30747231353214343752e-01) (6, 5.23996124269413110497e-01) (7, -4.64911874483614262488e+00) (8, 3.52295976930048715392e-01) (9, 4.87221009745428688120e+01) (10, 8.78531756446931821891e-01) (0, -8.59583029332483872231e-01) (1, -2.11139691623511677188e-01) (2, -2.57236120852294247729e-01) (3, -3.19996019455256741626e-01) (4, -2.58336549254718106372e-01) (5, 6.08477653391581219111e-01) (6, -9.04601647127808022875e-02) (7, 2.69517274234758741347e-01) (8, -1.51396751211075569765e-01) (9, 5.53051496284974497541e+00) (10, -3.64219580494007433291e-01) (0, 2.60377754169905051285e+00) (1, 6.99985783316896092998e-01) (2, 6.61829177059934270488e-01) (3, 7.54726271846101415264e-01) (4, 5.97444910325811151530e-01) (5, -4.31998035871278274200e-01) (6, -8.73193427400457550247e-02) (7, 5.25882308421094535333e+01) (8, -6.36893674541068322004e-02) (9, -7.39668392166367105744e+00) (10, -1.09566700919905701817e+00) (0, -1.00588167035747488853e+01) (1, -1.90580827299086985605e-01) (2, -1.73569050732581553476e-01) (3, -1.77070950273005900399e-01) (4, -6.74179989132571572119e-02) (5, 7.50193845559551819591e-01) (6, -4.01656014046597176215e-02) (7, 5.98469132086856703268e-01) (8, -1.19691197777018032422e-01) (9, 1.26846657707070131771e+00) (10, -2.01925948185320308648e-01) (0, 2.22965547600096833381e-03) (1, -1.92447016222327960122e-01) (2, -2.34299229664653552163e-01) (3, -1.74251498861163867105e-01) (4, -2.22497994286865008462e-01) (5, 5.86813000001009998918e-01) (6, -2.03445229655773163202e-01) (7, -2.62119983663957445419e-01) (8, -5.58777713620382948667e-02) (9, 7.43391070995364344753e+00) (10, -2.11943775727867073089e-01) (0, 2.24263946835706491356e+01) (1, 1.16873227654511935292e+00) (2, 1.15789903549964412477e+00) (3, 1.14259444771821483400e+00) (4, 1.19759685395772463856e+00) (5, -1.32594683266363810858e-02) (6, 6.61399920282459863330e-01) (7, -1.43215098810184304767e+01) (8, 3.44758129528822676590e-01) (9, -1.01260966996850410027e+00) (10, 1.38113016038674740749e+00) (0, 1.25992433344375509563e+01) (1, 5.21185768358965129110e-01) (2, 5.45295381509323329183e-01) (3, 6.14320890568752497884e-01) (4, 6.26780272983570307943e-01) (5, 3.90619044415260185943e-01) (6, 4.26626519528562175854e-01) (7, -5.08467404691767832503e+00) (8, 4.24693544982010007249e-01) (9, 4.87058198764155036997e+01) (10, 9.16151227996919614860e-01) (0, -9.36122844097230996652e-03) (1, -1.32337410834317131814e-01) (2, -2.50014428880696304436e-01) (3, -2.15959777024273796853e-01) (4, -3.03835061278824924980e-01) (5, 5.40996445522079483226e-01) (6, -1.59543686253038302647e-01) (7, 4.10889971748873150670e-01) (8, -1.50958929949682296723e-01) (9, 4.46552991545508337623e+00) (10, -3.21147010042785674422e-01) (0, -2.21503380350939806931e+01) (1, -3.72588505679152581784e-01) (2, -4.00704567962668511960e-01) (3, -4.89112959885380615965e-01) (4, -4.22191535764716241452e-01) (5, 7.94021690094086740608e-01) (6, 5.73854808963004447975e-01) (7, 2.80743933298401948662e+00) (8, 7.87608247416441975552e-02) (9, 2.49661028491619063630e+00) (10, -5.08515894333049223164e-01) (0, 1.32261570715314658031e+00) (1, -3.25607873583311668853e-01) (2, -2.86878228271479351896e-01) (3, -3.54462914252753114042e-01) (4, -2.83302843654627545256e-01) (5, 1.17423787094640319495e+00) (6, 5.92711093074070327624e-02) (7, 5.51366179735093608905e-01) (8, 3.42881016339345312538e-01) (9, -1.40293636906311336432e+00) (10, 5.61666868962457765946e-01) (11, -7.23853704051499458050e-02) (12, 2.49946774200921761899e-01) (13, 6.66214955436011613266e-01) (14, 3.08822220206021180733e-01) (15, 3.27356227890973849881e-01) (16, -7.71354997441870138264e-02) (17, -7.08776262120728722804e-02) (18, 2.85394349352842136014e-01) (19, 3.36068168766979480377e-01) (20, -2.97546927124860360880e-01) (21, 7.55610867400901664581e-01) 
