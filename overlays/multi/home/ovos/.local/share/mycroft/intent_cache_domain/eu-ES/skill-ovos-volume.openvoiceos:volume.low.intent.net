FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.48603011609657400527e+01) (1, -1.57090233367675913279e-01) (2, -6.27997943282071202642e-02) (3, -2.06813556802267234369e-01) (4, -1.40414235842222345774e-01) (5, -3.39484713230788504035e+00) (6, 5.23347040501588534056e-01) (7, -5.92898927488845650657e+00) (8, 2.33329206120004872815e+00) (9, -5.76854234987524883138e+00) (10, 8.14885312383607618436e-01) (0, -1.24612735746220087973e-01) (1, -1.29037554111810460686e-03) (2, -1.26057794491914287871e-01) (3, -7.61297812565836645726e-02) (4, -3.12239324554476199713e-02) (5, -1.15209838595536157024e-01) (6, -1.76143705600949274093e+00) (7, -6.32856892758214550909e-01) (8, -6.01000375540553521958e-01) (9, -3.44065510119824113922e-01) (10, -5.25860056468942654861e-02) (0, -2.00634392140224909751e-01) (1, -8.44156389936480261449e-02) (2, 5.74203376922277630712e-03) (3, -8.30367078885111548070e-02) (4, -1.03707047353652465072e-01) (5, -2.26097984459546041958e-01) (6, -1.09073046211835733565e+00) (7, -6.17703153156840878424e-01) (8, -6.63934863598644176008e-01) (9, -4.93523430462507850525e-01) (10, 6.38545960000060086603e-02) (0, 8.87813561127299522013e+00) (1, 1.58376870600892116769e-01) (2, 3.04543631075335607949e-01) (3, 2.24849240778399545038e-01) (4, 1.34975166915370065057e-01) (5, -3.45077611137687956244e+00) (6, 3.49820157376283358275e-01) (7, -3.70852449315596555124e+00) (8, 2.04325100390167213593e-01) (9, -4.51990620354938510417e+00) (10, 8.74764998068817023480e-01) (0, 8.97327093497628425212e+00) (1, 1.78830215631438305124e-01) (2, 1.88607307074976970895e-01) (3, 1.39358023105574685419e-01) (4, 2.13306659756613808954e-01) (5, -3.36741669480263983161e+00) (6, 5.07594069329255770384e-01) (7, -3.78732631134797070871e+00) (8, 2.53697847930641096070e-01) (9, -4.63815395842123034953e+00) (10, 9.12837926855094794476e-01) (0, 8.95444351897353385539e+00) (1, 1.34275539417971961909e-01) (2, 3.47971269984143488641e-02) (3, 9.02424814302819899847e-02) (4, 1.03293857177008979731e-01) (5, -2.87949822795557919264e+00) (6, 5.14944007483476351439e-01) (7, -3.56895064762185931784e+00) (8, 1.03743972382365301499e+00) (9, -4.50493733147907260417e+00) (10, 7.36688372304446947858e-01) (0, -1.15459141672655984934e+00) (1, -7.89318037328666094465e-02) (2, -1.56980122357363000019e-01) (3, -1.32495990872139285388e-01) (4, -7.58603838381713274641e-02) (5, -3.05428571903340329019e-01) (6, -6.15791181609663951235e-01) (7, -5.86734408849084454474e-01) (8, -5.59305306134444180088e-01) (9, -4.86080315347342084564e-01) (10, -8.97290620882132405889e-03) (0, -1.14820565075680658396e+00) (1, -1.09956289559358919017e-01) (2, -1.23903418808931686801e-01) (3, -4.27508083281462614167e-02) (4, -1.49110979318374886615e-01) (5, -1.51153691017262475915e-01) (6, -1.00114536999076464419e+00) (7, -5.78232759946191388067e-01) (8, -5.66957499442320767002e-01) (9, -3.65789793815045061187e-01) (10, -1.25625935084410694786e-01) (0, 1.30249202068115721431e+00) (1, 3.64949154808359876867e-01) (2, 4.53161227657633558508e-01) (3, 3.80260850443678632971e-01) (4, 3.33144406928854719396e-01) (5, 6.44264555900267099631e-01) (6, 3.78086758571400061157e-01) (7, 7.84867311002815815080e-01) (8, 3.16635501820958564778e-01) (9, 6.97251087798903768089e-01) (10, -8.42065008544715526417e-02) (0, -1.05978183193474215784e+01) (1, 1.37516929668284909605e-01) (2, 9.70626146803043460265e-03) (3, 1.06534763139583127378e-01) (4, 6.50435899318232424449e-02) (5, 3.87511746054847794341e+00) (6, 5.63310609846095600162e-01) (7, 2.98252124623284364802e+00) (8, 2.85818342300877059614e+00) (9, 2.10254301851277336510e+00) (10, -2.19468215180654069307e-01) (11, -2.32320928287629790976e-01) (12, -4.43268293190115292290e-01) (13, -2.36456047566669130822e-01) (14, -2.76808613252763457968e-01) (15, -2.67388598334912908072e-01) (16, -1.99579807433569400565e-01) (17, -2.54239514293210677742e-01) (18, -2.13549870820777615288e-01) (19, 1.29441345002236918305e-01) (20, 5.35625621466926049585e-01) (21, 6.49098123175444130517e-01) 
