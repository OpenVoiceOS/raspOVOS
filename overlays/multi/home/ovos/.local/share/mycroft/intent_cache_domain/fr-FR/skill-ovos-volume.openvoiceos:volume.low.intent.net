FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -8.24953088041218030035e-01) (1, 1.69237291508932879980e-01) (2, 1.52832714909334949072e-01) (3, 1.82731097632666372732e-01) (4, 2.21581218832751059011e-01) (5, -8.69309241684174516251e+00) (6, 1.11016816852765459744e-01) (7, 7.86695368191207888842e-01) (8, 6.39499186637652594634e-02) (9, 5.50571683969039660234e-01) (10, 2.47855450809857991157e-01) (0, 9.93123171911097668563e-01) (1, 2.74855133907704662910e-01) (2, 9.44322243498852131882e-02) (3, 1.38276741670279673668e-01) (4, 1.15314093248753773979e-01) (5, 4.13953846938497971308e+00) (6, 2.95589339149493313119e-01) (7, -1.12916809632996395507e+00) (8, 3.26099140854088809238e-01) (9, -1.99174385609947601772e-01) (10, 2.14010458557738869612e-01) (0, -8.03477192626181752066e-02) (1, -1.07380405322283351000e-01) (2, -5.71451040661035622459e-02) (3, -1.76795304075449549730e-01) (4, -3.35415455257593447813e-02) (5, 4.80842744295093105222e+00) (6, 7.85624083516218707501e-02) (7, 1.72755908911649685988e-02) (8, -3.01480096145787627993e-02) (9, -9.65909049899258964622e-02) (10, -2.73622465956713661894e-02) (0, 1.93589905107552628927e+00) (1, 7.86850725695772634793e-01) (2, 7.44705644831343160917e-01) (3, 7.50627917632742391874e-01) (4, 7.99243246600313650418e-01) (5, 4.06827322639114807856e+00) (6, 1.09195293684457195305e+00) (7, -7.38725721971153354017e-02) (8, 8.63681466564398014896e+00) (9, 2.04539964806666696440e-01) (10, -1.97899691430669499859e+00) (0, 4.50074451071960339732e+01) (1, 2.37489784060075564964e-01) (2, 4.07251069245889329995e-01) (3, 4.02087280450418138589e-01) (4, 3.70604807434632921304e-01) (5, 2.73565886802793256649e-01) (6, -6.84371468805439064909e+00) (7, -2.92742547643225381471e-01) (8, -7.19026873405683053520e-01) (9, -6.26169631258036485377e-01) (10, 9.36243203338138929581e-01) (0, 4.48515878869039426036e+01) (1, 3.07877291057008894981e-01) (2, 2.97513175818819142204e-01) (3, 3.43997862253088149131e-01) (4, 2.50996578547853677055e-01) (5, 1.80439594888266086059e+00) (6, -6.49243056189661338351e+00) (7, -6.96976742817461691892e-01) (8, -8.32759332988180833723e-01) (9, -4.43522375341230512547e-01) (10, 7.83106047863816767673e-01) (0, 2.25839023352361989438e+00) (1, 8.19837016920195837955e-01) (2, 9.18764291266547572512e-01) (3, 8.73269690136061926822e-01) (4, 9.21047208766089808840e-01) (5, 4.09429876751787613642e+00) (6, 2.95441007269448352446e+00) (7, 9.03738063055282703218e-02) (8, 8.53490560188751423709e+00) (9, 2.30338440457398818673e-01) (10, 2.38350263859240518727e-01) (0, 2.10285933969472722538e+01) (1, 2.95082258213960058058e-01) (2, 2.97386499275170690382e-01) (3, 2.97959307362042791212e-01) (4, 2.53120580841504516645e-01) (5, 8.38214087585332667807e-01) (6, -3.90053215396459895814e+00) (7, -2.80613523638501272117e+00) (8, -4.38220700936879759002e-01) (9, -1.04900860357562542013e+00) (10, 7.94790502985811331271e-01) (0, -1.77377969693425696640e+00) (1, -9.22347536411598917239e-02) (2, -4.46622973409012274937e-02) (3, -1.13111889090851368278e-02) (4, -1.88652188154728783775e-01) (5, 1.64698429862907902432e+00) (6, 5.29466950507331071685e-02) (7, 1.12658997761704071650e-01) (8, 6.07425893117729334691e-02) (9, -1.58233931037236423556e-02) (10, -1.63483719224048529295e-01) (0, 6.94028239538644564632e-01) (1, 1.76636450783003939868e-01) (2, 1.50803581670512332202e-01) (3, 2.46068619981993835788e-01) (4, 9.87081982648268935199e-02) (5, 1.15560266065869914343e+01) (6, 1.17065545696714581880e-01) (7, 2.29052327889726681232e-01) (8, 2.46940226482394331597e-01) (9, 1.31681458724276889694e-01) (10, 2.49536902982111091776e-01) (11, -1.95156252490524817489e-01) (12, 4.11353249922602692834e-01) (13, 5.48028956826458690799e-01) (14, 3.63753119295678717382e-01) (15, -1.99319029574577644137e-01) (16, -2.07697594886009528903e-01) (17, 3.39243128196073939762e-01) (18, -1.92536184911911378848e-01) (19, 5.93545832519385974102e-01) (20, -6.01299251487813213313e-02) (21, 3.12100151630189337482e-01) 
