FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=17 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.43537301350681545387e+00) (1, 6.66049734293343131952e-01) (2, 1.64391704042034270117e+00) (3, 6.47161677265474022391e-01) (4, 8.57439334694387489400e-01) (5, 2.48647593843252245893e-01) (6, 5.20421282246781413683e-01) (7, 5.70759488287544303198e-01) (8, 3.06583156033649439909e-01) (9, 3.00677246600765724072e-01) (10, -7.12028942563793121323e-01) (11, 8.57152219120504432759e-01) (12, 1.38895088081283396564e-01) (13, 7.95257799773124940401e-01) (14, 9.41185878122944852286e-01) (15, 4.49737551205049768122e-01) (16, -8.63714199193194309689e-02) (0, -2.53284320746531577129e+00) (1, 2.15763180392513564954e+00) (2, 1.05468992712247944254e+00) (3, 8.48229243800525445351e-01) (4, 2.23068722794819684907e+00) (5, -3.74391183181840336847e-01) (6, 1.16193736822517323937e-01) (7, 2.05295640408359736373e-01) (8, 8.50347270380572561876e-02) (9, 5.28563080394150452967e-01) (10, -9.65981923071737580422e-01) (11, 1.81317587189580908813e+00) (12, -4.88857341745546125633e-01) (13, 5.94846720785011884303e-01) (14, 2.03347949951361961496e+00) (15, 4.91862823588725128765e-01) (16, -9.08260393991124459490e-01) (0, 2.31486542661458427617e+00) (1, -1.39050395657206893318e+00) (2, -5.18747869722581467222e-01) (3, -3.43680193941329537033e-01) (4, -1.15675399731601924458e+00) (5, 7.45684623822048919806e-01) (6, 1.53091746876812678790e-02) (7, 2.31548127648930693323e-02) (8, -5.33437680889987317356e-02) (9, -1.14064787762274777450e+00) (10, 1.16850388868235488538e+00) (11, -1.18428825121130198994e+00) (12, 2.81089688386206071335e-01) (13, -3.38792772717708634556e-01) (14, -1.33032778071286972299e+00) (15, -3.00677124711159593495e-01) (16, 1.36086816230161566921e+00) (17, 6.69373548959077857567e-01) (18, 2.54543577358891059959e+00) (19, -2.70809690731093617444e+01) (20, -5.90026434090124674015e-01) 
