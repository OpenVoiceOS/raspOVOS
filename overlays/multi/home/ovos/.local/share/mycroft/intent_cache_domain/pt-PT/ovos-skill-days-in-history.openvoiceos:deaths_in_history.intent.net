FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.07720012791836339261e+00) (1, 5.68190598700504811980e-01) (2, 6.47971371028881470977e-01) (3, 6.96020485673408906280e-01) (4, 5.97617642794113668181e-01) (5, -3.50258602032935961734e-01) (6, 1.93331599197525427769e+00) (7, 8.77616738011333108460e-01) (8, 4.34090302233504754525e+00) (9, -6.31616051578894488472e+00) (10, 1.67100615928012863343e+00) (0, 7.35666651429368489978e-01) (1, 4.57714850919585691802e-01) (2, 5.75884609388452095580e-01) (3, 4.71408701407056318633e-01) (4, 4.28268006908040455016e-01) (5, 2.76088118458744813388e-01) (6, 3.38142623009015963476e+00) (7, 6.09403623860833465464e-01) (8, 5.30566169862912762056e+00) (9, -4.10054694682285436613e+00) (10, 3.55614738436448973857e-01) (0, -4.72055903187171221891e-01) (1, -5.08182762409059796127e-01) (2, -4.81694383765070288650e-01) (3, -4.75373683715669848038e-01) (4, -5.83465257877914811324e-01) (5, 3.80787265770264671261e-01) (6, 6.95456988690420541133e+00) (7, -8.97729896932778353991e-02) (8, -1.08103667580904816425e+00) (9, 4.95407847297278214338e-01) (10, -2.05703149600220611126e-02) (0, 1.96280470572551690722e+00) (1, 3.83796091627377988775e-01) (2, 2.65054348360795333228e-01) (3, 3.24463907133836104713e-01) (4, 3.95863707613248350103e-01) (5, 1.24262233483911091469e-01) (6, -5.25537524161359304031e+00) (7, 1.14407404542845037021e+00) (8, 1.19942447507058180811e+00) (9, 1.89619820215612442382e+01) (10, -9.85800666087328913978e-02) (0, 4.94331768079503131030e-02) (1, 4.52515728180694840055e-02) (2, 2.30989361495285067116e-01) (3, 1.39904680043964446678e-01) (4, 1.09391572267322559342e-01) (5, 5.59958008692440074583e+01) (6, -5.28438733103183722051e+00) (7, 1.86223052092190005657e-01) (8, -5.66188430891204144690e-02) (9, 1.47234581714722839729e-01) (10, 6.07277922311442730496e-02) (0, -1.62586195015040790857e-01) (1, -1.86735386771979088028e-02) (2, -5.91303403301062235897e-02) (3, -5.89542831106009135311e-02) (4, -1.86022400371533863650e-01) (5, 5.82560460183210934715e-02) (6, 5.71851739877026332692e+00) (7, -2.67170935377232465768e-01) (8, -2.09324568684491923998e-01) (9, -5.78371959738993648981e-02) (10, -2.27020148058993571916e-02) (0, -8.37059130679962265509e-02) (1, 2.82131668596517608261e-02) (2, -8.08394286130705973931e-04) (3, -3.17701313228357096197e-02) (4, 2.24820014505636329993e-02) (5, -3.45077588340936380895e-01) (6, 5.53528918421233626646e+01) (7, -1.55205127725968089480e-01) (8, -9.74537470574262931766e-02) (9, -9.55784237257570057356e-02) (10, -7.44648878800243385623e-02) (0, -2.35469552472716614488e-01) (1, -6.52896763184613315723e-02) (2, -7.65374315767949550682e-03) (3, -6.62697107890194980762e-02) (4, -1.59917924682194650954e-01) (5, 1.97031059539243019740e-01) (6, 3.94188510982791839865e+00) (7, -2.60633749089787924991e-01) (8, -2.34649514132070108330e-01) (9, -1.33858971577193758273e-01) (10, -1.48790358366594904771e-01) (0, -1.49312401835692831753e-01) (1, 2.50537427931052325558e-01) (2, 1.72981354265433345807e-01) (3, 2.13836009471636862012e-01) (4, 1.08754898278933517797e-01) (5, 2.32952374437921050365e-01) (6, -4.73435251813696122980e+00) (7, 1.14505955836409323290e+00) (8, 1.11706830525074973792e+00) (9, 1.89463181518822700866e+01) (10, -2.41835032048905745894e-01) (0, -6.73740423469756152114e-03) (1, -1.83357064083287252254e-02) (2, 1.39628845795920341483e-02) (3, 6.26684549793532358342e-02) (4, 8.89813277468016472493e-02) (5, 1.07532703824347564137e+00) (6, 4.72216647909374032110e+00) (7, 7.19428834723999594339e-04) (8, 1.25424239066371429052e-01) (9, -1.69331597107604397978e-01) (10, 1.02395447309464343588e-01) (11, -2.51092971472567982882e-01) (12, -1.18755186474420487697e-01) (13, 4.00117256245805241743e-01) (14, 4.94562891250481140304e-01) (15, 9.47291611569304470297e-02) (16, 4.76803313474624346213e-01) (17, 3.66985381309416713869e-01) (18, 4.94943868890963756257e-01) (19, 4.99729168539871759780e-01) (20, 1.58814070704162285663e-02) (21, 2.92432270531472748853e-01) 
