FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.19135876972969168008e+00) (1, 4.38683682894132620511e-01) (2, 4.77567234849355704007e-01) (3, 4.96252769445798880277e-01) (4, 4.47714077150247580228e-01) (5, -5.39679813656431228708e+00) (6, -2.32008226444562148938e-01) (7, 5.42745110650745710146e-01) (8, 5.27506518214309405579e+00) (9, 3.54348385509682284322e-01) (10, 2.41687767616290472006e+00) (0, -1.16102639761994708145e+00) (1, -1.15605909827092581676e-01) (2, -1.57530438604691958027e-01) (3, -2.31869195106366610126e-01) (4, -2.29795612844565844135e-01) (5, 1.34934446873068192296e+00) (6, 8.83962726630021650154e-02) (7, -1.57549898414623273935e-01) (8, 2.24590377146010133913e+00) (9, -1.76565740551285571547e-01) (10, -1.41603827522159469998e-01) (0, 4.94270771203643322078e-01) (1, 8.00322579155641061766e-02) (2, 1.22754006267614887427e-01) (3, 2.39011904777117284437e-01) (4, 1.54391406656332552672e-01) (5, -4.87999199256925741963e+00) (6, 1.83817686665562596282e+00) (7, 6.48357665419350026603e-01) (8, -2.91061458626055724608e+00) (9, 1.99630447810678450515e-02) (10, 3.18675781579351669048e-01) (0, -9.90009891849706114009e-01) (1, -8.47578029420180956155e-02) (2, -1.72909339823941798908e-01) (3, -5.52154462125106632042e-02) (4, -5.73009680297179996300e-02) (5, 1.29144016856713750308e+00) (6, 6.64617837077653289546e-02) (7, -2.08465454229247421081e-01) (8, 2.68823480931321023846e+00) (9, -1.45567935253910846205e-01) (10, -1.09473310456634456700e-01) (0, 3.77999996109133018507e+00) (1, 7.02179509706561288951e-01) (2, 8.10216899521415401253e-01) (3, 7.23263155229155740855e-01) (4, 7.40971874005858621715e-01) (5, 7.90966732899588897965e+00) (6, -1.97701002340772064025e-01) (7, 1.25802911673325956698e+00) (8, -2.34712884778430286303e+00) (9, 6.58534172482554636119e-01) (10, -5.84842170372435243841e-01) (0, 4.36863981152918512407e+00) (1, 4.08106592268046464156e-01) (2, 4.52959594101008500289e-01) (3, 4.20406923413571442794e-01) (4, 4.18220800131854142379e-01) (5, -5.23492580493386494567e+00) (6, -4.85821956822014677790e-01) (7, 4.65435617092280817797e-01) (8, 4.98723496551715594904e+00) (9, 3.72728181500633914958e-01) (10, 2.85097610511717869386e+00) (0, 1.27977745269531140160e+00) (1, 5.41064153185149221414e-01) (2, 6.37671175172587534341e-01) (3, 4.95025451054831755471e-01) (4, 5.59330622425337820047e-01) (5, 7.94997991721076324723e+00) (6, -3.17572458516814648544e-01) (7, 1.49329251347223390667e+00) (8, -1.82819090720597698940e+00) (9, 9.27498666116238323909e-01) (10, -7.21893973448263781556e-02) (0, -1.05140841183493960465e+00) (1, -1.84335076228100691598e-01) (2, -7.76986041817254308173e-02) (3, -1.76739008143860731881e-01) (4, -2.15968896612841521065e-01) (5, 1.22585015280205289478e+00) (6, 2.00664631132011578307e-01) (7, -2.10990607387502776549e-01) (8, 2.45397363902704546135e+00) (9, -1.09493870048734120992e-01) (10, -1.32455401168228042996e-01) (0, -8.25261800827836733419e-01) (1, -7.98053474599518392063e-02) (2, -4.35396911079086665342e-02) (3, -2.58968503648438139186e-02) (4, -4.48634506637253169248e-02) (5, 9.94155570133514965292e-01) (6, 2.24094425923476475715e-01) (7, -2.60639531866359019929e-01) (8, 2.56212535080710379631e+00) (9, -1.42279188218376900776e-01) (10, -4.83670309296371017660e-02) (0, 1.29348455404037365746e+00) (1, 6.26291226765502195839e-01) (2, 6.45258959552634348000e-01) (3, 5.86704563519347410683e-01) (4, 6.74785945734370451454e-01) (5, 7.83584585140389755509e+00) (6, 2.13920268435735944079e-02) (7, 1.21785037979960164556e+00) (8, -1.98322171004562219920e+00) (9, 1.02872719866009498801e+00) (10, 5.49104593894494813489e-02) (11, -1.31848696045367502760e-01) (12, 4.96169814512923634808e-01) (13, -1.83473357927919134003e-01) (14, 5.21141284749701894086e-01) (15, 4.34992107652096282600e-01) (16, -1.22564214639155677489e-01) (17, 4.18695180571909963962e-01) (18, 5.38789158359959996503e-01) (19, 5.29677574341025581006e-01) (20, 4.03700056575207244514e-01) (21, 4.22160481441903767674e-01) 
