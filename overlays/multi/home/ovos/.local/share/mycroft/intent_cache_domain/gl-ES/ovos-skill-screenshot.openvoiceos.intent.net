FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 6.71186566481940527495e-01) (1, 3.79889353831403597805e-01) (2, 2.76031486411683790738e-01) (3, 2.94045373519056185696e-01) (4, 2.11142211874120466764e-01) (5, -7.76431242711858260463e-01) (6, -7.77701940094113641777e-01) (7, -1.34722380541407504850e-01) (8, -2.32889005131681781080e+00) (9, -3.44910102344551328102e-01) (10, -5.21288141702908047748e-02) (0, -1.32229608841512247253e-01) (1, -1.29267862468392696895e-01) (2, 6.08942672317911853908e-02) (3, 4.41726907795359363673e-02) (4, -8.83009605402062525137e-02) (5, -1.52107275754926729583e+00) (6, 6.79104510256139715096e-01) (7, 2.27911671069594423678e-01) (8, -7.35826393531425204664e-01) (9, 3.43351079164802730492e-01) (10, 2.93177243940261067490e-01) (0, -1.98147236635589613085e-01) (1, -3.87524646792272581264e-02) (2, -1.10033353892312418809e-01) (3, -1.12013114717946421495e-01) (4, -1.08247315612779032579e-01) (5, 7.55448006410691541568e-01) (6, -2.32993789802482087481e-01) (7, 5.36436274208250796214e-01) (8, 1.07122711971674416942e-01) (9, 4.42888799004915234558e-01) (10, 1.21471049943169345786e-01) (0, 7.55974419562591148924e-02) (1, 4.64713639631195597063e-03) (2, 4.21810417534343867763e-02) (3, 7.86606452526084631849e-02) (4, 4.36289130808345942958e-02) (5, 2.18191012367799253013e+00) (6, -2.36874394161581958151e-01) (7, -1.89642211937896904006e-01) (8, 2.10917131965391163595e-01) (9, -2.59568953488999387691e-01) (10, -4.02937623140614487038e-01) (0, 8.23212369150915113103e-01) (1, 3.73271271308057650540e-01) (2, 3.08589599748247012112e-01) (3, 2.42149095703952543790e-01) (4, 2.31062166114442579801e-01) (5, -7.22218128629044886502e-01) (6, -9.32261985459827635658e-01) (7, -6.88698278890485515724e-02) (8, -2.38771863195921829615e+00) (9, -3.42862137042248160679e-01) (10, 8.66742792505644787937e-02) (0, 8.24840668717972791413e-01) (1, 3.80040853937261446926e-01) (2, 2.86344445963495064511e-01) (3, 3.27335059364431246731e-01) (4, 2.53009065707319014127e-01) (5, -6.48458297140912631740e-01) (6, -9.98706309959579763280e-01) (7, -9.75160968760191565474e-02) (8, -2.52989692997928372620e+00) (9, -3.37681479729096067910e-01) (10, 9.11413344520949353367e-02) (0, 5.67111423939792524962e-01) (1, 3.35865890163133895552e-01) (2, 2.80914810973356521284e-01) (3, 4.35082828181932668166e-01) (4, 3.03544031012008941328e-01) (5, -3.09237242547254975733e+00) (6, 2.33543667574319657732e+00) (7, -1.63821350466967247117e-01) (8, 3.34080732488765175248e+00) (9, -3.07266953551356802876e-01) (10, -2.82265958670599115266e-01) (0, 6.05406506687728662719e-01) (1, 2.56625585444207160446e-01) (2, 2.96244554407830207321e-01) (3, 3.66987563855881604447e-01) (4, 3.01962513662810294601e-01) (5, -3.05414734745061799970e+00) (6, 2.87840604544829403721e-01) (7, -5.24645504733716183665e-02) (8, 6.62338076069029124682e+00) (9, -3.70996749386451607489e-01) (10, -2.85122168929884567401e-01) (0, -3.58456214816706197901e-01) (1, -1.35824484585569810191e-01) (2, -9.33388525832161142226e-02) (3, -2.26717931984709214488e-01) (4, -2.13503194807814072886e-01) (5, 2.81710039624367869493e+00) (6, 1.98167063425883194228e-01) (7, -2.05852854998009454635e-01) (8, -3.23670381046168742500e-01) (9, 2.08415055814427430114e-01) (10, 1.28874800591333771305e-01) (0, -3.41291112754727954393e-01) (1, 9.76169580916024792749e-02) (2, 6.25738079050638107415e-02) (3, 1.10195430586681519802e-01) (4, 6.82722801664926159937e-02) (5, 4.47332166946602383106e-01) (6, 1.74464921687664531458e-01) (7, 3.87223055294328544207e-01) (8, -9.99207379474685630782e-01) (9, 4.67849115553383376742e-01) (10, 4.09408959801430938352e-01) (11, -2.98713798355763671122e-01) (12, -4.31214998959909348586e-02) (13, 1.28218657029011223747e-01) (14, 5.46303422676044925232e-01) (15, -2.45236008149092798991e-01) (16, -2.50710724574034815593e-01) (17, 5.19995332616874028631e-01) (18, 5.36827110346825175924e-01) (19, 6.76508200716047447543e-01) (20, 2.34785056072856518350e-01) (21, 1.96158464017112649236e-01) 
