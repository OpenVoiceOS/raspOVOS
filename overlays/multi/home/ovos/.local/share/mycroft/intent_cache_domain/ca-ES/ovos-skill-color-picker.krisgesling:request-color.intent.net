FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.70718175361240220855e-01) (1, -1.14669206711299820345e-01) (2, -1.87287285652236110134e-02) (3, -1.32247838350780355654e-01) (4, -8.80112794304648872468e-03) (5, -2.67614815650777326805e-01) (6, 8.03444586080549055795e-02) (7, -1.00815000796819997930e+00) (8, -4.11961889268891390259e+00) (9, -1.56990086947345908186e+00) (10, -1.29204288564972374909e-01) (0, 1.44066225322254687358e-01) (1, 3.53974658835931932366e-01) (2, 3.60291261066003953850e-01) (3, 2.56526905883356359794e-01) (4, 2.33714375484987413323e-01) (5, 4.09909172315537528863e-01) (6, 4.24747667222129809961e-01) (7, 5.39603118915033652669e-01) (8, -2.25674834906525312306e+00) (9, 1.02252501626426650638e+00) (10, 2.62760194850530814925e-01) (0, 1.22531742530586643114e-01) (1, 3.17524655330237104423e-01) (2, 1.82502979952152866172e-01) (3, 1.54291419851835864829e-01) (4, 2.61587998437937452323e-01) (5, 2.88047919794107820657e-01) (6, 2.20177200037897563556e-01) (7, 4.92897098491111129714e-01) (8, -1.36383902809479540252e+00) (9, 9.22475157227860775855e-01) (10, 5.49068935807840788343e-02) (0, 5.21778114207916088318e-01) (1, -5.54301850068715726283e-02) (2, 2.20884643924089789369e-02) (3, -6.19661020055163993051e-04) (4, -5.70635310876516019252e-02) (5, -1.42648655056693163257e-01) (6, 2.67017441780545081165e-03) (7, -1.53450235869345669171e-01) (8, -4.32478582210523654350e-01) (9, -9.89464870444085042855e-02) (10, 5.01455281150484863395e-02) (0, 4.21141803453052354644e-01) (1, -5.39946973351576806066e-02) (2, -1.65811792763862847488e-01) (3, -1.80899728837404544235e-01) (4, -7.13772296933272432096e-02) (5, -2.62956395898870320949e-01) (6, 4.91384137602669454625e-02) (7, -3.65653150451921260533e-01) (8, -1.83562119554011693445e+00) (9, -2.62394472250693622239e-01) (10, -4.88660093145885932042e-01) (0, -9.95416936158921461519e-01) (1, 5.02947330402758518986e-01) (2, 5.31749691742089192203e-01) (3, 6.84987865316298516483e-01) (4, 5.31971733945038716129e-01) (5, 7.19448614121270813015e-01) (6, 8.96271730458050086554e-01) (7, 1.11464377228974487188e+00) (8, 5.30762052704681774884e+00) (9, 7.20788882756702387056e-01) (10, 1.03799098453237292894e+00) (0, 3.96346781470708786621e-01) (1, -5.34070841539052640345e-02) (2, -5.52849284875539456752e-02) (3, 9.72776674163195326228e-02) (4, 3.33237983715387597883e-02) (5, -2.33510766770335609355e-01) (6, -9.78207034080050101543e-03) (7, -2.03880708809552196881e-01) (8, -4.44306378908140120476e-01) (9, -1.16886730846846351173e-01) (10, 2.07300825811638944618e-01) (0, 2.70185689958103769648e-01) (1, 3.75731874097391282952e-01) (2, 3.17362125803037797844e-01) (3, 2.45848923761650267616e-01) (4, 3.51125616420313035881e-01) (5, 2.93585921038329311639e-01) (6, 3.71122798829185474023e-01) (7, 6.84811163424371205366e-01) (8, -2.22827202140755353810e+00) (9, 8.90381317631549484837e-01) (10, 2.40634920907583316207e-01) (0, 4.18827606108202243895e-01) (1, 1.41823498814471187640e-02) (2, 1.61521101584947840424e-01) (3, 2.28561705260641942483e-02) (4, -8.02382619091196284955e-03) (5, 3.40208197035279449882e-02) (6, -5.66065090512113330834e-02) (7, 1.35536317897890667972e-01) (8, -1.89330106906601924877e+00) (9, -2.19706638968604622830e-01) (10, 2.27579639909441744372e-01) (0, 3.24900281288318104611e-01) (1, 4.65222896152380979706e-02) (2, 1.23368486862743265142e-01) (3, 1.02989450197780496588e-01) (4, 1.20800942283237344732e-01) (5, 4.70873262249309088023e-01) (6, 3.85641305963653535649e-01) (7, 3.99144414328414465221e-01) (8, -8.83105640199959474046e+00) (9, 7.69738815021399291894e-01) (10, 6.38178606830804989736e-02) (11, 5.38935105561678895469e-01) (12, 3.97479754586739508770e-01) (13, 5.46785936577098063971e-01) (14, -4.62261425221625332771e-01) (15, 5.41445838215153352202e-01) (16, -2.12981290871079698146e-01) (17, -1.17599735809001715303e+00) (18, 5.28675363285319388496e-01) (19, -5.58659070305746463525e-01) (20, -9.74887018692908280393e-01) (21, 3.18132420865797460596e-01) 
