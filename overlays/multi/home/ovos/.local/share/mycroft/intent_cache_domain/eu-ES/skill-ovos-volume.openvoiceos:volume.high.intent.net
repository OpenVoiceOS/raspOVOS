FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.01821452319463778835e+00) (1, 4.04464150723884696692e-01) (2, 3.49870684501598527838e-01) (3, 3.70217504856060142249e-01) (4, 3.02407319602440105566e-01) (5, 5.97799228060900800230e-01) (6, -2.01380362879611418681e-01) (7, 3.30885581203566980779e-01) (8, -6.76663742552772506933e-01) (9, 7.88324532820018797352e-01) (10, 2.90685978436804048908e-01) (0, 1.12644944902095955896e+00) (1, 5.36327584994690909070e-01) (2, 5.36764628601925863904e-01) (3, 5.61694934380906230054e-01) (4, 6.11879303468125468335e-01) (5, 5.96887379507840787340e+00) (6, -2.08493958993437367155e+00) (7, 7.80908095150841363719e-01) (8, -2.30474677104366065805e+00) (9, 3.26388511999114783180e+00) (10, 2.91272415373722892706e-01) (0, 1.06183683689270180750e+00) (1, 3.72562746462295646399e-01) (2, 3.77612943705986137122e-01) (3, 3.22914506253669908453e-01) (4, 3.85575453695724601477e-01) (5, 8.51460161776731783156e-01) (6, -3.31247935217000410990e-01) (7, 3.75910036477470677063e-01) (8, -6.13026962767616256933e-01) (9, 7.54590114368232756092e-01) (10, 2.97998924458361014178e-01) (0, 7.24589063306650071894e-01) (1, 5.93434251602078799426e-02) (2, 2.12967050796308621230e-01) (3, 1.17462331032314418189e-01) (4, 2.26843161111631497207e-01) (5, -3.06205685493503507177e-02) (6, -1.65705343009031924595e+00) (7, -5.42908270762036890278e-02) (8, -1.82635351966236014754e+00) (9, 1.03483152358874487542e-02) (10, 3.57668915395581918371e-02) (0, 5.55015413542589097773e-01) (1, 1.66733508532800833724e-01) (2, 5.96940569336797122180e-02) (3, 2.10014996653356655898e-01) (4, 1.82606307392873867812e-01) (5, -9.31412215587650443860e-02) (6, -1.70197738380230578770e+00) (7, -3.21595926687787622700e-02) (8, -1.87688225831005950361e+00) (9, -9.91882468790665494662e-02) (10, 6.20504835252606926077e-02) (0, 1.21410206522379082728e+00) (1, 5.45315130961793070874e-01) (2, 5.86572437895196086011e-01) (3, 5.59043213141816264233e-01) (4, 5.82947283399956828198e-01) (5, 5.94475427638114606310e+00) (6, -2.07556187196257191374e+00) (7, 6.97974952369583734324e-01) (8, -2.08337337358477192595e+00) (9, 3.09283820732101233375e+00) (10, 2.37151226554314309425e-01) (0, 1.09207444863401210711e+01) (1, 3.08942767047423461069e-01) (2, 2.61259610020655730356e-01) (3, 4.10626957380313017953e-01) (4, 3.87957060896891692270e-01) (5, -3.97274016078749081515e+00) (6, -2.27090783279690899832e-01) (7, -3.28576075408943601985e+00) (8, -2.24735033916011961441e-01) (9, -3.27220905234031622655e+00) (10, 6.95389820149245330150e-01) (0, 1.21516080376389989937e+01) (1, 2.39287177330885225679e-01) (2, 3.57584508366022457704e-01) (3, 2.60250764197740902528e-01) (4, 2.73377517468366970643e-01) (5, -3.78225916650069482472e+00) (6, -1.95581033688340344412e-01) (7, -3.77431796937572450190e+00) (8, -1.99695832889828783774e-01) (9, -3.82750138341772050410e+00) (10, 3.21322960594782880417e-01) (0, 1.07772575506763956810e+00) (1, 1.55339390204034200060e-01) (2, 1.63101740346036305773e-01) (3, 3.21910575196824422228e-01) (4, 1.80589925662360539782e-01) (5, -6.98679201623271239896e-01) (6, -1.32830058588175488332e-02) (7, 4.18377469141970048661e-03) (8, -1.64928183209445244906e-01) (9, -2.95611061453623868989e-01) (10, 2.66250419212854860795e-01) (0, 1.12106545582255412441e+00) (1, 3.91815553364227409094e-01) (2, 3.15933692213962724615e-01) (3, 3.00606972482870327124e-01) (4, 2.56846147534321000983e-01) (5, 7.03285062242209546568e-01) (6, -1.99725052487231657938e-01) (7, 3.79782836415635538518e-01) (8, -6.85422272573486313085e-01) (9, 7.62861391319068937733e-01) (10, 3.19758494103288881671e-01) (11, 3.60640200089076268242e-01) (12, 3.61090277953399874100e-01) (13, 3.91364472220996129082e-01) (14, -8.08573620442387008822e-01) (15, -7.88703633520838165438e-01) (16, 3.31521218343033163833e-01) (17, -6.26187154670386880717e-01) (18, -4.77591248706990600947e-01) (19, -1.25271311130249141108e-01) (20, 3.76769246767619359062e-01) (21, 3.18948906565046541584e-01) 
