FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.85221236003699796235e-01) (1, -4.24575772561795461435e-02) (2, -1.21124178586221487031e-01) (3, -3.91180482187039602060e-02) (4, -9.03586190619237727972e-02) (5, 8.27298793136114346547e-02) (6, -2.05013756766895052785e-03) (7, 1.31230226133866434424e+00) (8, -2.53101256370442499744e-02) (9, -9.99938956631648556694e-02) (10, -1.42744982282426817466e-02) (0, 1.10305983025099130224e+00) (1, 5.03521723163830800529e-02) (2, 3.84492737663495176559e-02) (3, 1.35998630286525451494e-01) (4, 1.43372671068977108888e-01) (5, 2.02096957043028185863e-01) (6, 1.48556376961871322573e-01) (7, 2.16102544194625156848e-01) (8, 2.21616670862607889614e-01) (9, -1.97679685246235636864e-01) (10, 8.11472784370990268821e-02) (0, 8.30108078563095430624e-01) (1, 1.36737481774632629872e-01) (2, 6.73067617135688911478e-02) (3, 4.74023531096622388614e-02) (4, 4.79853014367267530216e-02) (5, -1.47790438694726078417e-01) (6, 1.47083710396536915255e+00) (7, -7.23618417951091874585e+00) (8, 1.07564703736736447248e-01) (9, 1.95564064257270409097e-01) (10, 4.42484625777010975867e-02) (0, -6.58590277088943021333e-01) (1, 1.39680073695450206184e-02) (2, -7.27634414000243728848e-02) (3, -1.49423701958152799207e-01) (4, -1.12226585195753084112e-01) (5, -4.27451166618639452621e-02) (6, -1.36541247838944973525e-01) (7, 1.28410591904921655981e+00) (8, -1.25699412512769087558e-01) (9, 2.42319890128147551189e-02) (10, -7.10002535621431368762e-02) (0, 8.12048180477160874702e-01) (1, 1.20573329056354053290e-01) (2, 8.12080486367659643010e-02) (3, 1.04080045844169147284e-01) (4, 1.47891590620131962996e-01) (5, -2.68825253154886700813e-01) (6, 2.68453466125214368354e-01) (7, -2.10203933188183089698e+00) (8, -3.71498292610840552541e-01) (9, -8.61320072646836881747e-02) (10, 3.90385795559769321894e-01) (0, 1.38127450213661995271e+00) (1, 1.90104499122960374224e-01) (2, 1.98247454724175736773e-01) (3, 1.69855869970185563433e-01) (4, 1.41775678506953523028e-01) (5, 1.33552402802887204558e+00) (6, 1.24931156859165426454e+00) (7, -5.77791711280592812372e-01) (8, 4.43047578345962755897e-01) (9, -3.35991694081939906713e-01) (10, -2.58870752074634113971e-01) (0, 4.53576844614352836516e+01) (1, 2.85117232128896669341e-01) (2, 3.06991868540563539458e-01) (3, 3.09470944926061586333e-01) (4, 2.03854911848821568343e-01) (5, -8.87100093196072059243e+00) (6, -2.67484322609009161908e+00) (7, 2.00834395705556767453e+00) (8, 2.35359648060919079970e-02) (9, -2.53472697104491384579e-01) (10, 1.30693717042439150333e+00) (0, 1.60697362487715000867e+00) (1, 1.35436130680228139500e-01) (2, 1.13468555726195255429e-01) (3, 2.87009095765734467687e-01) (4, 1.46550541736269829274e-01) (5, 1.52239712127628568084e+00) (6, 2.23346238991354040238e+00) (7, -1.21975715445241883472e+00) (8, 5.01045967351623655617e-01) (9, -1.40329702379320242489e-01) (10, -2.33897678871093056552e-01) (0, 1.09090943578976440342e+00) (1, 5.27505821238475058887e-02) (2, 1.11984098578544147284e-01) (3, 2.27566184724421999297e-01) (4, 1.20916540051551349433e-01) (5, -2.53909843754317632314e-01) (6, 2.81209649868691236030e-01) (7, -2.09202469357712050879e+00) (8, -3.62038951036287637297e-01) (9, -3.01764109965066279484e-02) (10, 3.97824127402191807246e-01) (0, 4.54436790775861396696e+01) (1, 3.06326327224720140752e-01) (2, 3.59753032525528093633e-01) (3, 2.68434685965526720342e-01) (4, 2.44753931959617532232e-01) (5, -1.01059789095520304869e+01) (6, -2.62008729995788947065e+00) (7, 4.36603952730020561379e-01) (8, 4.73898149979605151638e-03) (9, -1.47251879502182614345e+00) (10, 4.10661397156109764239e+00) (11, 1.33426794673642407085e-01) (12, 2.64305913555408222848e-01) (13, -3.89446603610794306327e-01) (14, 3.85351634811795129565e-01) (15, -4.14602524407931105621e-01) (16, 3.24367866019084405416e-01) (17, -2.10918782302302737497e-01) (18, 3.31573007242582262766e-01) (19, -4.47707235119926383327e-01) (20, -2.19037642725867676141e-01) (21, 6.57414700144050234698e-01) 
