FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.99951261872726021585e-01) (1, -7.22790263348053324544e-02) (2, 7.11658470577765794873e-02) (3, -7.89398304872940409505e-02) (4, 6.79373465723563524366e-02) (5, -3.33166139643867609266e+00) (6, 1.41826962437867302308e-02) (7, 7.85150465222943239052e-03) (8, -5.46764593141299570789e+00) (9, 2.27298668665869618710e-01) (10, 8.80538172206176000811e-02) (0, -2.72324505981353937045e-01) (1, -3.59108284860087043078e-02) (2, -1.39175592437310258198e-01) (3, -2.00593887343926469136e-01) (4, -1.36415889933629075337e-01) (5, 1.39954878464886323108e+00) (6, 1.50766477309656726780e-01) (7, -1.57675489626933923448e-01) (8, 1.81054418105740522194e+00) (9, -1.89718806010793966665e-01) (10, -8.46651927380991514704e-02) (0, -3.66601016160396864230e-01) (1, -7.39639536290598448298e-02) (2, -1.92602834179921189595e-01) (3, -2.67982021718455067083e-02) (4, -5.99239305406046446345e-02) (5, 1.18568516096656484393e+00) (6, 2.66091004334879477344e-01) (7, -5.40427750275629739662e-02) (8, 1.75995590109486554908e+00) (9, -4.34522930442772514770e-02) (10, -3.09035316853953009875e-02) (0, 2.89906286293941051202e-01) (1, -5.37380516283939638211e-02) (2, 5.74029110617210250322e-02) (3, 1.57048948354293511909e-02) (4, 1.13012666182674137438e-04) (5, -3.41766436096509096387e+00) (6, -1.18724721166849917636e-02) (7, 1.11364618806568241305e-01) (8, -5.57526704238873804798e+00) (9, 1.51111773295386220273e-01) (10, 1.01743737526823335715e-01) (0, 9.51824521242864651782e-01) (1, 2.19397013221286185658e-01) (2, 2.46368889842532523549e-01) (3, 2.74330196116469748890e-01) (4, 2.31287312839530356801e-01) (5, -1.27832014416850281435e+00) (6, 1.88648835629971228789e-01) (7, 4.07561172862993759125e-01) (8, -2.33781764540401848507e+00) (9, 4.10449128933836726585e-01) (10, 1.90936853323481925404e-01) (0, 1.77792288953738802926e-01) (1, -7.14310310535858500325e-02) (2, 5.92590593046714644854e-02) (3, 1.92536510772277520698e-02) (4, 5.57837955898811133082e-02) (5, -3.34045636724908945325e+00) (6, 9.63614044096707256237e-02) (7, 1.06515751151767826266e-01) (8, -5.46723197715503062000e+00) (9, 2.65548980874998952206e-01) (10, -3.92657496848808837409e-02) (0, 1.71872161919551430520e-01) (1, -6.03233590476940501057e-02) (2, 3.14319454378654133953e-02) (3, 1.16678126401473670343e-02) (4, -3.25274742894600191190e-02) (5, -3.34012220870931741956e+00) (6, 7.64693451431034953991e-02) (7, 2.53724393102276691778e-03) (8, -5.48403496610146845569e+00) (9, 3.41342612249834920224e-01) (10, 3.77678445657981254335e-02) (0, 1.04773504973259079520e+00) (1, 2.74095584784053214467e-01) (2, 3.39639839861892112172e-01) (3, 2.52392963651917767720e-01) (4, 2.23667209182284720814e-01) (5, -1.95511474389154815690e+00) (6, 1.63654676077863991956e-01) (7, 4.23730452676760238617e-01) (8, -3.11671846803783481050e+00) (9, 5.59610758312632405875e-01) (10, 2.91305680904887620564e-01) (0, 5.98026598010698462460e-01) (1, 2.00100086959290934807e-01) (2, 1.51653241696048268761e-01) (3, 1.79760381909299327141e-01) (4, 1.36129852714229088173e-01) (5, -2.75798292037637726715e-01) (6, -2.04096402765299028692e-01) (7, 2.60101601811337845849e-01) (8, -1.24233482306142972718e+00) (9, 1.98936453732859619414e-01) (10, 1.01813949974465772774e-01) (0, 4.93195242211132631183e-02) (1, 4.28856640026050339731e-01) (2, 3.26337820753532181772e-01) (3, 3.75814585849719773325e-01) (4, 3.44994335338550339731e-01) (5, 5.47490691536896179059e+00) (6, 1.77162737042494744522e-01) (7, -1.05293027532430683024e+00) (8, 7.84530296339391775007e+00) (9, 1.71985350198697262147e-01) (10, 2.51319712986397714172e-01) (11, -3.60891240030334903288e-01) (12, 4.01597986905304515926e-01) (13, 4.31694283899444253283e-01) (14, -3.52095593741245604935e-01) (15, -7.58981690555048521540e-02) (16, -2.69983736696123355348e-01) (17, -5.24825558433462346919e-01) (18, -8.97027388959360655329e-02) (19, -3.38198677449656134875e-02) (20, 4.44888243026923146939e-01) (21, 2.85782067939716222238e-01) 
