FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=17 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.48708928854560333832e+01) (1, 3.94500502932200569628e-01) (2, -4.68799963593655999450e-01) (3, -5.53060389555056897137e+00) (4, -9.67668316841100661918e+00) (5, 7.19181116478427862049e-02) (6, 9.34561866592025558553e-01) (7, -7.82105875391088223303e-01) (8, -1.36275164901829753950e-01) (9, 4.78303975973407968780e+00) (10, -1.92395028425870168176e-01) (11, -2.41025446353352379436e-01) (12, -7.41545527712681590282e-02) (13, -4.94966010122579846975e-01) (14, 3.56354908719832552677e-01) (15, -4.75146399248513373603e-01) (16, -1.08173569483743072794e+01) (0, -3.18103146448520357481e+00) (1, 2.15801962175391892629e+00) (2, 8.45168818147724487666e-01) (3, -1.10158538016394969006e+00) (4, -1.13480812274589926147e+00) (5, -2.76557542566688990826e-02) (6, 1.80194646759173471295e+00) (7, 1.23926785645612769216e-02) (8, 2.25225315973330753039e+00) (9, 2.31279274628579178952e+00) (10, 2.09543121411300026224e+00) (11, 1.60017679903128562380e+00) (12, 2.08552313383999665675e+00) (13, -5.15050580010024239286e-01) (14, 2.23468606058899910138e+00) (15, 2.23545137971884955164e+00) (16, -1.32005235362825645851e+00) (0, -2.69225907309893530694e+00) (1, 1.65639812172009959212e+00) (2, 3.75967019361120757814e-01) (3, -4.78259467163990037442e-01) (4, -2.93938406387279360654e-01) (5, 1.21735519824010346435e-01) (6, 7.66301809837067970932e-01) (7, 4.37268977555084203956e-02) (8, 9.90741487019280886450e-01) (9, 7.94402376412617905466e-01) (10, 9.79400670869205458580e-01) (11, 6.54538634007039488161e-01) (12, 9.49499812227869499637e-01) (13, -2.64735072888690539661e-02) (14, 1.07792539810356480245e+00) (15, 1.07542284661712872484e+00) (16, -4.11785050090694382252e-01) (17, 1.33428575785482150806e+00) (18, 3.48453613374689865623e+01) (19, 3.94494205969818922952e+00) (20, -1.06178647697785266679e+00) 
