FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 6.76225624748593046043e-01) (1, 2.31374636907863806545e-01) (2, 2.82046665121603146176e-01) (3, 1.90744016934919519146e-01) (4, 1.82737406958627890408e-01) (5, -6.16998824259966771599e-01) (6, -2.08509166258880629385e+00) (7, -8.16667239306701209234e-02) (8, 1.75377824393459058605e-01) (9, -2.52947537516014731018e-01) (10, 2.90775418170138244545e-01) (0, 1.23506573035858185916e-01) (1, 5.34572356104500157059e-02) (2, 1.92319405889476074956e-01) (3, 1.22656522011721808019e-01) (4, 1.78896454787219300053e-01) (5, 3.38113799657725810910e-01) (6, -4.70130331906281924148e+01) (7, -4.05400281636835058130e-01) (8, 1.64216324866830298834e-01) (9, 8.06124702287373828824e+00) (10, 1.43487768632976853134e-01) (0, -3.01080078931120320718e-01) (1, 4.46090529145341946338e-02) (2, 7.13041851700882550746e-02) (3, -7.32282322492092932649e-04) (4, -6.62497786401172045201e-02) (5, 2.59240384173374971422e-01) (6, 9.00190399423428710968e-01) (7, 6.71503845265527393726e-02) (8, -1.80621339338594588675e-01) (9, 2.91888617454005239171e-01) (10, -1.12594193459254962586e-01) (0, 3.07756940704267734077e-01) (1, 9.06133014495839939073e-01) (2, 8.14212348099222160691e-01) (3, 8.77114612396230675095e-01) (4, 8.37166543837060905808e-01) (5, 1.75890876802335421880e-01) (6, -6.78378088572958759528e+00) (7, -2.86645598978079729768e-01) (8, 2.00517937317932286589e+01) (9, 7.90393442255799669738e+01) (10, 5.77886909125443701263e-02) (0, -2.64930062127227972102e-01) (1, 7.26024302315831926435e-02) (2, 1.90436484885335780004e-02) (3, -3.33425683307527892518e-02) (4, 2.85285505128026889432e-02) (5, 4.17128741461475771590e-01) (6, 1.63044943152559240751e+00) (7, -9.56525602026948029710e-02) (8, -1.84156353455722454626e-01) (9, 2.24900416313730894258e-01) (10, -1.65284814991747616708e-02) (0, -3.23903889357899821277e-01) (1, -2.15482062008527236463e-01) (2, -2.05966634955552535535e-01) (3, -1.40598764922765295804e-01) (4, -3.11254098083051933266e-02) (5, 1.57421540083317185177e-01) (6, -4.02565645305581920610e+00) (7, -4.93258730032470893434e-01) (8, -8.38438394873182879508e+00) (9, 5.23341526681858670855e+00) (10, -6.35243432123011597668e-01) (0, 1.13021085315159797169e+00) (1, 4.97745260835737890481e-01) (2, 3.57627682627291332285e-01) (3, 4.03394475818724285165e-01) (4, 3.30972753704638134042e-01) (5, -4.77725318877379789839e-01) (6, 3.37699247414644920795e+00) (7, 1.55672657138609693561e+00) (8, 4.19630961324048890049e+00) (9, -3.76231376430318986692e+00) (10, 8.46020872254120370748e-01) (0, 4.83096990300293294140e-01) (1, 2.92190558551686352384e-01) (2, 1.92982333986895515654e-01) (3, 2.80587545870678967130e-01) (4, 2.55153305171864464018e-01) (5, -2.18373341822422339931e+00) (6, 5.28909120838763957551e+00) (7, -7.82314231563900452926e-01) (8, -1.66448679104033880893e-01) (9, -5.86994415409882908108e+00) (10, 1.38990422395795926169e-01) (0, 1.28039686281682407731e+00) (1, 4.11553781118508932302e-01) (2, 3.28700553264733907888e-01) (3, 4.05392299976464864919e-01) (4, 3.14368951704618093679e-01) (5, -1.91801168743564987018e+00) (6, 2.75183030686012886079e+00) (7, 9.78787196542852289127e-01) (8, 4.75996112142708938109e+00) (9, -3.96461645168141352258e+00) (10, 2.34143605621053074728e+00) (0, -3.04112853122196491462e-01) (1, -7.74841099070777972191e-02) (2, 4.92111728502044806777e-02) (3, -7.10819801561737504847e-03) (4, 1.52156040979156935433e-02) (5, -3.02126275431542401506e+00) (6, 9.19621833977451785813e+01) (7, -3.24147975983053449056e-01) (8, -6.15224979548023021003e-01) (9, -3.35746125747395984806e-01) (10, -1.30557926435319257563e-02) (11, -1.47136582474085980676e-01) (12, -1.83651156850707403256e-01) (13, 8.17404288873603390719e-01) (14, 8.97516471080312849828e-01) (15, 6.22739164365388342404e-01) (16, 4.02701758278733557006e-01) (17, -1.79507738684413803432e-01) (18, -1.28196349198761994082e-01) (19, -2.08898958548918184386e-01) (20, 7.94395713756047783249e-01) (21, 4.15723305397005638007e-01) 
