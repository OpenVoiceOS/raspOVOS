FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 9.31267145232478193995e-02) (1, 2.94329569148788716682e-01) (2, 2.07008880036840703376e-01) (3, 2.20483817565212514289e-01) (4, 1.76142432200680043586e-01) (5, -3.04500637188915734832e-01) (6, -5.50124169247525252047e+00) (7, 7.05084358690115764468e-01) (8, 8.31577627251286255472e-01) (9, 9.08695172923028025025e+01) (10, 1.37174915609339670430e-01) (0, 1.99574452436779403719e-01) (1, 1.09963497723086400804e+00) (2, 1.09321072901232763108e+00) (3, 1.04492726380570455369e+00) (4, 9.60046494552927343058e-01) (5, -7.24505812458039577528e-01) (6, -6.31817098883797445552e+00) (7, 7.81634968217396686185e+00) (8, 8.96181869372767114790e-01) (9, 9.08666866379200968140e+01) (10, -1.80795076137493904156e-01) (0, 1.36477241545508110043e-01) (1, 2.31216671072625376548e-02) (2, 3.69410859548234341010e-02) (3, -1.12393228209830033093e-02) (4, 1.63950194617428796073e-01) (5, -4.50940142416041189755e-01) (6, -1.52700316858622233696e+01) (7, 1.64185020264841857740e-01) (8, 1.08061515875773353490e-01) (9, -2.38799319675191495937e-01) (10, 2.00562207154591290448e-02) (0, -5.78212193825717646334e-01) (1, -5.24395052313161413871e-01) (2, -6.32676880776239403126e-01) (3, -6.34361799775911339161e-01) (4, -5.27683217048001806937e-01) (5, 5.12811938824854096630e-01) (6, 7.24154899079660197003e+00) (7, 6.02439817504992619623e-03) (8, -5.63639983169228031556e-01) (9, 2.57124422037782518480e-01) (10, 9.68292588569656526021e-02) (0, 1.05746239966963237467e-01) (1, 1.08288707042677900816e+00) (2, 1.05211503262265226866e+00) (3, 9.92164349356727925944e-01) (4, 1.09921577776415868577e+00) (5, -6.01789596048935004902e-01) (6, -6.71860613014721774761e+00) (7, 7.92126699235224673856e+00) (8, 1.00792069645230175645e+00) (9, 9.07050644042312654847e+01) (10, -3.60713383687078614770e-01) (0, 4.17219506511027835027e-02) (1, 4.89898164600529890250e-03) (2, 1.57572628453676005789e-01) (3, 7.12120025309959520099e-02) (4, 2.08501717898289251618e-02) (5, 2.02524280604302919784e+00) (6, -1.97947501227021689374e+02) (7, 6.70391758592087327884e-01) (8, 1.90179184109476873132e-01) (9, -1.50859695374118057964e-01) (10, 7.11633777614321672011e-02) (0, 8.19583236607090648285e-01) (1, 4.34020849583882095679e-01) (2, 3.65436623512047475959e-01) (3, 4.31326391814488174781e-01) (4, 5.26071938393849025672e-01) (5, -8.34485468145856890132e-03) (6, -4.51614442288969769379e+02) (7, 2.35241371230450413421e-01) (8, 8.15548489163826162418e-01) (9, -6.58869562567483746385e-01) (10, 3.49691720645740722340e-01) (0, 1.06214500782577520033e-02) (1, 1.35698467225107644030e-01) (2, 1.84391565352949593493e-01) (3, 1.63854114443335030504e-01) (4, 2.36136056214842293688e-01) (5, 6.57374616460791205697e-01) (6, 8.28105670381693137472e-01) (7, 6.51685298696113701061e-03) (8, -3.35070932504411445052e-01) (9, -4.35786727378748683925e-01) (10, 2.36156995472533437486e-02) (0, -2.56563092362575739802e-01) (1, 1.74864477893545318399e-02) (2, -8.07832681158350413009e-02) (3, -4.85089995482728825182e-02) (4, -3.90491687277124410738e-02) (5, 6.54279978818712582012e-02) (6, 4.40373668023944304650e+01) (7, -1.69536697845686895603e-01) (8, -4.19426619230146902062e-01) (9, 1.48803613954747837589e-01) (10, 3.63496470060439627603e-02) (0, 8.22928672742804123708e-01) (1, 5.40497148869812038541e-01) (2, 3.97915835021316444120e-01) (3, 3.99547839997589027128e-01) (4, 5.11906380294143703580e-01) (5, 7.45529389306257783687e-01) (6, 3.34962449837941411701e+00) (7, 1.14016693967821033939e-01) (8, 6.60379441892130625291e+00) (9, -6.18896370179638566356e+00) (10, 5.30062944205785036011e-01) (11, 4.54692865614809427210e-01) (12, 3.68086628995155207278e-01) (13, -9.06362242212975094402e-02) (14, 6.42610343118421911690e-01) (15, 5.03651703600801803340e-01) (16, -6.77285333043347870241e-02) (17, -1.83881802167707525397e-01) (18, 1.42292233883958979535e-01) (19, 6.76383570564700553618e-01) (20, -5.20186632256384062778e-01) (21, 2.03453631683087915460e-01) 
