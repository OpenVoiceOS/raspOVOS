FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.42819976003366946316e+01) (1, 2.65583312451852959768e-01) (2, 3.26375047057880451895e-01) (3, 2.28899190515293171622e-01) (4, 2.68096262902034865316e-01) (5, -7.88912025037311703812e+00) (6, 2.27617829356052947531e+00) (7, 1.15908659036259950592e-01) (8, -2.34708926949977109144e-01) (9, -2.97039685787052420984e-01) (10, 6.46579377743591310335e-01) (0, 1.70288801731974750453e+01) (1, 8.45259628838620935554e-01) (2, 8.31042296833120142097e-01) (3, 8.56290347522817407722e-01) (4, 8.59898648804746423835e-01) (5, 8.15395058694059926552e+00) (6, 2.22289859520500776568e+01) (7, 3.06458802049788037181e-01) (8, 1.59713723610838553846e+00) (9, -4.27786995809861736451e-01) (10, -8.73408277944191097086e-02) (0, 3.73103830804464138282e+01) (1, 2.14597933744598567962e-01) (2, 2.22936418657709356506e-01) (3, 3.41265206044126634399e-01) (4, 3.08014248316694383423e-01) (5, -7.66093984941874417416e+00) (6, 2.31756637062867776322e+00) (7, 2.86481565285117922848e-01) (8, -1.09957638573920735148e+00) (9, 3.60488696605320679822e-01) (10, 6.41456343518419846994e-01) (0, 4.50057683558811447710e+01) (1, 1.34543663748290603932e-01) (2, 1.65623328574683731373e-01) (3, 3.09054776021029986577e-01) (4, 1.59001643695619171437e-01) (5, -9.12411870429200533295e+00) (6, 2.65226269336426323520e+00) (7, 1.58603852533077044340e-01) (8, -9.62702375185410685887e-01) (9, 4.77189520306519843196e-01) (10, 1.37363252130829871511e+00) (0, 1.69008950161249309474e+01) (1, 8.54313604182325159186e-01) (2, 8.12685787326417719001e-01) (3, 8.19506369179807458991e-01) (4, 8.45384142941556726569e-01) (5, 8.21887542309935170692e+00) (6, 2.20420014204686758319e+01) (7, 4.57317032938631684580e-01) (8, 1.34641151640423761826e+00) (9, -4.21053808387823358395e-01) (10, -2.22834910960541421332e-01) (0, 2.22503851876250019481e-01) (1, -9.40230995879374337187e-02) (2, -1.35031680063983111628e-01) (3, -3.18480827377044112270e-02) (4, 4.69613275959290393802e-02) (5, -7.21890252030023926721e+00) (6, 2.86097947815055464460e+00) (7, -3.62650588506582161052e-01) (8, -1.73060431901094986529e+01) (9, 6.94145885403048279461e-02) (10, 1.12434168196575964838e-01) (0, -1.02781897924298526448e+00) (1, 3.21535232305714679946e-02) (2, -1.61748502252609047125e-03) (3, 2.89491253733822478944e-02) (4, 6.57640832305142197800e-02) (5, -1.81704506668437654193e-01) (6, -3.82959087640301731881e+01) (7, 7.35470601896829778710e-01) (8, 2.77588599958212556285e-01) (9, 3.29930077399001375760e-01) (10, 1.82618534548785071969e-01) (0, -3.35994018628643309476e-01) (1, -1.93513856523363364515e-03) (2, -1.33981808286836978628e-01) (3, -7.54485271989616262944e-02) (4, 2.15086414224775063220e-03) (5, -7.36116517281660343031e+00) (6, 3.79123072271363970032e+00) (7, -7.51113437818905910204e-02) (8, -3.85490414997141739306e+00) (9, -1.68463337128269725895e-02) (10, 1.58846408410684092338e-01) (0, -1.20660558255997285571e+00) (1, 5.37132310509869370652e-02) (2, -1.77659553646853617115e-02) (3, -3.84819929659178938675e-02) (4, 8.69350757600754857346e-03) (5, -2.95474357324567504790e-01) (6, -3.83002035543577221688e+01) (7, 2.85905076158768312045e-01) (8, 1.53885038834418452769e-01) (9, 2.47039485468635938714e-01) (10, 1.32772654144277191257e-01) (0, -5.53236946016719688402e+00) (1, -6.97646747167374181586e-02) (2, -1.64389436208751377189e-01) (3, -1.64296140038516697013e-01) (4, -5.51629225547577428657e-02) (5, -7.79743315879425247239e-01) (6, 3.92651184407580657876e+01) (7, -1.84037601996684896388e-01) (8, -3.44912907505801280550e-01) (9, -5.29112877257388758423e-01) (10, 7.44707703396124687245e-03) (11, -1.87382022288784505504e-01) (12, 4.61927043913525570140e-01) (13, -1.87865852317710468888e-01) (14, -5.14962459063741162524e-01) (15, 4.85711621760052669750e-01) (16, -8.46459533248745954470e-02) (17, -1.43786683172797802310e-01) (18, -2.34910624746373572491e-02) (19, -1.23450667503522362733e-01) (20, 5.70518564221795254632e-01) (21, 3.88153536692493084104e-01) 
