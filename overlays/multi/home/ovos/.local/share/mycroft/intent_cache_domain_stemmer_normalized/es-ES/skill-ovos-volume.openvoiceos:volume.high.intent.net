FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -2.54065617330201498625e-01) (1, 3.19381920645278455373e-02) (2, -2.59901527070584932241e-03) (3, 6.78285291442912580129e-02) (4, 6.90787663707774501898e-02) (5, 2.47205297010558211479e-01) (6, -1.32540381082574825200e+01) (7, 1.57753571286132010520e+00) (8, -4.14524283404842486256e-01) (9, 9.34125107194939907629e-01) (10, 1.80770291207792734023e-02) (0, 2.29061616441787296861e-01) (1, 3.23077789857900560255e-02) (2, 8.57223621919668415936e-02) (3, 2.39369622781789755106e-02) (4, 1.80613765268688216542e-03) (5, 1.95174889342683566262e-02) (6, 1.46027099125807535707e+00) (7, 7.88534113834841593471e-02) (8, -1.01828828608143615031e-02) (9, 1.49092322385835251985e-01) (10, -1.34949947135339809456e-01) (0, 2.69132518504954596494e+01) (1, 6.38675641450026199131e-01) (2, 5.90684291156866714267e-01) (3, 5.45988757344820996309e-01) (4, 7.00289357635119125156e-01) (5, -7.67455656152350318422e+00) (6, 5.91121990287752030468e-01) (7, -2.98976425782086163707e+00) (8, 2.69145232234173747976e-01) (9, -4.82982682370205418376e-01) (10, 1.56800777746140118651e+00) (0, -2.20200014143317623905e+00) (1, -2.45595760980984595401e-02) (2, 6.05465468369513828972e-03) (3, 2.75447033008196714987e-02) (4, 1.22787270661220873635e-01) (5, 1.15921716576404820742e+00) (6, -1.51427470171118958575e+01) (7, 7.58945362928265199187e-01) (8, -4.28957312410510827050e-01) (9, 1.98381256311023612682e+00) (10, -1.32612824798996279535e-02) (0, 2.84722365594256621169e-01) (1, 2.61996530212332094656e-02) (2, 1.38603888667990313849e-02) (3, 5.28759204067159940821e-02) (4, -6.21642244683010512796e-03) (5, -5.49796740983003653230e-03) (6, 3.42902656329119803758e+00) (7, -5.98872923369933030102e-02) (8, -1.64207466286680509038e-01) (9, -7.66952729665166255124e-02) (10, -1.39681914548602570392e-01) (0, 1.86086463608789323132e+01) (1, 2.36332544199514688055e-01) (2, 3.47143543026733780721e-01) (3, 2.61257638297367367208e-01) (4, 3.39327794573593521932e-01) (5, -4.67330844722819271908e+00) (6, 1.27415605018698707696e+00) (7, -1.33768747844531654678e+00) (8, 1.31430208349965993442e-01) (9, -4.03867597432557678960e-01) (10, 7.68288331478735431190e-01) (0, 7.58716155258054997246e-02) (1, -1.37463054300138130814e-02) (2, 5.80606767296007603152e-02) (3, 1.59398683285634834572e-01) (4, 1.08833753442685920998e-01) (5, 1.52890783954557268887e-01) (6, -1.37462421350143770837e+01) (7, 1.08875839477668345801e+00) (8, -1.17096730892790629186e-01) (9, 7.50055826226012856139e-01) (10, 9.28830655613447075947e-02) (0, 7.21621661908615652514e-01) (1, 1.34785965107549698949e-01) (2, 1.16784077160228649817e-01) (3, 2.50421084486116496404e-01) (4, 2.50256277643312541326e-01) (5, -3.02364242497269142262e+00) (6, 7.96993819886248955342e-01) (7, -9.08980375285348030090e-01) (8, 2.85070507130909589488e-01) (9, -1.94198825640910066603e+00) (10, 3.32710820642251992041e-01) (0, -1.55149888509771960443e-01) (1, 8.82953105856499886883e-02) (2, -5.42659612010398043069e-02) (3, -9.83016864965834125112e-02) (4, -1.32272692035117281351e-02) (5, 1.85175096013900530867e-01) (6, 1.62644940939544402347e+00) (7, 3.40471727063613727182e-02) (8, -8.21221836065399724003e-02) (9, 9.68523679142471033421e-02) (10, -1.20588994814288655300e-01) (0, 5.73352296629576869691e+01) (1, 2.20233106268866984934e+00) (2, 2.25722109182103558567e+00) (3, 2.19801277322991817087e+00) (4, 2.13034571362956537399e+00) (5, 3.30724063962018306029e+02) (6, 1.56324648137860133801e+01) (7, 2.76083639410511096912e+01) (8, 1.45307593243005115369e+01) (9, 4.60409063026106069039e-01) (10, 1.33355002227410279758e+00) (11, -1.03163671513834903637e-01) (12, 4.84737756857459622939e-01) (13, -9.97164520722208846948e-01) (14, -2.08441528905679102079e-01) (15, 4.74403327541829900049e-01) (16, -9.03391588746278867417e-02) (17, -9.94753003178859701583e-02) (18, -1.96325923151710601822e-01) (19, 3.67473724023599512822e-01) (20, 5.93186067051928667127e-01) (21, 4.10369554554547155689e-01) 
