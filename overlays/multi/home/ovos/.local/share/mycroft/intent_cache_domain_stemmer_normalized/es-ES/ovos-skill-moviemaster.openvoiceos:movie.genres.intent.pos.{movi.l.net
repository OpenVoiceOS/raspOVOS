FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=16 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (16, 6, 5.00000000000000000000e-01) (16, 6, 5.00000000000000000000e-01) (16, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.36418072510451438006e+00) (1, 1.94932205587976303818e+00) (2, -1.04492649944287502306e-01) (3, -5.84775145072994328999e-02) (4, 1.83682573860994624582e+00) (5, 1.63758516338915721677e-01) (6, 5.92829704700082893432e-01) (7, 5.39423626964651181659e-01) (8, 2.34869664069923794747e-01) (9, 2.06363109414179390910e+00) (10, 6.87792712486113955173e-01) (11, 1.26401752972212610970e+00) (12, 3.16005101761799633842e-01) (13, 1.80919130432659641672e-01) (14, -8.77235818441669756851e-01) (15, -1.26971240082950198591e+00) (0, -5.44123679030627371844e+00) (1, 1.91865982473247331797e+00) (2, 1.98941588935060337251e-01) (3, 4.25839754032270922934e-01) (4, 1.86796904355425352762e+00) (5, 2.21619732705206767820e-01) (6, 6.38533518133729938171e-01) (7, 5.81740931724868848285e-01) (8, 4.86793595980864968187e-01) (9, 2.02864525382120675090e+00) (10, 1.06189309886995886245e+00) (11, 1.34938693973005419124e+00) (12, 3.06816591284256756644e-01) (13, 9.72878831506315544519e-02) (14, -3.29187209561572768912e-01) (15, -1.60475904228850807520e+00) (0, -5.26263756681174399432e+00) (1, 1.99241377026193466904e+00) (2, 2.49788184252185657686e-01) (3, 5.14101276384966610067e-01) (4, 1.87860698998112196634e+00) (5, 3.28093253103346804700e-01) (6, 7.03463666556448208311e-01) (7, 7.07861830597482755145e-01) (8, 3.51642453578028035821e-01) (9, 1.89003842984039804875e+00) (10, 8.76636787670537431794e-01) (11, 1.37032257946909075130e+00) (12, 2.09385967216473234398e-01) (13, 3.42201973379198387537e-02) (14, -2.48481794104086078923e-01) (15, -1.57924572485252778797e+00) (16, 2.48623404915802881376e+00) (17, 1.33097247379284535640e+01) (18, 8.01963484393546011120e+00) (19, -2.32244964493009220519e+00) 
