FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=9 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.84688505806644320817e-01) (1, 7.39825387531081041237e-02) (2, 1.65658393972472284261e-01) (3, 2.05998065047816342199e-01) (4, 1.17054523997859080686e-01) (5, 3.07853323842278259193e-01) (6, -2.46368836570831994948e+00) (7, 4.26710077993996239609e-01) (8, 1.92125173383311337316e-01) (0, -1.57082214166120504029e-01) (1, -2.29911528250314762395e-04) (2, -1.71833695493361054973e-01) (3, -9.56145465594735732839e-02) (4, -1.65649557135721742229e-01) (5, 1.93690771481430573653e-01) (6, 9.07902780630940542395e-01) (7, 5.68240439428877253114e-01) (8, -2.37824895744888298310e-01) (0, 2.78768425711114764542e-01) (1, 6.13413696223059634605e-02) (2, 1.86228798800067013586e-01) (3, 1.35042401128367517416e-01) (4, 9.74914264910975297829e-02) (5, 7.50918777289119598883e-01) (6, -2.47851407487246255812e+00) (7, 2.57285559051163403854e-01) (8, 9.14331209891596635719e-02) (0, 1.10756722878127811249e+00) (1, 5.69122288566189293846e-01) (2, 6.33299071531849167016e-01) (3, 7.01662231426792404321e-01) (4, 7.30898220282154342797e-01) (5, 1.33383477588823157189e-01) (6, 2.19921209107582615871e+00) (7, 2.00036868262237266247e-01) (8, 1.50454257106231348295e+00) (0, 1.89734162139485657006e-01) (1, 3.43577330072929343352e-01) (2, 2.79164363702346873808e-01) (3, 3.16337158044387778411e-01) (4, 1.50585425158550251235e-01) (5, 1.34904743789395799303e-02) (6, 5.97675509007639682579e+00) (7, -5.35814949973604681688e-01) (8, 1.90633704895409233915e-01) (0, -8.76038290156878618475e-02) (1, 1.31645647464122576495e-03) (2, -7.37433672172037035786e-02) (3, -1.19140522204062043743e-01) (4, -1.42827694885155259685e-01) (5, 2.26172672829067777922e-01) (6, 8.20388648369664408300e-01) (7, 5.24350320829939264833e-01) (8, -2.52715492015449572083e-01) (0, -8.67762894775167659134e-01) (1, 4.69653157172527160879e-03) (2, -8.88036342682514516500e-03) (3, -2.07407042604890942683e-02) (4, -1.70726479612013398723e-01) (5, 2.80416550359165572814e-01) (6, 8.47458708742017008397e-01) (7, 5.44671317472051996766e-01) (8, -2.83464857702819927887e-01) (0, 3.47665684201438784928e-01) (1, 5.09730075531760196128e-02) (2, 1.14549497240141948118e-01) (3, 1.35082164877013299886e-01) (4, 7.74114524060049852272e-02) (5, 9.63994690682026478257e-01) (6, -2.51793345768305520949e+00) (7, -3.34451719774993688761e-02) (8, 1.17558123641566356077e-01) (0, 3.02642793702672330269e-01) (1, 3.29995249470283469329e-01) (2, 3.09849237164070090422e-01) (3, 2.22879451771785724867e-01) (4, 2.06481725981046665419e-01) (5, 4.57217232941350554265e-02) (6, 4.37212092732958979013e+00) (7, -5.03444621070406217456e-01) (8, 2.45686819786461480009e-01) (0, 2.76896708952973635842e-01) (1, 2.23996631502485860077e-01) (2, 2.15243622361994374481e-01) (3, 1.80907398163653004852e-01) (4, 1.44750062077618285583e-01) (5, -2.23152903913578948281e-01) (6, 5.45622794027632984637e+00) (7, -6.13899657233736517625e-01) (8, 1.79439296955498345243e-01) (9, -2.24909432119971663955e-01) (10, 5.55017350845030232875e-01) (11, -1.91905204094296816253e-01) (12, -1.99830390215539971388e-01) (13, 4.66396312359443343531e-01) (14, 5.86892283193758412807e-01) (15, 5.61321316890409871547e-01) (16, -2.08578873313313872817e-01) (17, 4.03126078787913955104e-01) (18, 4.12693428937068618190e-01) (19, 3.74540942851176894557e-01) 
