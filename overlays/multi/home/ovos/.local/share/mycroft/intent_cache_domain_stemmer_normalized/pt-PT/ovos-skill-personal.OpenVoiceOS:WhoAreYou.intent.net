FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.88584866067120437982e-01) (1, 3.25249013807849957081e-02) (2, 1.10340933254011147602e-01) (3, 4.88269132878856593205e-02) (4, -4.06097891423626011775e-02) (5, -1.05353010498353216740e+00) (6, 2.72966059297147733975e-01) (7, -9.00139473546271551285e-01) (8, 4.86674121226598255241e-01) (9, -1.25670752722303158411e-01) (10, 1.22730209992769401239e-01) (0, 4.49358729923077770962e+00) (1, 2.88448682040802206750e-01) (2, 2.98084517926803793664e-01) (3, 2.25198483080498029762e-01) (4, 1.61517852903238612328e-01) (5, 4.83353220377874936986e+00) (6, 4.99515288177465821562e-01) (7, -1.89352830860260024082e+00) (8, 3.29607911593315983101e-01) (9, -3.65300290206100469748e-03) (10, -2.71133836700547414811e-01) (0, -1.29880606933635722733e+00) (1, -1.13191482894157119388e-01) (2, -6.63464416787373501005e-02) (3, -6.81770344302403130676e-02) (4, -8.91195883080708461943e-02) (5, 8.92220050468641834307e-01) (6, -2.28297964737341013697e-01) (7, 9.52366626592858200873e-01) (8, 6.67898099424427682536e-02) (9, 8.26273154385063546279e-01) (10, -3.13281140535966590166e-02) (0, -1.52895461647550745532e+00) (1, -1.91814628712867446536e-01) (2, -2.35424687192189140139e-02) (3, -1.82506193444477993193e-02) (4, -7.10638661906468072083e-02) (5, 1.14569161380419926033e+00) (6, -2.21016736949031011861e-01) (7, 9.76142755480988388861e-01) (8, 1.03261191725928180851e-01) (9, 7.81946506268951790908e-01) (10, -9.95669908136025960665e-02) (0, 4.22596696894019518975e-01) (1, 1.33062027882898586961e-01) (2, 3.03178035210931950960e-01) (3, 3.23223256062830144320e-01) (4, 1.88330706697071387223e-01) (5, -1.75512714484923915137e+00) (6, 2.82337447344652381354e-01) (7, -4.73614110391186504678e-01) (8, 2.86559225379896143071e-01) (9, -3.92600440041239839850e-01) (10, 3.32368742980830800260e-01) (0, 4.61463624143906780972e+00) (1, 2.14696393623372172588e-01) (2, 2.23648020341416453594e-01) (3, 2.45147199645062485729e-01) (4, 2.85593779101391886943e-01) (5, 4.82237142030907239842e+00) (6, 5.08627206686472321806e-01) (7, -3.06767870060341230065e-01) (8, 1.99120238608715943718e-01) (9, -2.30572210517914155670e-01) (10, -3.62812930514880949140e-01) (0, -1.42214096217195629102e+00) (1, -6.73486192748295464661e-02) (2, -1.52163421087001510257e-01) (3, -9.04091795012700039091e-02) (4, -1.69004400298344709119e-02) (5, 1.09319970516118036841e+00) (6, -2.11544024302954214445e-01) (7, 9.20059515866024857367e-01) (8, 4.41597666146343603710e-02) (9, 7.51954968936417000869e-01) (10, -1.35262647947658687375e-01) (0, 3.34793883611851539328e+00) (1, 3.55834271123735612630e-01) (2, 4.49034232011167766530e-01) (3, 3.95886461546747336904e-01) (4, 4.85652241220800640065e-01) (5, -3.20347750068118886091e+00) (6, 1.32468455320363109884e-01) (7, -6.02232517991672988700e-01) (8, -1.47668608884175878160e-01) (9, -8.08300405080885298048e-01) (10, 3.45046732806018274786e-01) (0, -1.48910646765272258740e+00) (1, -2.94311095998036308108e-02) (2, -2.04868597261642165774e-01) (3, -6.51360650584446726485e-02) (4, -1.65050515256856628055e-01) (5, 1.00567915049942957495e+00) (6, -1.50651196494097250334e-01) (7, 3.73668743895132271504e-01) (8, -1.46114531750567711033e-02) (9, 8.43385260469887154677e-01) (10, -7.57886491030351217013e-02) (0, 3.43737531583159783288e-01) (1, 1.88916608838699484618e-01) (2, 2.58561551718376303466e-01) (3, 2.00199805347583859039e-01) (4, 2.79765233544967739654e-01) (5, -1.74430985549682215918e+00) (6, 3.40626408993593421393e-01) (7, -5.52148080210732139506e-01) (8, 3.72827441751432453465e-01) (9, -4.77250010327513796149e-01) (10, 2.03125261076562535489e-01) (11, -2.53594775222569096540e-02) (12, 6.47784229062510652497e-01) (13, 4.63465262205225547731e-01) (14, 5.47018804758121257592e-01) (15, -4.25855232423850843526e-02) (16, 5.82199681294639281859e-01) (17, 5.24785009726234541461e-01) (18, -3.62766236261085450732e-01) (19, 4.33148209006411155642e-01) (20, -9.03523316628047634680e-02) (21, 3.21733658942673195202e-01) 
