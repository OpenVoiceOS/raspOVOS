FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.93131996485306767219e-01) (1, 2.02614678180008306319e-01) (2, 1.07254779076366740798e-01) (3, 3.10540107248118424366e-02) (4, 1.02069115376263008788e-01) (5, 4.88314036270607043644e-01) (6, -2.61074252514461280228e+00) (7, 6.52654400081546848256e-01) (8, 9.63873523289416334059e-01) (9, 2.64892462267644042129e+01) (10, 1.29308656179895009597e-01) (0, -2.59556103031209295384e-01) (1, 7.28819677987554009890e-02) (2, -2.03551565846940805959e-02) (3, -4.62627140423795407687e-02) (4, 2.23796166567594850827e-03) (5, 1.03653887884128208252e-02) (6, 1.86091182108044620236e+02) (7, -4.93225361216483304538e-02) (8, -3.06132267363985355324e-01) (9, -3.43187085201589062514e-03) (10, 3.65877318115145830357e-02) (0, 2.69401083145151931664e-01) (1, -4.00545043422023183677e-01) (2, -3.38654880894461995933e-01) (3, -3.18169777764121364250e-01) (4, -3.57093682063857498221e-01) (5, 1.97388185326698578637e-01) (6, -2.85652333963520455029e+00) (7, -3.72377777249651487068e-01) (8, -3.63721007800919426955e-01) (9, -9.12165206535995864989e-01) (10, -3.25840285923154782122e-01) (0, 4.39790399617442506752e+00) (1, 2.61493945065257860083e-01) (2, 2.67158919516322923560e-01) (3, 2.93805995527026908576e-01) (4, 2.23885008636234256185e-01) (5, 6.08412983104467453010e-01) (6, -4.77052691974717646417e+00) (7, 1.10196665044993458160e+00) (8, 1.52798007046338435444e+00) (9, 1.47106296676331197659e+02) (10, 1.42514801912786204241e-01) (0, 1.51111484457051314934e+00) (1, 2.47740488090811283861e-01) (2, 2.89679269829092811417e-01) (3, 1.47420540043411762987e-01) (4, 1.90085161068735630785e-01) (5, 8.87707453110197081125e-02) (6, -4.17992306435695137878e+00) (7, 1.62581279868034767055e+00) (8, 1.05089717016832100427e+01) (9, 6.47272875010737749335e+01) (10, 1.59981303868699659132e-01) (0, 1.02519297753369564852e+00) (1, 6.75005293358593050357e-01) (2, 6.70085071493415895816e-01) (3, 6.58192790020732942935e-01) (4, 7.28351793159275118228e-01) (5, -2.74621821313339620207e-01) (6, -4.00548493822788398422e-01) (7, 4.73969004052825770845e-01) (8, 2.15896637844330285816e+00) (9, -5.93775214305063503417e+00) (10, 3.54686494935723661825e+00) (0, 2.12954382754306115189e-01) (1, 1.62658871432035889715e-01) (2, 6.80548486311134820381e-02) (3, 6.57748703856167460380e-02) (4, 1.98666916509359803289e-01) (5, 6.42629272121766792480e-01) (6, -3.00711668653030939069e+00) (7, 5.54845052977482211531e-01) (8, 9.11185324782393668563e-01) (9, 2.66048704608447188491e+01) (10, 6.28200848421267277200e-02) (0, 1.35678785174762256416e-01) (1, 2.79152273937766481904e-01) (2, 4.01442922457759310273e-01) (3, 3.41664254471366279908e-01) (4, 3.50744485661094063111e-01) (5, -1.12596724927369259972e-01) (6, -4.53757531305780048569e+00) (7, 1.90942619941008917550e-01) (8, 6.72133055374688065386e-02) (9, 9.60476458798078108758e-02) (10, 2.39037774149567561199e-01) (0, 4.32980955226824726356e-01) (1, 2.73922815915634276784e-01) (2, 1.42928954210569308936e-01) (3, 2.50263467427780106345e-01) (4, 1.27514809366752551734e-01) (5, 5.06194051345813855747e-01) (6, -4.09849678510214499028e+00) (7, 4.73679755219252263121e-01) (8, 8.38364489966891501815e-01) (9, 2.65091617702610129470e+01) (10, 1.77977080233458040404e-01) (0, 1.49350525899406427754e+00) (1, 4.56015825953155429229e-01) (2, 4.72344175662666232451e-01) (3, 3.91060475941568397307e-01) (4, 5.20719558682113503600e-01) (5, -1.04597533415825250103e+00) (6, 5.05239374180643241452e-01) (7, 8.15871495502275556611e-01) (8, 4.67663699000548316320e+00) (9, -6.86213696828045716103e+00) (10, 2.86433609266862987397e+00) (11, 2.72058594241120887958e-01) (12, 1.91855728348522802662e+00) (13, 6.55362000037530423313e-01) (14, 2.00951551677777645599e-01) (15, 2.19759027674176543599e-01) (16, -1.89252783387386880332e-01) (17, 2.16010342552640288716e-01) (18, -4.06951696209527979953e-01) (19, 2.22071192943933404251e-01) (20, -1.87257789840392707692e-01) (21, 3.84688089401305111092e-01) 
