FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.38935655134821689494e-02) (1, 4.36610792831940153214e-01) (2, 5.00785489635032887712e-01) (3, 5.10663730160755280352e-01) (4, 4.94466093437237297348e-01) (5, 2.90379630464360971587e+00) (6, 4.76019422025266036869e-01) (7, 5.58768588317066494664e-02) (8, 5.95038050218756220389e-01) (9, 1.44268975307620834769e+01) (10, 1.19814628514615800770e-01) (0, -9.64749531016934946948e-01) (1, -1.30719021322706185906e-01) (2, -1.62554577352979651161e-01) (3, -2.41292104485013952919e-01) (4, -2.44753081711509695717e-01) (5, 2.35261795023025221951e+00) (6, 9.81227722437046610826e-03) (7, 4.43297651238149237329e-01) (8, -2.30824630059199564780e-01) (9, 1.50329846891998775149e+00) (10, -6.93954094628075224271e-01) (0, -6.04512192760232713873e-01) (1, -1.42807866491357543515e-01) (2, -9.51056881323257968797e-02) (3, -1.76285597540418392803e-01) (4, -1.82622793175260311749e-01) (5, 2.82111352509183532788e+00) (6, 1.89750776626169959105e-02) (7, 4.99230135805314612085e-01) (8, 3.69495048882775706245e-02) (9, 9.87703322068934519429e-01) (10, -8.20551834384182554594e-01) (0, -5.49962322651342053192e-02) (1, -1.52703521399047553730e-01) (2, -9.50922720117833575326e-02) (3, -3.43416161818768661540e-02) (4, -1.51923721282031715107e-01) (5, -5.93945646869552090763e-01) (6, 3.49264001101918641207e-03) (7, 6.74996267775733183036e-01) (8, 9.56821381921367231094e-02) (9, -2.36543164315452036917e+00) (10, 2.74411770170527938295e-02) (0, 8.05178332108164585001e-02) (1, 3.96577409894000754154e-01) (2, 5.50613767892848549046e-01) (3, 4.32158171624671683109e-01) (4, 4.12131025881301626956e-01) (5, 2.89171114254043359892e+00) (6, 5.10198394269528732181e-01) (7, -2.40275111781924691212e-02) (8, 5.36245990727937993370e-01) (9, 1.44864106153525185050e+01) (10, 5.00873827963754861270e-02) (0, 5.06377450898919612143e-01) (1, 3.91049838716197417643e-01) (2, 4.71308327371287749674e-01) (3, 5.02657181674647679515e-01) (4, 4.28054354244876311686e-01) (5, -5.04547077140354538471e+00) (6, 3.75910746540406626170e-01) (7, 2.67978699557139883325e+01) (8, 9.27680995018503595784e-02) (9, 3.57501963880124840056e-01) (10, 2.70672347689938586068e-01) (0, -1.00582643704234464543e+00) (1, -4.00143694517693160329e-01) (2, -4.26042672631821273121e-01) (3, -4.35609158513626693043e-01) (4, -5.21912809846481917653e-01) (5, 2.20198886900733814542e+00) (6, 3.46170280594971790933e-01) (7, 8.97643708947096219397e-01) (8, -4.51374986950814488518e-02) (9, 3.34783832631627431553e+00) (10, -6.36635496596077543607e-01) (0, 3.32017871140157308507e-02) (1, -1.29990127629923707397e-01) (2, -1.87027302331614297959e-01) (3, -1.80906523711371225449e-01) (4, -9.50313627192257426302e-02) (5, -4.30033231085602318799e-01) (6, -1.86908117905192766584e-02) (7, 6.15069303433139036308e-01) (8, 7.14017686010404856178e-02) (9, -2.41047254072878436304e+00) (10, 7.13218259280520572307e-02) (0, 2.85573782919561029725e-02) (1, -1.40539888984846955688e-01) (2, -1.39871139771628266724e-01) (3, -1.42735344118761875443e-01) (4, -1.96921271032976907822e-01) (5, -4.35652034288708245313e-01) (6, -6.43017872646384769375e-02) (7, 5.39823304633338052483e-01) (8, 9.92263899610788396854e-02) (9, -2.44545027854297236658e+00) (10, 5.90656804507571769891e-02) (0, 1.05199631492813905354e+00) (1, 4.78796036688562687278e-01) (2, 4.90485550610300358176e-01) (3, 4.93547336904283817649e-01) (4, 3.73911538867231663108e-01) (5, -5.04786850235008621723e+00) (6, 5.03751001215798188682e-01) (7, 2.68230889901175508783e+01) (8, 1.42508828042632595956e-01) (9, -2.40040356693848988545e-01) (10, 3.42272367621731854470e-01) (11, -1.07608245378966710382e-01) (12, 5.26126079914479904964e-01) (13, 4.94930192889203424134e-01) (14, -1.11518278725069780455e-01) (15, -1.07331515914435765557e-01) (16, 4.75215194744279256778e-01) (17, 4.81701474196724377386e-01) (18, -1.88277184077924208960e-01) (19, -8.25627148222148610213e-02) (20, 5.03112969917466457126e-01) (21, 3.93153771109099325898e-01) 
