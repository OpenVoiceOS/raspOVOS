FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 9.92489595565171911495e-01) (1, 1.68699571132444631205e-01) (2, 2.51288415133737841334e-01) (3, 9.34052702186339844914e-02) (4, 2.44945944487833272563e-01) (5, 3.12148188295491957334e-01) (6, -1.80362066860306269334e+00) (7, 1.63169894371699708557e+00) (8, 1.00428682951428163705e-01) (9, 1.30977637413959779877e+00) (10, 1.83199037404756542857e-01) (0, 1.86346508683284195484e+00) (1, 2.17554514385918718045e-01) (2, 2.69325663425187211697e-01) (3, 1.51763823904255995556e-01) (4, 1.83306095279912095730e-01) (5, 7.64948885814885737311e-01) (6, -1.78162989254105097459e+00) (7, 2.96489459664264609629e+00) (8, 1.60367675489237526687e-01) (9, 1.33022232901334103161e+00) (10, 9.14121721947834531274e-02) (0, 1.06605513355149200683e-01) (1, 1.27448585502893702026e-01) (2, 3.26351194308111686171e-02) (3, -5.95834360792329084666e-02) (4, -6.54012648007173784392e-03) (5, 1.09591795443840830937e-01) (6, 2.90780486380426084203e-01) (7, -1.52503665770594258599e-02) (8, -2.07033583292218041627e-02) (9, 2.13757281895469247734e-01) (10, -7.83619340415409593170e-02) (0, -1.08809923784197337682e+00) (1, -1.53306057999097605782e-01) (2, -1.15106826970540865318e-01) (3, -7.07725867128008445661e-02) (4, -5.69845124578112413594e-02) (5, 3.97069151279617227335e-01) (6, 1.52361661265939485332e+00) (7, 2.44601555234885276180e-01) (8, 2.90830844878068262016e-01) (9, 2.82292894007158345904e-01) (10, -5.68803865944186415327e-01) (0, 4.67373417076875607989e+00) (1, 4.45109091224514252794e-01) (2, 3.22425707700096375596e-01) (3, 3.20014655115448243272e-01) (4, 3.63421417298160798204e-01) (5, 1.35583891273149479417e+01) (6, -3.07797825042373407456e+00) (7, 1.60482668289878183110e+00) (8, 6.47312337451720121173e-01) (9, 1.13149710102379974508e+00) (10, -1.37475403649893102020e+00) (0, 9.17017049901308567428e+00) (1, 3.53747714434258675453e-01) (2, 2.87442796055667137978e-01) (3, 2.51994017634980582709e-01) (4, 2.94488079969279503700e-01) (5, -2.92711036796950585526e+00) (6, -4.59013022458759856193e+00) (7, 6.73789359163505241490e-02) (8, -9.78269688915982982280e-01) (9, 6.04810802474138098361e-01) (10, 5.13235800202630709776e-01) (0, 5.99892524878379185260e-01) (1, -1.47890597552407077053e-01) (2, -9.77103409205706298357e-03) (3, -1.07049494687202727261e-02) (4, -6.18737933074267436317e-03) (5, 3.01229877409147882295e-02) (6, 2.21821361501470581956e-01) (7, -1.43906181322210113827e-01) (8, -1.36990031711469398973e-02) (9, 2.90256129065598299233e-02) (10, -1.23543331021211774678e-01) (0, -1.56887290140103896618e+00) (1, 4.67166523036821809689e-02) (2, 1.32077045231805662029e-01) (3, 3.44251790617546796908e-03) (4, 4.16225636896952119748e-02) (5, 2.51226979459619303992e-01) (6, 5.16061106027692773068e+00) (7, 1.47455177415553939646e+00) (8, 3.93742235924658900981e-01) (9, 1.73653731325420584986e-01) (10, 1.11815720856181716192e-01) (0, 1.19791391942973768359e+00) (1, -1.91432755638241886009e-01) (2, -1.19274042744517430603e-01) (3, -8.67360906790495406193e-02) (4, -1.30030222434878439675e-01) (5, -9.96973605198358051038e-03) (6, 2.59341055740627757231e+00) (7, 5.19953458068145191184e-01) (8, -6.23692545041601362210e-02) (9, 1.08125820233417391236e+00) (10, -6.94603128470376307568e-02) (0, 1.16901310198136627605e+01) (1, 3.00452016417376732704e-01) (2, 4.15357139084927773354e-01) (3, 3.27222954605214333412e-01) (4, 3.76529824112050270912e-01) (5, -2.19739568977900745494e+00) (6, -4.60107945934890150141e+00) (7, 9.57868543304976338693e-02) (8, -1.88676403192425512501e+00) (9, 4.04950499346964642999e-01) (10, 7.85580236823706479221e-01) (11, 3.31992416877153084798e-01) (12, 3.13892401058080361409e-01) (13, 3.22909381394586192382e-01) (14, 2.76799191401054300865e-01) (15, 4.51366080002223668544e-01) (16, -7.53852072239719617208e-01) (17, 8.32107173691647578062e-02) (18, -3.04625943957249845084e-01) (19, 3.99887475468760980601e-02) (20, -8.83632275770349639998e-01) (21, 6.51218353210765488193e-01) 
