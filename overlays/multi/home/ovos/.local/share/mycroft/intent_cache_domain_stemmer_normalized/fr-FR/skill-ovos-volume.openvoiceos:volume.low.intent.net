FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.68415952192478179938e+01) (1, 3.89513610397082710168e-01) (2, 5.17542450223666405940e-01) (3, 4.06411653850776100061e-01) (4, 5.34864185844164952144e-01) (5, -8.47453663044768545909e-01) (6, -4.18661658827029903307e+00) (7, -1.48226686846191135771e+00) (8, 1.52592462357304781140e-01) (9, -1.57520579505976066059e+00) (10, 1.18865991018708405669e+00) (0, -6.95114955064108919203e-01) (1, -3.33030659178950236288e-02) (2, -1.58502987347052287648e-01) (3, -1.45530092934824974160e-02) (4, -6.36929410914637422758e-02) (5, 7.63247339456956885861e-01) (6, 1.20624961049143580860e-01) (7, 1.90143641573666893896e-01) (8, 6.60077616600237782052e-01) (9, 4.96851221226609629511e-02) (10, -3.62175731979410775452e-01) (0, 1.01542859967350174166e+00) (1, 7.61382207932263660455e-02) (2, 1.73478379911401858759e-01) (3, 5.59794270159989573732e-02) (4, 5.18940874101906854388e-02) (5, -7.68552510147125556017e+00) (6, 2.35181290429837097644e-01) (7, 1.47394605918919729959e-01) (8, 6.13710180968867979878e-01) (9, 7.33085025083681340874e-01) (10, 1.74571843455679054369e-01) (0, 1.02120771228933793218e+00) (1, 7.13090195040998880982e-02) (2, 4.74323375802336369422e-02) (3, 1.85603496311217364134e-01) (4, 1.50518876193076189818e-01) (5, -8.29472381386581680829e+00) (6, 2.52281417943881203225e-01) (7, 1.71963168562011076146e-01) (8, 7.60514744670020670370e-01) (9, 1.77036010057339976420e+00) (10, 2.71391651596249949829e-01) (0, 2.10124608685998559565e+01) (1, 5.79975367181910783643e-01) (2, 5.93354970806254655713e-01) (3, 5.29774852686538011426e-01) (4, 5.25684692555083654675e-01) (5, 6.83485998790773585654e+00) (6, 1.45508322241982330780e+01) (7, 4.28741577897422576626e+00) (8, -3.55273142139826525465e-01) (9, 5.92787927434104897073e-02) (10, 9.17718525027645254388e-02) (0, 2.09736721178321481318e+01) (1, 6.27293915146006852979e-01) (2, 5.26499741368903539929e-01) (3, 5.14480360441817552442e-01) (4, 4.44794685238017295514e-01) (5, 6.88964470308336451865e+00) (6, 1.46279630187233795624e+01) (7, 4.13423234481566126419e+00) (8, -5.02993322591696601087e-01) (9, -5.23122557952742026077e-02) (10, 1.41774695644320353205e-01) (0, 1.69031225981049537666e+01) (1, 3.95143067930442237756e-01) (2, 4.24943497870665931604e-01) (3, 4.18196412269574546716e-01) (4, 4.10039851908189201257e-01) (5, -9.08480451567331592777e-01) (6, -3.76472197174696621147e+00) (7, -1.53622729673000857353e+00) (8, 2.48450008430739707754e-02) (9, -1.54901420284326896137e+00) (10, 1.22289113335911259206e+00) (0, 7.38574312917037123505e-01) (1, 1.17018426040000803057e-01) (2, 1.11498723009414560381e-01) (3, 1.00821675935573465410e-01) (4, 1.32269600609130733027e-01) (5, -1.02325314660889112872e+01) (6, 2.26465112670440810172e-01) (7, 1.25512618540897180619e-01) (8, 6.74568504381843347950e-01) (9, 3.24557097337588773556e+00) (10, 1.49053727022387449974e-01) (0, 6.07814365915103071991e-01) (1, 1.04503432491607539667e-01) (2, 1.06656225601024501337e-01) (3, 2.24846454540080897821e-01) (4, 1.96885394135303370966e-01) (5, -1.06029103328090847924e+01) (6, 2.72779519595280850464e-01) (7, 9.56095913522313767263e-02) (8, 6.81806717263978456778e-01) (9, 3.15612482360944257564e+00) (10, 1.28126972100712749336e-01) (0, 2.09172573646335244746e+01) (1, 5.67808732979907304639e-01) (2, 5.00010021620406419629e-01) (3, 4.61378269487036918317e-01) (4, 5.68910673850192338818e-01) (5, 6.83146030288252070761e+00) (6, 1.45675184476455559235e+01) (7, 4.20801637206428313931e+00) (8, -2.77422940370943671429e-01) (9, 9.75138610646386350123e-02) (10, 2.30390634665430887873e-01) (11, -2.18081962030875736724e-01) (12, 6.77285739841784972803e-01) (13, -1.21800146838021788875e-01) (14, -3.17593305189619479201e-01) (15, 3.12539148338717265307e-01) (16, 3.58054603644293589770e-01) (17, -3.78370468934800030514e-01) (18, -1.33984464500361943884e-01) (19, -2.16630543861800667349e-01) (20, 4.01596193001869383732e-01) (21, 3.46097613651200064488e-01) 
