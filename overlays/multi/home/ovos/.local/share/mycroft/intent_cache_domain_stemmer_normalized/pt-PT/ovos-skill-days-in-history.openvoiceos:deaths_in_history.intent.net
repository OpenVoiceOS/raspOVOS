FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.03426951548501722122e+00) (1, 4.00227167503293568274e-01) (2, 2.83519823895152622839e-01) (3, 2.90250324503835255285e-01) (4, 3.05294380382951313635e-01) (5, -1.70499928758429189490e-03) (6, -4.28728838407103385322e+00) (7, 4.99617347718413373769e-01) (8, 3.94122687775959046341e-01) (9, 3.94523124279316084184e+00) (10, 5.02707606727947009517e-01) (0, 3.74341700460871174005e-01) (1, 3.49909596936162525793e-01) (2, 2.64114555762466118871e-01) (3, 3.74451377288754994055e-01) (4, 2.31046126083787523031e-01) (5, 1.63016279509560851890e-01) (6, -3.44108992059802387686e+00) (7, 4.85204688436225661796e-01) (8, 2.95203598289405344524e-01) (9, 2.98121809052070441126e+00) (10, 4.85478988463748706295e-01) (0, -7.10681647512599989902e-01) (1, -1.81445532838311829327e-01) (2, -1.44715228477922175365e-01) (3, -8.68219299115807752187e-02) (4, -8.82455272473962049062e-02) (5, 5.08586273064956850831e-01) (6, 4.06571751016040128235e+00) (7, -4.44872418805462410685e-01) (8, -7.19568209606866404648e-01) (9, -1.43168607155290339428e-01) (10, -4.81837907949506871752e-01) (0, -6.01433747635363280537e-01) (1, 1.58451233307652268600e-02) (2, 2.45147977272801090298e-02) (3, -1.32573243578261917852e-01) (4, 4.17613569895558117695e-02) (5, -4.57348876159676620556e-01) (6, 3.18095354067915181417e+00) (7, -1.59465526366045329265e-01) (8, -1.87659441956692873976e-01) (9, -1.50633426857482405348e-01) (10, -2.42625728463591872064e-01) (0, 8.28659736801213409052e-01) (1, 5.53205527590989953879e-01) (2, 5.99543955402850881065e-01) (3, 7.25313745664834641502e-01) (4, 6.68111957835435532616e-01) (5, 2.06801316210483765445e+00) (6, 3.14902613634095684958e+00) (7, 1.15512911583334942911e+00) (8, 1.63594865314455373273e+00) (9, -2.87070280442037040203e+00) (10, 5.50189704308173155312e-01) (0, 1.97562854971846113550e-01) (1, 3.62492062942441517492e-01) (2, 3.04066904680188709875e-01) (3, 3.04438547090943867346e-01) (4, 2.79695690469201674322e-01) (5, 2.36874242594735356926e-01) (6, -3.47947364290332172843e+00) (7, 5.58746582991317630729e-01) (8, 2.13482538311397018749e-01) (9, 2.98398399935802283167e+00) (10, 4.50758686835635913326e-01) (0, -6.10989415315816297714e-01) (1, -3.56403967290524828493e-02) (2, -6.97447886615399637034e-02) (3, -7.00499048381451883127e-02) (4, -1.49965912654936750492e-01) (5, 2.99484576437478944300e-01) (6, 2.06505848336426867107e+00) (7, -3.63819705038566421074e-01) (8, -2.31455633076932065784e-01) (9, -4.96039552488000135000e-02) (10, -4.64423233786642186693e-01) (0, 1.00811700901433742850e+00) (1, 4.06461381213124806067e-01) (2, 2.66735219256337807714e-01) (3, 3.26795452134069019934e-01) (4, 3.69040989176686817785e-01) (5, -6.63663084106235340309e-02) (6, -4.31120302670304145209e+00) (7, 5.52433068157370588125e-01) (8, 3.46873094935287507035e-01) (9, 3.81248863668974147245e+00) (10, 3.85901796246637118770e-01) (0, -6.28093399557754561435e-01) (1, -1.63068936566320099590e-01) (2, -1.32501350144353602367e-01) (3, -1.47408442000356409984e-01) (4, -8.65462286271722058828e-02) (5, 5.25718914271168236141e-01) (6, 4.14246015178818982605e+00) (7, -3.73181672482986281913e-01) (8, -7.23860027152757212754e-01) (9, -1.79975458780732788799e-01) (10, -5.35884967217266083850e-01) (0, 9.59119726032464758880e-01) (1, 4.07472350494321400305e-01) (2, 3.03941787080224568030e-01) (3, 3.81531306402143055578e-01) (4, 3.44337017135079914709e-01) (5, -6.98455352168702026106e-01) (6, -3.95282495314453541368e+00) (7, 9.01444780930102051997e-01) (8, 6.98931571817153929338e-01) (9, 3.82375553877886042997e+00) (10, 4.22810672397483711116e-01) (11, 3.37482186576260279054e-01) (12, 3.20000695606602381105e-01) (13, 5.48269561969817131875e-01) (14, 5.67950547030515950908e-01) (15, -2.39424975062495742906e-01) (16, 3.65031587501896570558e-01) (17, 5.66227205238336472881e-01) (18, 3.25522656163109491700e-01) (19, 5.28933319741070717690e-01) (20, 3.53936570307148645753e-01) (21, 2.04760854729177127487e-01) 
