FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.48593102833747203739e+00) (1, 3.99061908075194204937e-01) (2, 2.36660075435023209423e-01) (3, 3.25235930332522349406e-01) (4, 2.27384899803976914257e-01) (5, 4.57570848985807465681e-01) (6, 2.53540005439031157497e-01) (7, 7.82978910393218985320e-01) (8, 2.73362302380283528347e-01) (9, -1.70249940472713001682e+00) (10, 2.29462293674552036959e-01) (0, 1.25669035789176177786e+01) (1, 3.41185171603946346686e-01) (2, 4.49916739463596004889e-01) (3, 4.40643918216018337652e-01) (4, 4.28094465732364315436e-01) (5, -1.77843539933298422362e+00) (6, 5.46349678575443142847e-01) (7, -4.13972449509115048016e+00) (8, 6.06424634859920735908e-02) (9, -5.95580446981115896321e-01) (10, 5.39875763492522642295e-01) (0, 1.60086320033787998796e+00) (1, 2.50430481804626958731e-01) (2, 2.43491085542458074453e-01) (3, 1.45245531870144467534e-01) (4, 2.84792872442024724844e-01) (5, 4.39829500318037358575e-01) (6, -1.67356464178150601829e-02) (7, 5.98759261414640753252e-01) (8, 1.37796782400036565974e-01) (9, -1.31756709103018221363e+00) (10, 2.34797937562071873385e-01) (0, 1.26355150517626952933e+01) (1, 3.71703625379466662260e-01) (2, 4.61293217448854098173e-01) (3, 3.22474513857030520292e-01) (4, 5.14767286090516806851e-01) (5, -1.32209110217682446198e+00) (6, 1.03065118899104579686e-01) (7, -4.15643859214274247194e+00) (8, -1.73856491869363166680e-01) (9, -6.46254476620680806676e-01) (10, 4.86853814739700019310e-01) (0, 1.61004312029599461198e+00) (1, 6.38247152710243903506e-01) (2, 4.90829037422224612186e-01) (3, 5.75021458709522814701e-01) (4, 4.67033287430092314274e-01) (5, 4.40544340929815037011e-01) (6, 1.56293065395192964928e+00) (7, 7.65970277971725455046e-01) (8, 1.67103418041029261332e+00) (9, -2.57293937769785774350e+00) (10, 2.53655474875404574053e-01) (0, 7.11898741994365535923e-01) (1, 2.73198055716206478216e-01) (2, 1.79733517261196962256e-01) (3, 2.08472015948941075125e-01) (4, 1.59516754241635194678e-01) (5, -1.25484179196067469064e-01) (6, 4.65233466527019912107e-01) (7, -4.32869288598882861335e-04) (8, 2.20041923507642789293e-01) (9, -4.38049720525407337846e+00) (10, 3.06779231232858773382e-02) (0, 1.04117310293140064026e+00) (1, 4.67679527031554242988e-02) (2, 5.13962906228674909492e-02) (3, 6.24786269294395069096e-02) (4, 1.14550071619667001577e-01) (5, 1.71137187773544613423e-01) (6, 2.03971309371982156833e-01) (7, 1.34202471004543300026e-01) (8, 1.11842117071513813542e-01) (9, -1.58034493280991034103e-01) (10, 1.63817210039360255935e-01) (0, 1.18171562066454560735e+00) (1, 6.64046533642054931379e-02) (2, 2.02616294497037052746e-01) (3, 8.19546669957877532697e-02) (4, 2.01966365450406193371e-01) (5, 1.66566873784845081996e-01) (6, 9.37975289682901752775e-02) (7, -1.87444008072073564630e-01) (8, 2.96336460578120637621e-02) (9, -2.58710370074735851453e-01) (10, 3.54557671425147769018e-02) (0, 6.60650683317645870396e-01) (1, 1.95600486041952931204e-01) (2, 1.77607978375603547949e-01) (3, 2.22299079092194401541e-01) (4, 3.29216050596882692236e-01) (5, -1.07158042489564078736e-01) (6, 3.99099780810165338796e-01) (7, 6.70960392651806097897e-02) (8, 1.80054865081330950360e-01) (9, -4.27016006707810902299e+00) (10, 2.21520002288080330999e-02) (0, -1.72104945381773877244e+00) (1, -4.26764761532207256844e-02) (2, -9.71325313175578908265e-02) (3, -1.86429691823806392703e-01) (4, -1.27499622615661251102e-01) (5, 3.24410999628302262887e-02) (6, -2.18932746256548738106e-01) (7, 3.74923990406890450533e-02) (8, -1.65474813771338935542e-01) (9, 2.45839022929889505775e+00) (10, -1.71537965962026583977e-01) (11, 4.94368743345263705891e-01) (12, -2.98322582228080390720e-01) (13, 4.84858285829547153156e-01) (14, -2.72511126084224342137e-01) (15, 4.82039865300181613605e-01) (16, -6.06685422198365520430e-01) (17, -1.65786406732457082709e-03) (18, -1.00776757929410196168e-02) (19, -5.88594190613816570234e-01) (20, 3.76467082483781412261e-01) (21, 2.95622473462298418490e-01) 
