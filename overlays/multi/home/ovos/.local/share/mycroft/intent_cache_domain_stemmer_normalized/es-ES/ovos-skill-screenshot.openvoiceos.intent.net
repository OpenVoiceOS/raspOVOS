FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.25124448290926826299e-01) (1, 2.12545359119388321822e-01) (2, 2.58863316461059145279e-01) (3, 1.71025463923427406554e-01) (4, 1.98694547250482467238e-01) (5, -1.27918421424951661569e-01) (6, 1.43784535341567987210e-01) (7, 2.00657534198558695770e-01) (8, 1.47148799485015842947e+03) (9, 8.17782266918859468419e-02) (10, 2.15410028467041075873e-01) (0, -1.88641323855822051758e-01) (1, -1.74109132771904567827e-01) (2, -1.47674964552338222612e-01) (3, -2.02919440155441860307e-01) (4, -2.30050774102623228901e-01) (5, 1.82155163952872622968e-01) (6, -8.62167696437122765829e-01) (7, -1.25572228359654702867e-01) (8, 1.47148799485015842947e+03) (9, 1.10107067369875133434e-01) (10, 3.44279015369534507895e-02) (0, 4.00002768615543013997e-01) (1, 1.99308325906364464108e-01) (2, 2.53115785677997195968e-01) (3, 2.29175021310416920928e-01) (4, 1.65383783538906120603e-01) (5, -4.06122236030995786837e-01) (6, 6.61405254573524947759e-02) (7, 3.51465399422476565405e-01) (8, 1.47148799485015842947e+03) (9, 7.12223529720697606793e-02) (10, 7.16298679414226341367e-02) (0, -1.21724593820346491668e+01) (1, 3.26445088093035129528e-01) (2, 2.25232154433477327338e-01) (3, 1.97718546097028657904e-01) (4, 2.94308930937998258770e-01) (5, 1.50000000000000000000e+03) (6, 1.50000000000000000000e+03) (7, 8.04658632418030038025e+02) (8, 1.47148799485015842947e+03) (9, 1.50000000000000000000e+03) (10, 1.46292106439860223333e+01) (0, 2.94048404508381600042e-01) (1, 1.14197071859764714974e-01) (2, 3.62165907716042526077e-02) (3, 4.07605061625726794450e-02) (4, 5.73542083000429248063e-02) (5, -2.98993174971534159123e-01) (6, -4.86710900858708994665e-01) (7, 4.45824673719015240891e-02) (8, 1.47148799485015842947e+03) (9, -6.80054415783813287488e-02) (10, 1.19926182386746044140e-01) (0, 9.23269451038432192869e-02) (1, -2.07088124694475121668e-03) (2, -3.22617067092860337896e-02) (3, -8.29334555262539047105e-02) (4, -1.00083477615735258470e-01) (5, 1.81575954844381781683e+00) (6, 2.38282894454461185996e-01) (7, 2.21106223662254408602e-01) (8, 1.47148799485015842947e+03) (9, 1.89802987134856104845e-02) (10, 6.36177737437621848571e-01) (0, 1.49917884115688661950e+03) (1, 9.36851070546998521138e+00) (2, 9.30438862466706950727e+00) (3, 9.27804455631866176191e+00) (4, 9.19871253245963593770e+00) (5, -1.09369933185957393107e+03) (6, -1.14664175971082636352e+03) (7, -9.23542868747904208249e+02) (8, -1.09038515898759351330e+03) (9, -1.09283240337562324385e+03) (10, 1.07642202557186806189e+03) (0, 3.13293128109141671978e-01) (1, 1.40593969116627220961e-01) (2, 3.11465159843384298277e-01) (3, 3.16926584432542013392e-01) (4, 1.69600353429733508026e-01) (5, -4.30882296630198335929e-01) (6, 4.30306225424681026981e-02) (7, 2.57738619458639128812e-01) (8, 1.47148799485015842947e+03) (9, -4.52003897283799402351e-02) (10, 1.90868264982544694197e-01) (0, 1.47243442554736758687e-01) (1, 6.84627033532825834961e-01) (2, 5.77626004577843521481e-01) (3, 6.07520233602492298886e-01) (4, 6.66353873671261198730e-01) (5, 1.91220754162827605294e-01) (6, -2.24105021574768914494e-01) (7, 8.41291245932480724612e-01) (8, -8.36162723949700215087e+00) (9, -1.28865182883905582401e+00) (10, 9.00627843550712681164e-01) (0, 3.42515082527988234951e-01) (1, 1.57092992426487998436e-01) (2, 1.97230018885466484857e-01) (3, 1.13905362995002201743e-01) (4, 1.45837132097813682030e-01) (5, -4.76069068998800692949e-01) (6, 1.30009901791581672370e-01) (7, 6.71781609600224371981e-02) (8, 1.47148799485015842947e+03) (9, -3.00754833998681803120e-01) (10, 5.16530826757438804586e-02) (11, 1.42546939197015110068e-01) (12, 4.13933569676232737677e-01) (13, 1.39395749086616754786e-01) (14, 2.86473660163071408569e-01) (15, 1.53828199386376818225e-01) (16, 1.96151492306720326431e-01) (17, -4.81102150335620815103e-01) (18, 1.33441528788447061338e-01) (19, -3.18593885513645147523e-01) (20, 1.31629816024493001558e-01) (21, 2.47368807248466937576e-01) 
