FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=17 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.89877292916759454755e+00) (1, 1.60398258204747468092e+00) (2, 5.62346963993882398292e-01) (3, -1.11672502290757713617e+00) (4, 4.29182263632392979602e-01) (5, 4.10728986931641004698e-01) (6, 5.68498179930123970216e-01) (7, 1.78200897133861202448e+00) (8, 7.31054295418424815445e-01) (9, 1.18748080916264053819e+00) (10, 1.69950840824349280744e+00) (11, -1.64584757774164169852e+00) (12, -9.40070965850929862739e-01) (13, 1.61229886942104050362e+00) (14, 1.53704304118274870383e+00) (15, 1.62133415904408462183e+00) (16, -5.87251891132530734474e-01) (0, -3.75083595917209589032e+00) (1, 1.64701322402049332716e+00) (2, 6.39800495318745721107e-01) (3, -1.11865707658179402983e+00) (4, 5.64665567684376035196e-01) (5, 3.36569103968937299864e-01) (6, 3.91642063751783842118e-01) (7, 1.67276334322009700983e+00) (8, 7.28251923439665049820e-01) (9, 1.12736018187859055040e+00) (10, 1.66456520014508124738e+00) (11, -7.53177339589305772094e-01) (12, -8.03715994173864967109e-01) (13, 1.66119906388715454781e+00) (14, 1.64333307744176315701e+00) (15, 1.38207560704078225022e+00) (16, -5.94209943648514427039e-01) (0, -3.78140433535560571698e+00) (1, 1.64606163586665421583e+00) (2, 6.31811565570210564857e-01) (3, -1.27514912359364585015e+00) (4, 6.20301392483913693887e-01) (5, 2.68873054159481539305e-01) (6, 4.95862179118070234285e-01) (7, 1.26788259879702880362e+00) (8, 8.95452792682674414415e-01) (9, 1.07974027283051032811e+00) (10, 1.51842718028529044538e+00) (11, -6.72863680296562960415e-01) (12, -8.62494918416571443309e-01) (13, 1.58955226921037384713e+00) (14, 1.62754897774369644559e+00) (15, 1.58176129799848563806e+00) (16, -5.88403214451012179786e-01) (17, 1.18888328456970562286e+01) (18, 1.19292033174725879974e+01) (19, 1.98741197437994578934e+01) (20, -4.91973027121360484415e+00) 
