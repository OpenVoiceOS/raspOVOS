FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -4.89856540747738045116e-01) (1, -2.15920058657420900483e-02) (2, -7.69597736765635920264e-02) (3, -8.51358694602740717627e-02) (4, -4.13304714133037945367e-02) (5, 1.73052889575452638338e-02) (6, 1.82206706294089482157e+00) (7, -5.35455502423034454540e-02) (8, -2.50199721299378986805e-03) (9, -7.62019250124489233578e-01) (0, 1.04214160042152026442e+00) (1, 9.00742556236917790713e-01) (2, 8.65855003975565251650e-01) (3, 7.67141520314152058901e-01) (4, 8.56137554429704961123e-01) (5, 7.31122101300707427640e+00) (6, -6.86440276954553763034e+00) (7, 1.54396129360755836046e+01) (8, 7.07241408070052912649e+00) (9, 1.00672742746859422702e+00) (0, 8.96867603954227543550e-02) (1, 2.56156675524236328301e-02) (2, 1.92974222452116067617e-01) (3, 1.85153407604169900624e-01) (4, 1.17328090221357456135e-01) (5, -2.74730082908106520989e-01) (6, 3.65911910437654031014e+00) (7, -2.80568701603964965052e-01) (8, -7.29406643015623606940e-02) (9, -9.62396844185815503181e-02) (0, 1.37649097518790619610e+00) (1, 1.02097471958281649229e+00) (2, 1.04311915641905916807e+00) (3, 8.53744180558939969039e-01) (4, 9.19238822220584173017e-01) (5, 7.37421248416255803448e+00) (6, -6.21973030056521825060e+00) (7, 1.55279432645400881086e+01) (8, 6.54128813516886964408e+00) (9, 4.88857971545648695155e-01) (0, -5.44765166109110521298e-01) (1, -1.08199415375484532531e-02) (2, -4.04427198340191265680e-02) (3, -6.48706929851922435223e-04) (4, -1.70757896976639234055e-01) (5, -1.87322438526086598731e-01) (6, 2.12122204769382971534e+00) (7, -1.09345958696277167910e-01) (8, -1.11758490450476616340e-01) (9, -1.00936962142760444650e+00) (0, -7.15144251356888416282e-01) (1, -7.53229473552620293342e-02) (2, -6.52135804509284232322e-03) (3, -9.13358096799766899787e-02) (4, -1.06020531507483709022e-01) (5, 4.94194229478115143461e-02) (6, 1.96210102328330204813e+00) (7, -1.34828251795370585020e-01) (8, -1.70492489688332977960e-02) (9, -9.09462148902286715568e-01) (0, -3.47291655603680160525e+00) (1, -7.42018623621598233386e-02) (2, -1.76781351712478718152e-01) (3, -5.21971909673348555270e-02) (4, -7.00530289085046081743e-02) (5, 5.66656802877444493172e-02) (6, 1.87587995784741345950e+00) (7, -2.65688816523413848247e-02) (8, 5.13223842186687692624e-02) (9, -1.06647615510630155988e+00) (0, 4.80566633348149085947e+00) (1, 6.39720793524984676637e-01) (2, 7.21127170780901272096e-01) (3, 6.85379486242536861695e-01) (4, 6.22472051242593971132e-01) (5, -1.51211355085457133107e-01) (6, 5.02019604206995229134e+00) (7, 4.32252160679171204549e+00) (8, 1.62935798318949820462e+00) (9, -2.20304223342795113538e+00) (0, 2.11198968309525031417e-01) (1, -1.23149242712850437442e-01) (2, -1.07090593754760252110e-03) (3, -8.25543417329706145924e-04) (4, -2.92603564399432358734e-02) (5, 7.75769737575231488336e-02) (6, -4.26234039856219215636e-01) (7, 7.87675251248626667744e-02) (8, 2.67316047347028962200e-01) (9, 4.59858161710058749305e-02) (0, 1.87412684125482869035e+01) (1, 4.68468531518985153994e-01) (2, 5.46735651343871364993e-01) (3, 5.25924093574095974368e-01) (4, 4.06741380244303840641e-01) (5, -4.63052712753767536924e+00) (6, 6.39973788417310895937e+00) (7, 4.37762051150857178783e+00) (8, 3.42669926848297862199e-02) (9, -9.49439470973983179469e-01) (10, 4.11932032209911924703e-01) (11, 6.96125568512443448910e-01) (12, 2.97287794329946342131e-01) (13, 6.63704449001315976986e-01) (14, 9.40956243539628145456e-01) (15, 4.17631555003204923970e-01) (16, 4.36616264184412461447e-01) (17, -1.08854634638426858717e-01) (18, -2.01893344435877558629e-01) (19, -1.48742617461777415500e-01) (20, 3.60828835567442385290e-01) 
