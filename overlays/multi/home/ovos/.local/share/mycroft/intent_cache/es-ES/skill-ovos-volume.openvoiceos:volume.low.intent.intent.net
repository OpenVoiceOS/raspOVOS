FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.42747849729149134390e+00) (1, 5.54596442209643769594e-01) (2, 6.09558581786317166262e-01) (3, 5.15219643192452836367e-01) (4, 6.59780702965421017581e-01) (5, -6.39695687476305607788e+00) (6, 2.61661000043361752887e+02) (7, -3.53997325370637550002e-01) (8, 9.74799394876372904406e-01) (9, 1.88047207454830811457e+00) (10, 4.00941653297040345727e-01) (0, 7.96452020426581142054e-01) (1, 2.92716880813274835105e-02) (2, -5.38033174797858324001e-02) (3, 7.74913630634886316240e-03) (4, 3.82957211882215941418e-03) (5, -4.83669066867275532218e+00) (6, -2.36610916004970306403e+00) (7, -3.08066491477966886148e-01) (8, -8.98933543002258111931e-02) (9, -1.21674756225095478945e-02) (10, 2.18618454243165675349e-03) (0, 2.69093380664121717416e+01) (1, 7.36186095728557421403e-01) (2, 6.55292050196807696061e-01) (3, 7.83747703983466936783e-01) (4, 7.78154389216583086686e-01) (5, -6.79870733138778993165e-01) (6, -7.96872633830167842461e+00) (7, -3.90170429631501081147e+00) (8, 7.33763192243187534203e-02) (9, -8.47002764111960071425e-01) (10, 2.29368833099509350504e+00) (0, 9.26073962774030756684e-01) (1, 7.39105733733869812996e-02) (2, 1.96131558964798613509e-01) (3, 2.13341729591438822311e-01) (4, 2.41163300226280685745e-01) (5, -1.30747399659106662284e+01) (6, 1.75449512410051217115e-01) (7, 1.22181549090350261011e+00) (8, 3.09527322824826922609e-01) (9, 4.77718246081903108458e-01) (10, 1.70691728878998340291e-02) (0, 9.36688939060918879242e-01) (1, 1.40313332089479131382e-01) (2, 2.32867029615934029163e-01) (3, 1.31434002080257072986e-01) (4, 2.60866624423558957080e-01) (5, -1.30752481290349891196e+01) (6, 3.17956391425654946725e-02) (7, 1.66387492923288249358e+00) (8, 4.01243826923595003908e-01) (9, 5.69732518395094644781e-01) (10, 1.62147545306862816972e-01) (0, 1.35811726129574772415e+01) (1, 6.05522862441463360916e-01) (2, 6.98978892333431134354e-01) (3, 6.38315020866317639481e-01) (4, 6.64097077019138226639e-01) (5, -4.38372511737411652000e-01) (6, -6.53598979230729959511e+00) (7, -1.56515709320263396442e+00) (8, 1.80444094789826564584e-01) (9, 1.24221176528484203438e-01) (10, 7.73396872744879559924e-01) (0, 1.00221309282584747358e+00) (1, 3.13603579510953300868e-01) (2, 2.65254891921784752284e-01) (3, 1.80936574448850140406e-01) (4, 3.12888249267842644130e-01) (5, 2.04822186789736768731e+01) (6, 4.87280830814881866786e+01) (7, 7.34515163242755730888e-02) (8, 4.09116640611376147785e-01) (9, 2.40480965177447325587e-01) (10, 2.63313007624359662895e-01) (0, -7.18943856510955159322e-02) (1, 2.22562884145568656846e-01) (2, 2.75019475423883219545e-01) (3, 1.72672488891671943589e-01) (4, 2.84551390611718912904e-01) (5, 2.04633893928751966484e+01) (6, 4.32048354250008070920e+00) (7, 3.07744393464249796355e-02) (8, 2.36242581945083990336e-01) (9, 2.71184503288759259920e-01) (10, 3.36226763083263158194e-01) (0, 8.65258308432537148747e-01) (1, 3.79367291440274534420e-01) (2, 2.33755919595267758204e-01) (3, 2.40548025329616055323e-01) (4, 3.33263217915799436764e-01) (5, 2.04316787979826948174e+01) (6, 4.86336430236144607875e+01) (7, 2.23744711868894130546e+00) (8, 4.20854829176153522052e-01) (9, 2.78604345570237110774e-01) (10, 2.00901518602075668252e-01) (0, 3.03315477565622764189e+01) (1, 5.19784191106913207037e-01) (2, 6.69127234672663329107e-01) (3, 5.68251141761896727544e-01) (4, 4.93333504950163648228e-01) (5, 1.39607994492863735081e-01) (6, -7.74344936435992892143e+00) (7, -4.01411800816088515376e+00) (8, 7.23952956783540518826e-02) (9, 3.72429644721020414178e-02) (10, 2.08310494908662136027e+00) (11, 4.28462615756109832521e-01) (12, 1.18238929444576049144e-01) (13, -3.93019270527592001763e-01) (14, -4.66241523779794497795e-01) (15, -2.94627024698137618408e-01) (16, -2.02097359037722024144e-01) (17, 2.74763507206896351320e-01) (18, 2.92360169251421442294e-01) (19, 2.42268939785192566339e-01) (20, -2.43427491533748679986e-01) (21, 2.94084075831045566041e-01) 
