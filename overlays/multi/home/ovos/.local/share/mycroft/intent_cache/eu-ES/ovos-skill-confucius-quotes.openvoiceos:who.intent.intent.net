FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.81830541591049871242e-02) (1, -2.36458813658175254213e-01) (2, -1.78271701445994162905e-01) (3, -3.24708862116751428850e-01) (4, -1.73189138880190635073e-01) (5, 7.66879606512701345045e-02) (6, -9.55114674302423166674e-02) (7, 2.56419963713477816469e+00) (8, 8.77009618548071218092e-02) (9, -2.42394893759944440870e-01) (0, 1.63985641407564358696e-01) (1, -4.92699669004049850551e-02) (2, -6.57451675534811430524e-02) (3, -4.15339918137160035716e-02) (4, 1.15996122240963880912e-01) (5, -7.00103897052988949357e-02) (6, -3.31807669731462306917e-02) (7, 3.62825719783772360216e+00) (8, 3.13726962703473333249e-02) (9, 5.70024343903809196932e-02) (0, 4.99705670076347974184e-01) (1, 6.92663331741940613284e-01) (2, 7.17679159010302658572e-01) (3, 7.65716341550004120364e-01) (4, 7.88152818793904419437e-01) (5, 3.58365004050344104769e-01) (6, 5.96109057091087968239e-01) (7, -5.47530295852868320594e+00) (8, 4.96018084011883519935e-01) (9, 6.26463670486275714389e-01) (0, 4.47829277257321911598e-01) (1, 1.24481133437218199900e-01) (2, 1.02824821388783016474e-01) (3, 3.84031378389020419672e-02) (4, 1.56522437250675744425e-01) (5, 4.10990085928570669793e-01) (6, 5.93830218417535249387e-01) (7, 2.36968694891294850979e-01) (8, 4.65740457334648250143e-01) (9, 3.62672475318311204884e-01) (0, 2.90203107255295023048e-01) (1, -4.74214331508246017544e-02) (2, -6.69817031622496061871e-02) (3, 1.00877165675106947318e-01) (4, -1.90325157046881444922e-02) (5, 1.02933567444683557457e-01) (6, 8.07284552124655080396e-02) (7, 3.57614283035267721544e+00) (8, 5.30545424401828932814e-03) (9, 6.10365065511017448885e-02) (0, -9.61255538951598875741e-01) (1, 4.48517040195050342710e-01) (2, 2.69804009081902607115e-01) (3, 4.14483309568944080503e-01) (4, 3.45324435116829975279e-01) (5, 9.65890371948053411133e-03) (6, -3.88531067357400991535e-02) (7, 8.90786901926638030602e+00) (8, 6.61204018607692567411e-02) (9, 2.21255360963916902772e-01) (0, 1.47634571152523236259e-01) (1, -6.55623452067938261578e-02) (2, 1.06751739859524671927e-01) (3, 2.44760884642037622461e-02) (4, -5.96388281584349227993e-02) (5, 1.47907924455418469945e-02) (6, 4.23173147705709629118e-02) (7, 3.65132877002228628527e+00) (8, -1.24129958341717056003e-01) (9, 1.39826032098084133970e-02) (0, -9.34043170750780693545e-01) (1, 1.25439842897251352394e-01) (2, 2.76484084981277689064e-01) (3, 3.07693732636764749611e-01) (4, 2.96239925282791360939e-01) (5, -1.07001400648061739673e-01) (6, -5.33746608481745418717e-02) (7, 8.99324457412601852013e+00) (8, 1.66961261048870282520e-02) (9, 2.74227543595409517518e-01) (0, 2.60412005441978677833e-01) (1, -4.53124718070593429653e-02) (2, 4.76455612778100279225e-02) (3, 6.81184529065522598179e-02) (4, 1.00576951980534498587e-01) (5, 1.08372357169987207359e-01) (6, -4.74925350877130336702e-02) (7, 3.61043990920056234728e+00) (8, -9.10677238208048717150e-02) (9, -8.10083580438346190222e-02) (0, -1.07254716926877091687e+00) (1, 2.15933521943882184013e-01) (2, 1.63352215396955685600e-01) (3, 1.42923181610897259697e-01) (4, 3.10332519667938455665e-01) (5, -6.07395946070594452970e-02) (6, -1.06287983892951354292e-01) (7, 8.96892604297043227746e+00) (8, -6.18561237141532563322e-02) (9, 2.91878818395710115663e-01) (10, 7.34367733115803833499e-01) (11, 4.93406610104276777129e-01) (12, -3.75850553641736917498e-01) (13, -8.41584058257959372051e-02) (14, 3.14356299656691484667e-01) (15, 2.99891892552319627718e-01) (16, 4.42015893604850884646e-01) (17, 3.57273954629841905550e-01) (18, 9.83160625822663697404e-02) (19, 3.19724160909596544222e-01) (20, 3.73374807763628746571e-01) 
