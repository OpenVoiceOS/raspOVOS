FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.90630295750400091670e-01) (1, -6.26381291293180292312e-02) (2, -4.32959450625455544159e-02) (3, -3.66812600039518113548e-02) (4, -1.34685653283504208622e-01) (5, 1.13287859331207618752e+00) (6, 6.61551195091258370518e-02) (7, -1.52556518777568955736e-01) (8, 9.38846137393091306755e-01) (9, 8.17600439980960769226e-01) (10, 1.78288528170404483864e-02) (0, 7.57384366762875593793e+00) (1, 3.86396785400136166810e-01) (2, 3.20708667061551377930e-01) (3, 4.54755355737431699037e-01) (4, 4.29381315729840451478e-01) (5, -1.23356578103696645776e+00) (6, -4.35132084933424412299e+00) (7, -1.02961272812057336168e+00) (8, -1.45673638163670027268e+00) (9, -5.81430671562481737880e+00) (10, 2.88358375353536811048e+00) (0, 2.43078044924186859044e-01) (1, 1.16369075187268938842e-01) (2, 1.82320392213494858469e-02) (3, 1.22342898555818327400e-01) (4, 7.19965905767988578745e-03) (5, 4.51995631423817290084e+00) (6, -4.59344842190039570617e-01) (7, 8.01857572253698441811e-02) (8, 4.97894789611824339914e+00) (9, 5.30112623266222104945e+00) (10, 2.23860977770860530844e-01) (0, 5.04197936493907983291e-01) (1, 4.24416426040166050448e-01) (2, 4.17407985247605473056e-01) (3, 4.74196064688199192538e-01) (4, 3.63748523808949675296e-01) (5, -3.00319954854430470892e-01) (6, 1.32316978903194759631e+00) (7, -1.03437258925148056754e+00) (8, 1.98786326736737989362e-01) (9, -6.67012484527630178377e+00) (10, 5.22359534777963041563e+00) (0, 1.76031427897097109136e-01) (1, 1.15017767651710386967e-01) (2, 9.98497490126227238427e-02) (3, 1.09560425980720410610e-01) (4, 1.43905784590873608852e-01) (5, 4.42540356213417851450e+00) (6, -6.31441637011896511611e-02) (7, 6.68823730405324889947e-02) (8, 3.71735595734443080573e+00) (9, 6.18304974071093038646e+00) (10, 1.37713665487690983813e-01) (0, 4.77010602957065543706e-01) (1, 1.69992845161590494518e-01) (2, 1.05523783369693646694e-01) (3, 2.59978954035376477738e-02) (4, 6.25113506156538822900e-02) (5, 4.64220269307970845318e+00) (6, -8.04351135226618330076e-02) (7, 2.03924702344821726085e-02) (8, 4.01192241923967873873e+00) (9, 4.99971547847051933644e+00) (10, 2.14927034446472542406e-01) (0, 4.44177509093669542484e-01) (1, 3.87271305968669266040e-01) (2, 2.13659977962878472324e-01) (3, 2.35368518581298119541e-01) (4, 2.83752827396300755236e-01) (5, -2.14610182428667117804e+00) (6, -2.86275857448984816234e-01) (7, -2.27931256779161151016e-01) (8, -2.35992290559533435257e+00) (9, -1.40603741949917782073e+00) (10, 5.28690151581927780811e-01) (0, -1.50434990894607462097e-02) (1, 1.67134617807875390794e-01) (2, 1.30649251284609579926e-01) (3, 2.33983596208105798508e-01) (4, 1.32970852198611017014e-01) (5, 2.01460754423042187611e+00) (6, 1.14710807116475237144e-02) (7, -8.64559529556819805762e-02) (8, 1.58059441979107107024e+00) (9, 2.51836751169951567064e+00) (10, 1.61956740882532201908e-01) (0, 4.82822594080958622520e-01) (1, 3.31039439730170248488e-01) (2, 4.15619667462828523696e-01) (3, 4.00006305269720918716e-01) (4, 2.79584709398272512892e-01) (5, -2.13362583598584976130e+00) (6, -5.76592155332411260282e-01) (7, -3.60288553684781798214e-01) (8, -2.29721871498349505814e+00) (9, -1.58994244248956806409e+00) (10, 5.95406764875851535201e-01) (0, 5.41796463435943564946e-01) (1, 1.26352600580504487171e-01) (2, 1.32069431072524140491e-01) (3, 1.65527097708037446155e-01) (4, 1.34710199958136628284e-01) (5, 4.55739548871484689840e+00) (6, -1.99305669877184510330e-01) (7, 5.65402059333483947290e-02) (8, 4.59452344435984016968e+00) (9, 5.11185784389634445546e+00) (10, 1.19475835928196313929e-01) (11, 9.16259232086313657462e-01) (12, -2.30074841355979997637e-01) (13, 3.01154063139557837125e-01) (14, -1.36470509918377558289e-01) (15, 2.98772763592458634463e-01) (16, 2.39772007856965146955e-01) (17, -4.32005902525624208010e-01) (18, 6.38763371314850170180e-01) (19, -4.31950619217595033206e-01) (20, 2.23163359364632107873e-01) (21, 5.44771403333716430772e-01) 
