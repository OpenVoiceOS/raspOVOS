FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.97632624428343628420e-01) (1, 3.56946199219425497340e-01) (2, 3.54363500158985433863e-01) (3, 3.41689570825298605250e-01) (4, 3.32890390436847982691e-01) (5, 1.14442598725455857434e+00) (6, 9.37632911944199349819e+01) (7, 2.73803408758745259188e-01) (8, -8.17702844293655672914e+00) (9, -3.11307506556990931657e+00) (10, 2.75541970366537025683e+00) (0, -2.96642556631172005233e-01) (1, -5.32496569003634734329e-01) (2, -5.00657712366156970596e-01) (3, -5.13964821841292662796e-01) (4, -5.87328751828246509170e-01) (5, 3.99762769130431294684e-01) (6, -5.77147396027019254916e-01) (7, -2.41236333966990984523e-01) (8, 1.26466553202312717019e+01) (9, -2.90266134954141374891e+00) (10, -4.34171786273295279290e-01) (0, 9.13584019423570731222e-02) (1, -1.08791811572619870341e-01) (2, -2.17362521397182012484e-01) (3, -2.38306420104810262606e-01) (4, -2.78185142385073913562e-01) (5, 1.41442386697095218162e-01) (6, 5.68742287719574912686e-01) (7, -4.13064704494384427047e-02) (8, 3.18886159680064951161e+00) (9, 1.27732198227011495284e+00) (10, -1.10547228336661529369e+00) (0, -1.60665965539249833149e-01) (1, -1.66051078926443607120e-01) (2, -2.02052761207937747745e-01) (3, -1.08430583368658559063e-01) (4, -1.30867351185202124286e-01) (5, 1.83246791322063479202e-01) (6, 7.00919137331168951022e-01) (7, 3.92118067178914911364e-02) (8, 2.04746399297338532719e+00) (9, 1.29273878382249374219e+00) (10, -1.26324042776408851374e+00) (0, 8.66942704527619678601e-01) (1, 6.54008781192699367724e-02) (2, 1.17962913739482533337e-01) (3, 2.09100293147365140500e-01) (4, 1.87189163791934537473e-01) (5, 2.06328042866252669985e-01) (6, -1.23586619551443233078e+00) (7, -8.97283086034977084289e-01) (8, -4.79694608942199351986e+00) (9, -1.91383235906993198761e+00) (10, 1.09351059191382840585e-01) (0, -2.83604371667222798781e-01) (1, 3.97972350126404084136e-01) (2, 3.42852090603012360503e-01) (3, 3.50618136232990540435e-01) (4, 2.47281079298157124846e-01) (5, 1.00878928244413512694e+00) (6, 2.49977672065903799847e+00) (7, 9.84568140136708791488e-01) (8, -5.63882337544734824064e+00) (9, 4.81051983292491325273e-01) (10, 8.97591662829757996889e-01) (0, 5.70500574990544628839e-01) (1, 1.53136991522626053630e-01) (2, 1.08104862830952871966e-01) (3, 8.31618985155384432550e-02) (4, 1.32985056004838064769e-01) (5, 6.16312414015482840091e-02) (6, 6.08246625330246759589e-01) (7, 7.25441818168080843066e-01) (8, -2.03340738852833879946e+00) (9, 1.73740418832127696547e+00) (10, 2.03449613536116416523e-02) (0, -2.02627029683721315845e-01) (1, -5.89022511469083978608e-02) (2, -1.43177951298828542992e-01) (3, -4.05755961166578346666e-02) (4, -1.35240706227894247338e-01) (5, -9.92122869574514565372e-01) (6, -2.07838901409706844348e+00) (7, -2.77176306717344278496e-01) (8, 4.45523037451044217505e+02) (9, -2.64379646995506734086e+00) (10, -1.68857164779638822816e-02) (0, 5.63696528583300882076e-01) (1, 1.43011422880329780449e-01) (2, 1.48167097993530949562e-01) (3, 5.24747465330741916234e-02) (4, 9.69511438686035537105e-02) (5, 3.92102226629147010417e-01) (6, 4.89901768108928326040e-01) (7, 7.08475146279608725131e-01) (8, -1.42043968914159735029e+00) (9, 9.94054349651396385035e-01) (10, 1.17504503496956541619e-01) (0, -2.95728839003931875951e-01) (1, 2.55828899290020173041e-01) (2, 2.43557468946630606421e-01) (3, 3.19464792456085555639e-01) (4, 3.73124529566223495092e-01) (5, 5.79056335652030518268e-01) (6, 1.53470022740451952359e+01) (7, 5.07350208634562593879e-01) (8, -4.36071983854787958990e+00) (9, 5.09522925415120342407e-01) (10, 3.79526727672921115886e-01) (11, 2.88416566626219061931e-01) (12, 5.63668303251277591492e-01) (13, 5.30648741184566730134e-01) (14, 5.71559708356918938499e-01) (15, 2.38529894677551007609e-01) (16, 2.89130391852049140056e-01) (17, 2.64001360733922052493e-01) (18, 2.02842966119933271196e-01) (19, 2.38600003132968752073e-01) (20, 3.26393348513298098279e-01) (21, 2.28575600752966956453e-01) 
