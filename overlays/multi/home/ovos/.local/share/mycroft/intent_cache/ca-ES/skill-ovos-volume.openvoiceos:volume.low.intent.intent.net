FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.64714476901783624108e-01) (1, 4.78008667865188308088e-01) (2, 3.68977534511478133528e-01) (3, 3.58105744161994643537e-01) (4, 4.52490414419563002912e-01) (5, 1.93698908016882604954e+01) (6, 5.15101692315251047383e-01) (7, 4.41291952597996086638e+01) (8, -1.78170689626582827714e+01) (9, 4.31762023047483955995e-01) (10, 7.81369860808960731147e-02) (0, 9.28276166850640405137e-01) (1, 3.59666019148469073841e-01) (2, 3.18099170393586150318e-01) (3, 3.29595282144188872486e-01) (4, 3.76328244275689172138e-01) (5, 4.38541708764086237693e+01) (6, 4.02353269671345881164e-01) (7, 3.65669320951491769733e+01) (8, -8.20564393436132633042e+00) (9, 2.98967494655015486593e-01) (10, 3.61805716641916907950e-01) (0, -5.96865491226776345712e-01) (1, -3.22139899274342944580e-01) (2, -2.83797617098325127838e-01) (3, -4.28313820650809695678e-01) (4, -2.77397538563245171783e-01) (5, 5.25653915989237274786e-01) (6, -2.20978870835017660035e-01) (7, 4.10773715685103180117e-01) (8, 4.82317854442492510714e+00) (9, -2.55992897755761839029e-01) (10, -3.30484784640157458480e-01) (0, 6.83681333738164420666e-01) (1, 3.75959428885371871321e-01) (2, 3.01726247110755629866e-01) (3, 4.91693179049880690901e-01) (4, 4.59947879352958388655e-01) (5, 4.33818822907875798478e+01) (6, 7.50898826390016815857e-01) (7, 6.96757321071091340059e+00) (8, -1.51773916369848222985e+01) (9, 1.06296437168110813332e+00) (10, 2.81002360830637398781e-01) (0, 6.35524648195999297684e+00) (1, 1.24362141821857608015e+00) (2, 1.27547382746215975935e+00) (3, 1.26087434027668154890e+00) (4, 1.17219910953518069441e+00) (5, -5.01843476134344967932e+00) (6, 2.84345321474894496827e-01) (7, -5.21288023108390774496e+00) (8, -3.51403076421126092654e+00) (9, -1.03190824180583717862e+00) (10, 6.68684702648814410786e-01) (0, -4.41386566713913131021e-01) (1, -2.66937132021420886474e-01) (2, -4.36956333955758502441e-01) (3, -3.51950327893727765716e-01) (4, -4.40151418587201526122e-01) (5, 4.72696515309648568337e-01) (6, -1.57649160148115530244e-01) (7, 3.89988968829605819888e-01) (8, 4.76620763941430691801e+00) (9, -1.60087792110602089579e-01) (10, -2.53318760044658419783e-01) (0, -1.04636531283899644751e-01) (1, 2.42268643812821482530e-01) (2, 2.51586481468365763536e-01) (3, 2.17354345606253718248e-01) (4, 1.97038460062430503816e-01) (5, -8.92449215778401194754e-01) (6, 2.12610373108939459952e-01) (7, -1.28413137213361938649e+00) (8, 8.27288935425262295098e+00) (9, 1.12334299282887961624e-01) (10, 1.61147670987261737441e-01) (0, 6.41408935022318993369e-01) (1, 1.84243686269622397589e-01) (2, 1.20317175756793473540e-01) (3, 1.38933039974074828971e-01) (4, 1.33397713075976837027e-01) (5, 1.60801445943321019705e-01) (6, 1.88890896950878128813e-01) (7, -1.28363292489338465252e+00) (8, 8.66241248809794051056e+00) (9, 4.81936782220926396292e-02) (10, 3.89561756282000570817e-02) (0, 6.91985148972092622977e-01) (1, 5.07127648017280940884e-01) (2, 4.20350027999752406949e-01) (3, 4.80578040501945857876e-01) (4, 3.68822997233742122525e-01) (5, 4.33889884533952496781e+01) (6, 6.70292239158857605652e-01) (7, 4.34378640546474770190e+01) (8, -1.51572220622174711480e+01) (9, 2.62376630900743729180e-02) (10, 2.50049674364594354259e-01) (0, 8.10517958459515575065e-01) (1, 4.99106907030446800277e-01) (2, 4.42707285961015550857e-01) (3, 5.90292819520337630479e-01) (4, 5.79347901722295333116e-01) (5, 4.32785410615276120438e+01) (6, 4.88407210976177175166e-01) (7, 4.35325956537399605395e+01) (8, -1.51338254472784967675e+01) (9, -1.74097472976354417096e-01) (10, -2.97414848821538901902e-01) (11, 2.21958996749240078383e-01) (12, 1.94482164478617824965e-01) (13, 6.59008172790130908325e-01) (14, 1.91040137803870357924e-01) (15, -2.63631351909501987496e-01) (16, 7.05825430790504748657e-01) (17, 1.97456556820603978952e-01) (18, 1.26780628031456399363e-01) (19, 1.99066000080424465590e-01) (20, 2.38988505697566189223e-01) (21, 2.69040485525977812742e-01) 
