FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 6.89504853405599238414e+00) (1, 3.42811123980811149892e-01) (2, 4.29940515710642845448e-01) (3, 3.84731048299601585683e-01) (4, 4.29476150824358970937e-01) (5, 5.22137849281767252307e+00) (6, -2.20940108806877866243e+00) (7, 3.32360193882385079878e-01) (8, -5.98619924675169376438e-02) (9, -8.29408426369885876106e-02) (10, 5.13157713187904040986e-01) (0, -1.22512379068697851991e+00) (1, -1.17241210954100824604e-01) (2, 2.95300113988807809429e-02) (3, -5.91005084084579301873e-02) (4, -8.94367058741161596869e-02) (5, 1.31496476406895856925e+00) (6, -1.24033275354004786095e+01) (7, 1.54684553785108724888e+00) (8, 1.25914356331092075481e-01) (9, 3.08703084377861036725e-01) (10, -4.79990363912601691876e-01) (0, 2.13316781728054607470e+01) (1, 7.05182569257963520748e-01) (2, 7.79328627668369633419e-01) (3, 7.49521991394508702022e-01) (4, 7.52314081572044712765e-01) (5, -7.56433228915048427865e+00) (6, -2.61090907935543459217e+00) (7, 3.14632508413093814337e-01) (8, 8.12416482556646446511e-01) (9, 5.28859870343145854754e-01) (10, 9.25359387560030444142e-01) (0, 1.40478074129828183203e+01) (1, 4.10180188461854722259e-01) (2, 4.09013665958955552338e-01) (3, 4.16323698803499009369e-01) (4, 3.46569761380269791839e-01) (5, 4.70908709401190694877e+00) (6, -2.31926192552834242733e+00) (7, 4.59573336027216672317e-01) (8, 2.02680129092251803957e-01) (9, 5.50388319877888276821e-01) (10, 5.57249168826312701874e-01) (0, -2.01224535167672247127e+00) (1, 8.80015279484626239404e-02) (2, -8.35186903046730433342e-02) (3, -5.06244924532535459094e-02) (4, 6.10480393601295079353e-02) (5, -4.97956418485395804763e-02) (6, 1.17468738137374129060e+00) (7, 3.15877378853304913164e-01) (8, -4.26253775992345984580e-01) (9, 7.34111750752060830683e-03) (10, -5.33390081589473652457e-01) (0, -2.32353122157737557174e+00) (1, -5.84119957274628689992e-02) (2, -9.03684001750184179302e-02) (3, -3.48173480815125654941e-02) (4, -1.91069188374919984863e-01) (5, -2.59032037664939686350e-02) (6, 1.20887727957632762354e+00) (7, 2.46822493283663318975e-01) (8, -2.39237443733137378210e-01) (9, 6.48939657813762116323e-02) (10, -5.28550545492559642469e-01) (0, -1.60186946105123584339e+00) (1, 2.77996103981076880807e-02) (2, 4.60370894291936699760e-02) (3, 9.27778299668370609599e-02) (4, 5.43283488371908013237e-02) (5, -3.78134120926937766782e-01) (6, 1.17143532679659267970e+00) (7, 4.20395728608435825802e-01) (8, -1.45982652823530734132e-01) (9, 1.27398688287822647175e-02) (10, -5.09389552514649346371e-01) (0, -2.31437121329590622665e+00) (1, -1.64781546944502088614e-01) (2, -6.66779938877851419132e-02) (3, -2.26802169138315440344e-01) (4, -1.74698970073583814688e-01) (5, -6.67718869565872980010e-02) (6, 1.36115598243134661161e+00) (7, 3.30967979638491283101e-01) (8, -3.27146685597581199367e-01) (9, -5.47986693086413234610e-01) (10, 2.29575091696312301448e-02) (0, 7.94253735201330446358e+00) (1, 4.15950162923587551944e-01) (2, 3.55879655695213070743e-01) (3, 3.38746464467300167911e-01) (4, 3.71859407639755001895e-01) (5, 6.83132494374460286224e+00) (6, -3.38105902159967852683e+00) (7, 3.34117385614619843981e-01) (8, 1.75606202271705924556e-01) (9, 6.05912477643503644131e-01) (10, -3.57685661441097924040e-01) (0, 5.25551200769873627650e+00) (1, 4.21174437868229556425e-01) (2, 3.35611389147869754179e-01) (3, 4.03761596548191714628e-01) (4, 3.48912418949238467558e-01) (5, 6.46095284002481218266e+00) (6, -3.11809503374824759092e+00) (7, 1.21498153309715162251e+00) (8, 2.27311479032592078653e-01) (9, 8.27153160132564235774e-01) (10, -2.07045419522054685579e-01) (11, 4.04932156374382901109e-01) (12, -3.96842567507971422103e-01) (13, -3.10401844037389906816e-01) (14, 3.53933341970372128404e-01) (15, 6.14420224469533149403e-01) (16, 6.48246412750117295687e-01) (17, 6.44815985497566246387e-01) (18, 6.03552576688137110139e-01) (19, 4.02107464860690155817e-01) (20, 4.32192805002940216852e-01) (21, 3.94643237576533190136e-01) 
