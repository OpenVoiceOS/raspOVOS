FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 6.25221629201204365245e+00) (1, 7.49161969769091662386e-01) (2, 9.31180838394255805390e-01) (3, 8.57527438449950274446e-01) (4, 9.12994313883872088411e-01) (5, -4.96721364830289946468e+00) (6, -1.29959195580289121885e+00) (7, 6.49249697312829998630e-01) (8, 1.33265159040009972813e+00) (9, -2.95350352718980690980e-01) (10, 8.57196598258674224979e-01) (0, -6.96162555543868299957e-01) (1, -3.09806457699246702575e-02) (2, -1.16311206573338030146e-01) (3, -9.25095445455021564429e-02) (4, -1.36907144540638486818e-01) (5, 4.30665656889088685477e-02) (6, 2.39621978281107894659e+00) (7, 2.19679941518646948406e-01) (8, -1.55638229383626314206e-01) (9, -1.82766939847718301593e-02) (10, -7.96619423971213075353e-02) (0, -5.99144582488953680866e+00) (1, -2.09394344728957765289e-01) (2, -3.49426546734344201095e-01) (3, -3.24442373257648131180e-01) (4, -2.32344636362563666809e-01) (5, 3.54209515457847068376e-01) (6, 2.66591765681031889201e+00) (7, 7.46683449637631113660e-01) (8, 2.85674543698592076080e-01) (9, -3.36791163121878886488e-02) (10, -4.09195459740385680814e-01) (0, -6.06876084566112639074e-01) (1, -1.22391540585628233462e-01) (2, -1.53667423902621974596e-01) (3, -2.78856080053916766115e-01) (4, -2.17265877901187703936e-01) (5, 6.13796867395051057303e-02) (6, 2.53435171516047619278e+00) (7, 2.32674021003776709815e-01) (8, -2.70334075228359574372e-02) (9, -1.72559952614061345999e-01) (10, -1.73726888070878410231e-01) (0, 4.19824878700640269358e+00) (1, 4.85446228738779073097e-01) (2, 5.38065447565073018410e-01) (3, 5.07358714099878316262e-01) (4, 5.40463998075479512551e-01) (5, 2.70676536930135220871e+01) (6, -5.02730709389523955366e+00) (7, 1.84454323258516672546e+00) (8, 2.10347886239124870489e-01) (9, 9.44380103917572433225e-01) (10, -5.52858919526953540569e-01) (0, 2.72343915427345883984e+00) (1, 5.81509212513625528551e-01) (2, 4.52084573467433359362e-01) (3, 5.07738391419112589098e-01) (4, 5.03441112597644235827e-01) (5, 2.69454812218776673660e+01) (6, -4.20409597441895677150e+00) (7, 1.88812036300681063139e+00) (8, 8.10751049266591750087e-02) (9, 4.10643151710877507554e-01) (10, 2.86114291393952807629e-02) (0, 2.52831160581571234847e-01) (1, -8.40205927192136797776e-02) (2, -9.29422569273874871199e-02) (3, -8.95277005433008782331e-02) (4, -1.13703582882778417296e-01) (5, 1.10867903537331335961e+00) (6, -1.33374452265183212063e+01) (7, 5.99018287061219734113e-01) (8, -1.63985909295632061380e-03) (9, -2.37998538536317860714e-01) (10, 7.94958683643274266473e-02) (0, 1.45830556309682529692e-01) (1, -8.15143182181998493530e-02) (2, -1.60207941436454268103e-02) (3, -1.43851083447901845203e-01) (4, -6.35715990566893818237e-02) (5, 1.07309426523028128564e+00) (6, -1.32129790233651878140e+01) (7, 6.27469625471597458599e-01) (8, -6.51819006872727049751e-03) (9, -2.83951533114109366807e-01) (10, -3.65028914479345739652e-02) (0, 1.42546728086660579038e+00) (1, 2.33441643629400952253e-01) (2, 3.66230037275879771741e-01) (3, 3.65926724139779002698e-01) (4, 3.15798219505875499280e-01) (5, 2.46169580312662681365e+01) (6, -2.20196869110356896471e+00) (7, -6.52177567549538261815e-01) (8, -4.88262963159329710372e-01) (9, 4.16899349590920897946e-01) (10, 2.43947410163666417171e-01) (0, 3.66517057962362391432e-01) (1, 7.45453099589859780050e-02) (2, -1.02739213885732982856e-01) (3, -3.84775448031073746824e-03) (4, -9.48844094891036493600e-02) (5, 4.10269689180366214920e-01) (6, -1.17390406545395697968e+00) (7, 7.22410931069170381136e+00) (8, 1.40644068354131707155e-01) (9, -2.07089349759870783307e-01) (10, -7.46434155041289654209e-02) (11, -2.85188569780319478575e-01) (12, 5.36313184719218405405e-01) (13, 6.24539527807274508220e-01) (14, 5.54131961649026449912e-01) (15, 3.90498710416000149870e-01) (16, 3.79425175338904019373e-01) (17, -3.51756149140143636878e-01) (18, -1.38603059216490770433e-01) (19, 4.34057382408182201861e-01) (20, 1.00789900507115740003e-01) (21, 3.19034192266994809373e-01) 
