FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.18510410879828320674e+00) (1, -6.27923932054471334308e-02) (2, -2.32468518255038114617e-01) (3, -7.10633423307370365452e-02) (4, -6.39671560504865011065e-02) (5, 4.74996452199959784402e-01) (6, -9.20782541654033420686e-02) (7, 5.71285966466165251454e-01) (8, 5.00220282567593654832e-01) (9, 7.78537523608267334474e-01) (10, -1.00578959278217111573e-01) (0, 2.70334759424136310269e-02) (1, 1.66854280545431915561e-01) (2, 1.39086205079275881946e-01) (3, 2.26133022024351926227e-01) (4, 2.16193552627283874790e-01) (5, 3.58718775493487740391e-01) (6, 1.27901770581909823843e-01) (7, 1.17948171820937183774e-01) (8, 4.95436633504134193995e-01) (9, 4.13470566996357591361e-01) (10, 4.81370125564988815547e-02) (0, -3.82964607125376899077e+00) (1, -1.23634642715331158347e-01) (2, -2.71180829758521202422e-01) (3, -2.84589319283839348174e-01) (4, -1.72740748877402400252e-01) (5, 1.41054406090892214820e+00) (6, -1.24518974153738967020e-02) (7, 4.77931911375839846290e-01) (8, 6.48749896099167777663e-01) (9, 9.52772129258708688759e-01) (10, -1.56180344129664000485e-01) (0, 2.28079142226722808573e+00) (1, 9.68718937565828386305e-01) (2, 1.05323481463387080836e+00) (3, 9.93093117107893053053e-01) (4, 9.80951881935144487379e-01) (5, 3.85589226664331974348e+00) (6, -9.78339429003907024018e-01) (7, 1.47649069030496882249e+00) (8, -9.61974328781133536381e-01) (9, -2.77389397234137380366e+00) (10, -8.74570472060785863988e-01) (0, 1.96227694491218018769e+00) (1, 2.00134131838503342982e-01) (2, 3.62763371099653619822e-01) (3, 2.37878385712328416224e-01) (4, 3.57709970106306451854e-01) (5, -1.79867763642347799902e+00) (6, 8.19655521480058268935e-02) (7, -4.28407752105515848839e-01) (8, 2.51938892435345840326e-01) (9, -7.31718289060069193575e-01) (10, 3.69708762602250828877e-01) (0, 1.99335473219226289032e+00) (1, 2.36758213271322709437e-01) (2, 2.15478986282768764093e-01) (3, 3.24872340788069202677e-01) (4, 2.73062277366342975871e-01) (5, -1.78004640538371550207e+00) (6, 1.75887168821475370351e-01) (7, -5.11900813469212900841e-01) (8, 2.37778935954137848885e-01) (9, -7.02567762089444158846e-01) (10, 4.04983476833741973255e-01) (0, 9.91783668423256514757e+00) (1, 2.69622250453158462946e-01) (2, 3.30477423027678518519e-01) (3, 3.89066114083453207240e-01) (4, 2.86300911561175375208e-01) (5, -2.74756741276051830170e+00) (6, 1.44278766582061152013e-01) (7, -2.81283456893691408141e+00) (8, 9.76666630252244205579e-01) (9, -1.76613483569670015605e+00) (10, 4.51399017862233631337e-01) (0, 1.60211436433441178906e+01) (1, 3.68261996618186482344e-01) (2, 5.00347390732442387495e-01) (3, 4.11101020834123143111e-01) (4, 3.80013611129199513350e-01) (5, -3.86279786909679723195e+00) (6, 1.06349440286208601902e-01) (7, -3.12707645711687165502e+00) (8, 5.18993745318148680390e-01) (9, -1.74521439665364819227e+00) (10, 4.90470800899712400245e-01) (0, 1.14196348627003008147e+01) (1, 5.53486255805726190538e-01) (2, 4.97155720244879861802e-01) (3, 5.49885002773041864366e-01) (4, 4.59159491460557123155e-01) (5, -4.95367219587063178921e+00) (6, 3.53263156454740268053e-01) (7, -3.57931301668271606431e+00) (8, 3.62501873394940332673e-01) (9, -1.25158365577713204253e+00) (10, 5.69461767317846700998e-01) (0, 1.75383488080188509883e+00) (1, 3.24521364944670709374e-01) (2, 2.48545648830626658699e-01) (3, 1.67046198921893485068e-01) (4, 1.63958641069625266073e-01) (5, 5.51907600477117288307e+00) (6, -9.25933613546099376634e-02) (7, -1.31677099385591600544e+00) (8, -1.00713548493484705837e+00) (9, -1.89062829837584689407e+00) (10, -7.03884157292646195359e-01) (11, 6.89048336019104534778e-01) (12, 1.43129970230809838716e-01) (13, 8.23996148710132048087e-01) (14, 6.16014373927561198485e-01) (15, -4.15583794998525130060e-02) (16, -4.14399003672002302912e-02) (17, -1.80646776510123091564e-01) (18, -1.43399841440562059169e-01) (19, -1.47801853073481370693e-01) (20, 5.65158079742708374660e-01) (21, 6.54613055623811113115e-01) 
