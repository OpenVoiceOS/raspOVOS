FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=17 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (17, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.55432884698478623875e+00) (1, 1.38066995350228438610e+00) (2, 6.79501800639078279431e-01) (3, 3.98788828141387496906e-01) (4, 5.60545677618693338751e-01) (5, -1.60258164406408276648e-02) (6, -1.46310684638371948929e-02) (7, 2.99842267440780352317e-01) (8, 8.16730310428324335348e-01) (9, 3.42832930876648012131e-01) (10, 1.46207399568680962432e+00) (11, 1.52780386824502611987e+00) (12, 1.91820266374597175885e-01) (13, 1.65936603592056730605e+00) (14, 3.07186557206189370106e-01) (15, 1.46146040194825910241e+00) (16, -5.34373537198941428450e-01) (0, 2.08174784259107248374e+00) (1, -7.43141533295045508822e-01) (2, -9.57282685466402161190e-01) (3, 7.99728028218204683997e-02) (4, -2.00666725582343496992e-01) (5, 3.92774430908378380245e-01) (6, 9.84781063850123927761e-02) (7, -9.43700286125771087287e-03) (8, -8.94381293555688516861e-01) (9, 1.61502619011254650028e-02) (10, -6.79807464801889027584e-01) (11, -7.64341226826177999598e-01) (12, 5.54589102490271046442e-02) (13, -7.67957771629367291766e-01) (14, 4.07274997763920967864e-02) (15, -8.81319232252031459218e-01) (16, 8.04965865579493566884e-01) (0, -5.47238363628951951512e+00) (1, 1.33289015499459395642e+00) (2, 8.02193532111571294685e-01) (3, 2.94587287790473384419e-01) (4, 4.70032150225352329809e-01) (5, -1.03398380219994992513e-01) (6, 3.12721076054224486618e-02) (7, 3.29693189965709343436e-01) (8, 8.53911330211344132834e-01) (9, 5.03378714187777021039e-01) (10, 1.43011498353604515898e+00) (11, 1.50927391667260790697e+00) (12, 2.56356808074960362820e-01) (13, 1.66757836387772062636e+00) (14, 1.80410040768658935795e-01) (15, 1.43424035393790982873e+00) (16, -5.30447848634164054182e-01) (17, 5.50233343599858493178e+00) (18, -5.10879092795077411893e+01) (19, 5.48004648445668429702e+00) (20, -7.69558471347231654036e-01) 
