FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.69278209321231258855e+01) (1, -1.67215458701995445256e-01) (2, -1.85920938502696586614e-01) (3, -1.93966589521316123967e-01) (4, -2.01213821422008110051e-01) (5, 2.23532596787803239380e+00) (6, 6.87441366512408613687e-01) (7, -5.90023579278583953922e-01) (8, 1.67144096109054385302e+00) (9, -3.11846392625023949297e-01) (10, -2.91149451542701720452e-01) (0, 9.65816000503472715799e-01) (1, 1.65624705394693455540e-01) (2, 2.89722291072794180256e-01) (3, 2.81916228374429922443e-01) (4, 1.96067278524824223362e-01) (5, 3.82877635868067056624e-01) (6, -2.90226126645542370497e-01) (7, -4.88434356948739378801e-01) (8, -5.79707444809678174025e-01) (9, 5.29849700913792645562e-02) (10, 2.00527519753883776144e-01) (0, 3.57537978441327108925e+00) (1, 4.22233526681176751794e-01) (2, 4.31611892853490664024e-01) (3, 5.05215604875794976891e-01) (4, 5.74294936929456323327e-01) (5, -6.66707359336693539120e+00) (6, 4.60202957589383843118e-01) (7, -3.56853744774046788280e+00) (8, -2.61327123302674424465e+00) (9, 1.23471715449784436025e+00) (10, 4.70133337906943626905e+00) (0, 3.48032519520133032387e+00) (1, 4.45560257223813538374e-01) (2, 6.12793411401479026779e-01) (3, 4.90494660375087321302e-01) (4, 6.07240597990720165633e-01) (5, -5.12557276406583284256e+00) (6, -5.90540919249252005407e-01) (7, -2.59777979668154834769e+00) (8, -2.68129483271098267849e+00) (9, 5.28239112382395181733e-01) (10, 3.07538840464625007343e+00) (0, 9.80325714887314014767e-01) (1, 2.60972778292873963668e-01) (2, 2.53182391616085633590e-01) (3, 1.15849013420322971557e-01) (4, 1.19338042082766132568e-01) (5, -1.33229128795649520711e-01) (6, 3.46780215944485215029e-03) (7, -1.00715069812924218340e+00) (8, -6.10627574533057160266e-01) (9, 1.25094716273497275072e-01) (10, 5.74546884918551159505e-02) (0, -1.68854133039743317113e+01) (1, -1.79469446991881387277e-01) (2, -2.91501964782675537791e-01) (3, -2.01506222938498513741e-01) (4, -2.88877900098761353220e-01) (5, 2.54176969585295209342e+00) (6, 5.89156868655314691630e-01) (7, -2.95187096354134148601e-01) (8, 1.80908807613937194780e+00) (9, -2.83490568971053236158e-01) (10, -4.43135908759074437224e-01) (0, 4.23079705443778902874e-02) (1, -9.07785487847652394766e-02) (2, -1.58880841501268466054e-01) (3, -2.20868252732071002065e-01) (4, -8.10783547835674245352e-02) (5, 4.23513827381071461531e-01) (6, 3.86224439914922168526e-01) (7, 1.32612930901546088869e+00) (8, 5.07991480545628149024e-01) (9, -8.09835156148663581899e-02) (10, 9.74658928624804604679e-02) (0, -7.22718135241080389397e-02) (1, 1.07837284235759972573e-01) (2, 1.30485685794159200768e-01) (3, 2.59833129374786642174e-01) (4, 2.01161118476196582039e-01) (5, 9.82537838242125038946e-01) (6, -1.57701077770027175973e-01) (7, -1.02961356712433964589e+01) (8, 7.13541756399881821338e-01) (9, 1.16089239731198806682e-01) (10, 2.64630508276497089515e-01) (0, 9.37588730097942524289e-01) (1, 2.40991272575739268191e-01) (2, 1.91266649014833756137e-01) (3, 2.32676528937700632937e-01) (4, 1.04387116230610199619e-01) (5, -2.78675680355930543985e-01) (6, -6.42075862031758481940e-01) (7, 8.33190143560651996779e+00) (8, -2.09448800239703469828e+00) (9, -7.03851577753718121722e-02) (10, 3.19999284559420449359e-01) (0, -1.11856826351697843935e-01) (1, 8.28220283438136778420e-02) (2, 1.37800995056765296365e-01) (3, 8.00687705923488340920e-02) (4, 1.40510385681560101556e-02) (5, 1.06070748264501757419e+00) (6, -1.58252593224636223246e-01) (7, -1.33382745440668184500e+01) (8, 8.15162305254586994074e-01) (9, 5.44716276211603678270e-02) (10, 1.99916610744266753930e-01) (11, 5.11951253753277879532e-01) (12, 5.66824728855918502646e-01) (13, -2.40102686733682485487e-01) (14, -2.67209973078036289174e-01) (15, 5.88525033623652937820e-01) (16, 5.96161273984232020773e-01) (17, 5.83377453457543992954e-01) (18, -1.92705262873567456650e-01) (19, 4.75937847566229643714e-01) (20, -8.06586035239116316831e-02) (21, 6.42721724553945938929e-01) 
