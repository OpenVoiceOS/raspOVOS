FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -4.75999320410586967967e+00) (1, -1.81419377366737938351e-01) (2, -2.25983453134731920864e-01) (3, -2.68892054597572982555e-01) (4, -2.02163298765854509975e-01) (5, 3.34582427700098272716e-01) (6, -5.17322729572256667652e-01) (7, 3.17540520575231699496e+00) (8, 9.85648674270652724871e-02) (9, -1.80520384102248770963e-01) (10, 4.38272140383003452890e-02) (0, 4.80533738708607245371e+01) (1, 2.14902358016484912318e-01) (2, 2.64416882655137741587e-01) (3, 3.18472893795484324198e-01) (4, 3.25466470679754038553e-01) (5, -6.68653311150930740325e+00) (6, 3.33962098844898747085e-01) (7, 8.73758015772283735956e-01) (8, -1.33621710315223141130e+00) (9, 2.73672972325553265360e-01) (10, 2.72413981193387277902e-01) (0, -7.68461786326956886040e-02) (1, 1.70570929692185901949e-02) (2, -9.26926339819036937406e-02) (3, -2.27419863297545002401e-02) (4, -4.01995907380186534574e-02) (5, -3.99315246938321100068e-01) (6, -1.38817009936195889797e-01) (7, 5.65105770636166759857e+00) (8, -1.38500100284965812353e-02) (9, -1.32430851754618272853e-01) (10, 1.23053553266320767068e-01) (0, -1.08231765654732442483e-01) (1, -5.96228556888176602392e-04) (2, -1.52385580854670937834e-01) (3, -9.90313249995876526688e-02) (4, -7.65448096563984270446e-02) (5, -2.21588419376465617372e-01) (6, -1.58056772728434907771e-01) (7, 4.75047809225148132128e+00) (8, 6.92930938392726791797e-02) (9, -1.20076311654845238608e-01) (10, -4.12811779901351566524e-02) (0, 1.18577523494128075909e-01) (1, 2.53109855554637767916e-02) (2, -2.86100992955627513037e-02) (3, -1.41168983675899577246e-02) (4, 9.72999034426746123305e-02) (5, -4.20339842820595610551e-01) (6, -5.80818798669758065700e-02) (7, -2.56189968440506365965e+00) (8, 7.89658358546065364258e-01) (9, 1.20256015351275280523e-01) (10, -2.86075382243276797600e-02) (0, 1.99866080204338913973e+00) (1, 2.70891073049729147471e-01) (2, 3.02385094465439485667e-01) (3, 2.49723370076840367471e-01) (4, 1.86846709700053154046e-01) (5, 1.17651160416795104169e+01) (6, 1.65592777240182703613e-01) (7, 6.41327590216087717323e+00) (8, 1.35422019358633001218e+01) (9, -6.63766097414721090164e-02) (10, 2.19918366355451561089e-01) (0, -2.04485347869292199974e+00) (1, 7.55676251874747573201e-02) (2, 1.56685366951066634700e-01) (3, 1.37632816754418962901e-01) (4, 1.76428518913823800807e-01) (5, 1.89775484906847480282e-01) (6, 2.64346726392473307943e-01) (7, 3.02982790762111653393e-01) (8, 8.94432756923413119221e-02) (9, 2.53627976029483392928e-01) (10, -9.06720674607779053567e-01) (0, 3.18867955766289207231e-02) (1, -4.58096223561355320175e-02) (2, 5.63085568459442409317e-02) (3, 4.36383163050920071246e-03) (4, -2.87448200075218242289e-02) (5, -1.56843120063419605437e+00) (6, 3.13613969854286411820e-02) (7, 6.47366988002118759482e+00) (8, 1.05541216918423996773e-01) (9, -7.19510439954669650531e-02) (10, -8.55326701706607961384e-02) (0, 1.93798034930915719087e+00) (1, 2.90452140273277925608e-01) (2, 2.06328186096375404412e-01) (3, 2.99926745595162080882e-01) (4, 2.58048268856232443369e-01) (5, 1.37113692875320936082e+02) (6, 6.50756760170543152588e-02) (7, -1.49611280604881482503e+01) (8, 2.74390110280191095171e+01) (9, -9.73440688240756468730e-02) (10, -8.85413035717564911309e-02) (0, 7.69316828743908609489e-01) (1, 2.86344709695999788401e-01) (2, 3.00707536639397310374e-01) (3, 2.47466186882679850534e-01) (4, 2.85620394052689241526e-01) (5, 5.78281287456363557453e+01) (6, 7.34921200802409885222e-02) (7, -1.32242180498928405541e+01) (8, 1.37287049719313554164e+01) (9, -2.03029196846714748981e-02) (10, -1.36326898631151355312e-01) (11, 5.44684248097139556144e-01) (12, -1.87222162436983863287e-01) (13, 3.19387622361284151395e-01) (14, 5.55544229375860010300e-01) (15, 1.90422898720905391101e-02) (16, 3.17084939281140176792e-01) (17, 2.04891740273030642516e-01) (18, 7.90807454598901893306e-02) (19, 3.51547002571197197973e-01) (20, 3.09514850156875409581e-01) (21, 1.74827460668688622336e-01) 
