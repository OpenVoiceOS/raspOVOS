FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -4.65101686785391488232e-01) (1, -1.58586946451250904166e-01) (2, -2.71220167302672288123e-01) (3, -1.18916955911700164550e-01) (4, -2.80853097462241130255e-01) (5, 1.99322612266855186958e-01) (6, 7.40084946313547842323e+00) (7, -9.03256190179389850670e-01) (8, -3.69087663763833351283e+00) (9, 1.34835537558891749299e+00) (10, -1.84076215115887564799e-01) (0, -2.19361356154012226982e-01) (1, 2.44804733014174560890e-02) (2, 5.55678089619361312290e-03) (3, 1.16794966673918840933e-02) (4, -4.43977377796105424079e-02) (5, 2.67334864871707933442e-01) (6, 3.42516268365749387836e-01) (7, -3.77941284999095961084e-02) (8, 1.56708934733652549554e-01) (9, -2.87191853084879517022e-02) (10, -2.14705258728542125457e-01) (0, 2.01743399217217600494e+00) (1, 3.16378802708349560202e-01) (2, 1.91104915640316036507e-01) (3, 3.50738958171568249167e-01) (4, 2.46762894741258648201e-01) (5, 1.14089711551631411979e+00) (6, -1.01152329678778478694e+01) (7, 4.40519227333392304757e+00) (8, 1.62344789909883346901e+00) (9, 6.07993848203051712176e+00) (10, 8.71304187217278730548e-02) (0, -1.43337797750255790952e-01) (1, 4.82694890488771441439e-02) (2, 1.79798241843370301685e-02) (3, 8.59685477008012149946e-02) (4, 8.31224706162598941939e-02) (5, 1.19926910868226224705e-01) (6, -2.91485570645608649265e-01) (7, -3.98387189138480410366e-02) (8, -5.75347575297998001242e-02) (9, 4.11980289455554163358e-02) (10, -2.91036790638344866622e-01) (0, -1.78980548327016347754e-01) (1, 7.50981815240834987835e-02) (2, -7.15795680065401708492e-03) (3, -5.52291922189737313764e-02) (4, 7.20790572545980251506e-02) (5, 1.00361283805291925941e-01) (6, 5.98922434618115340221e-01) (7, -1.50447483749747007442e-01) (8, -2.91734137895365217008e-02) (9, -6.10858044194715482655e-02) (10, -1.27655000960063075288e-01) (0, 7.19944774672485854339e-01) (1, 9.59484126475533605216e-01) (2, 1.03059979161330139696e+00) (3, 1.11287334194489395678e+00) (4, 1.12189392378636276781e+00) (5, -2.47016166869077374857e-01) (6, -9.53808565767131888435e+00) (7, -1.43088409287472906328e+00) (8, 1.40270699198935333563e+00) (9, -3.43140624930476656473e+00) (10, 2.63247606301566461351e-01) (0, 3.00496780260785800731e-01) (1, 2.72870619648499457277e-01) (2, 2.31126134151024537067e-01) (3, 2.43744437449974898913e-01) (4, 3.08549840801758734621e-01) (5, -1.37402337119034867463e+00) (6, -6.04560958176817742071e+00) (7, 2.34763306094457391282e+00) (8, 2.95230035818800651271e+00) (9, 4.91568005824367926238e+00) (10, -7.88019336501767642122e-02) (0, 3.26317121133320453552e-01) (1, 4.19616711812614895383e-01) (2, 2.46864622014164203367e-01) (3, 2.21111638026832868498e-01) (4, 2.31404905694126361571e-01) (5, -2.25728747158873449541e-01) (6, 3.26713201234148753471e+00) (7, 1.55356508856771213800e-01) (8, 9.51963774602712931028e+00) (9, -5.11959434969786197911e+00) (10, 1.06117388305807658355e+00) (0, -7.78668172642902334246e-01) (1, -1.53588129072706663925e-01) (2, -2.93463049321692093230e-01) (3, -1.36791674285452413695e-01) (4, -3.08138904958288717406e-01) (5, 3.24405656747763004866e-01) (6, 4.47996556000916079654e+00) (7, -3.28589375171036923717e-01) (8, -8.44592561643408984429e-02) (9, -3.20751129538030865795e-01) (10, -2.66865599041615664255e-01) (0, -4.97267616437246706695e-01) (1, -1.60259557091776694282e-01) (2, -1.95327256941382337851e-01) (3, -2.84810085916106181525e-01) (4, -2.86114973151270823859e-01) (5, 2.55020759566561339149e-01) (6, 7.14157558739166997697e+00) (7, -8.50950722752612742639e-01) (8, -2.96550299635248659769e+00) (9, 7.61716793287432913928e-01) (10, -1.35286227513627593799e-01) (11, 4.24605309627487048640e-01) (12, 5.30378194245249234839e-01) (13, 6.56457386739497494865e-01) (14, 3.22836703377678557736e-01) (15, -1.00954115050406155463e-04) (16, -1.06557224487418639169e-01) (17, 5.58369760402046555470e-01) (18, -1.28775434129813737272e-01) (19, 4.36000327866864378912e-01) (20, 4.59272704567977252399e-01) (21, 2.80569312568322848378e-01) 
