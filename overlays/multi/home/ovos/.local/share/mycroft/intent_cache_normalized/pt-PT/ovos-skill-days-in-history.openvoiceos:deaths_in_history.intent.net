FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 6.04834831214753787521e-01) (1, -4.89259050163546628520e-03) (2, 8.11463789555140202481e-02) (3, 9.67892827006884282071e-02) (4, 3.12751247611721319014e-03) (5, 2.17312907631812962661e-01) (6, -1.44499999880790710449e+03) (7, 2.10903798871549280625e-01) (8, 4.36864316061109370093e-01) (9, 5.75497072088229782993e-02) (10, 5.74662116556313093407e-02) (0, 6.86823488925188962995e-02) (1, 2.32305971789486309653e-01) (2, 2.44219398009903332358e-01) (3, 2.83237477648384583517e-01) (4, 2.11348337936527630454e-01) (5, 5.36814505729762913688e+02) (6, -5.87806554174925999234e+00) (7, 6.66998581118860456307e-01) (8, 2.94212820379543105176e-01) (9, 3.75829754928083703547e+00) (10, 4.98625625393492810655e-01) (0, 6.78455480430867674535e-01) (1, 1.14285967692758949621e-01) (2, 1.05032726570989998205e-01) (3, 1.23890056654837071859e-01) (4, 2.16076440557863680425e-01) (5, -3.00141718532278167775e-01) (6, -5.50769736939544518606e+00) (7, -4.41828068535870116529e-02) (8, -3.20038698175211078212e-01) (9, -2.28573149933233743525e-01) (10, -7.18931664419777849417e-01) (0, 5.90067965489617790587e-02) (1, -5.68943634541692541684e-02) (2, -1.89647966614550550490e-01) (3, -3.43852505000295446957e-02) (4, -3.28906789334478186215e-02) (5, 1.55745240928487627130e-01) (6, 1.46999999880790710449e+03) (7, -1.05608710018941561598e-02) (8, -7.58365682422668940799e-02) (9, -1.21193823198463196400e-01) (10, -1.95521230213856228586e-01) (0, 1.39230440421777568183e-01) (1, 6.06160982973279915953e-01) (2, 6.86737082428636513853e-01) (3, 7.31459065801801644469e-01) (4, 6.81379571087064705992e-01) (5, -1.57669868315869998732e+00) (6, 4.67879852834843401599e-01) (7, 8.28105802342283481288e-01) (8, 4.40747530874874460238e+00) (9, -7.50548881053023375642e+00) (10, 3.22779818018680320080e+00) (0, 6.93326124092710660562e-01) (1, 4.04454783599444284148e-02) (2, 1.16124876837797978701e-02) (3, 6.67833552758761161972e-02) (4, -8.86162956547060370882e-03) (5, 5.43211237328035737026e-02) (6, -1.44499999880790710449e+03) (7, 2.10673025550169229492e-01) (8, 2.56092778935998688361e-01) (9, -4.72597334635747740039e-02) (10, 1.29344646143575520147e-01) (0, 1.53764098990712944071e-01) (1, 6.29832481891863027457e-01) (2, 6.39233468026868978384e-01) (3, 7.40904597491972127798e-01) (4, 6.20723655373804250601e-01) (5, -3.60497243057483895967e+00) (6, -9.78807797928727740810e-01) (7, -8.31872910211649663204e-01) (8, 3.74139041346329159765e+00) (9, -8.10131054137025330419e+00) (10, 3.04318606793495671781e+00) (0, 6.07608326369626250418e-01) (1, -5.04520294550487718999e-02) (2, -2.52612878714297958357e-03) (3, 7.53010528799464839977e-02) (4, -4.80964166406223497807e-02) (5, 3.07631008074355991044e-02) (6, -7.67413994711868781451e+02) (7, 3.39510812165064124191e-01) (8, -2.02935188156563917072e-01) (9, -7.47104888750712986534e-02) (10, 6.57295721126516102606e-02) (0, 1.73574388331902473848e-01) (1, -2.81237263467311537912e-01) (2, -3.04602008547782576731e-01) (3, -2.18692820754050831766e-01) (4, -2.76417691495418282877e-01) (5, 1.91676549256224959539e-01) (6, 1.46999999880790710449e+03) (7, -4.68634797940863545573e-01) (8, -3.13769784910005355982e+00) (9, 3.84685926229495434026e-01) (10, -7.95700783488932916754e-02) (0, -1.54945867628954292439e-01) (1, 3.09916097225293785122e-01) (2, 2.97740552128896329709e-01) (3, 2.90159720481977023709e-01) (4, 3.27373999656781711209e-01) (5, -1.20103009588778419925e+00) (6, -5.41695645976358886031e+00) (7, 1.61290287985750713240e+00) (8, 1.26186963993008589568e+01) (9, 1.15993353131466356842e+01) (10, -3.58952870338745469447e-02) (11, -1.31118054104115040748e-02) (12, 3.80430372196166655119e-01) (13, 1.72881617520687230760e-01) (14, 5.47205943185442578169e-01) (15, -1.28564224088182660344e-01) (16, -1.73537336161160876680e-01) (17, -2.88495378650819478406e-01) (18, -2.81820936831976676107e-01) (19, 5.59162523168676983687e-01) (20, 4.94084411790657429542e-01) (21, 4.33442515052250565422e-01) 
