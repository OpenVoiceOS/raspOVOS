FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.55713302034371903204e-01) (1, 7.87278829630442059107e-03) (2, -1.18879022015966701042e-01) (3, -1.81341068758405998818e-01) (4, -8.91441422838347846680e-02) (5, 4.63679514299352057982e-01) (6, 1.03340310086601672168e+00) (7, -1.69104917712565139265e-01) (8, -7.85203252114485134028e-02) (9, 6.81346955753141247936e-02) (10, -1.36989043174086583265e-01) (0, 1.75157800597144763799e+00) (1, 3.09146636746142844387e-01) (2, 2.91346476993773972897e-01) (3, 2.96596238038753068356e-01) (4, 2.75433087370608842281e-01) (5, -4.37757514826689853038e+00) (6, 1.01073593660855003407e+01) (7, 1.09664095990911913958e+00) (8, -5.09566154180054819989e+00) (9, 1.18537711585071914300e+00) (10, -2.27248634439612712255e-01) (0, 1.55852519339197992387e+00) (1, 4.96639827366858965796e-01) (2, 3.96705703910380846899e-01) (3, 4.23634680088550097388e-01) (4, 5.39618598397761495278e-01) (5, -4.35523141908536803868e+00) (6, 4.74296221222020530917e+00) (7, 2.00959701777176640292e+01) (8, 2.47417025891339648069e+00) (9, -4.19647186830933360824e+00) (10, 1.98104327678499836374e-01) (0, 3.00275797375955721957e-01) (1, 2.18789246211562371514e-01) (2, 1.73629451821360691888e-01) (3, 2.57988807092223326745e-01) (4, 1.07516788224492218506e-01) (5, -6.95692343306340443831e-01) (6, -4.45347676318852503385e+00) (7, 3.70904470706663325075e-01) (8, 1.81670572583820977286e+00) (9, 7.79261481658854915366e+00) (10, -9.60163917765292362905e-02) (0, -1.16871173955087304108e+00) (1, -9.00599874133364702855e-02) (2, -1.33162430996195523675e-02) (3, -1.54212991851021358203e-01) (4, -5.68269114439647581027e-03) (5, 4.72788458645394871205e-01) (6, 1.16826306260571555207e+00) (7, -9.11484185601678792432e-02) (8, -1.23343599301597689477e-01) (9, 1.55877818371683241061e-01) (10, -2.12126642858597225239e-01) (0, -3.35756314513705522717e-01) (1, 1.42609152255911107721e-02) (2, -1.63388612562571061693e-01) (3, -1.28574100078683202508e-02) (4, 1.60893530234283168266e-03) (5, 2.87602162683090134543e-01) (6, 1.37936805552844377054e+00) (7, -1.72079634976603196295e-01) (8, -8.30823859176495482792e-02) (9, -3.41853703402707695402e-02) (10, -1.81071903308264309240e-01) (0, -1.80890083149512354010e-01) (1, 9.91556425339147270992e-03) (2, -3.97794507002218258873e-02) (3, 5.67958135849401871131e-03) (4, -1.14597678140340580866e-01) (5, 2.99955049964248154915e-01) (6, 2.96374377108339848164e+00) (7, -2.89365902120569062905e-01) (8, 6.05698460952454817430e-02) (9, -3.97668936114900950507e-01) (10, -1.06803073185589555982e-01) (0, 1.28363387214989438379e-01) (1, 1.64703782498891032615e-02) (2, 9.64643154144341902212e-02) (3, 1.05078781008725849477e-01) (4, 1.37177670359616921125e-01) (5, -2.76862056035125647657e-01) (6, -6.04774248813288450854e-01) (7, 1.37455641066140388240e-01) (8, -1.04155162394288902661e+00) (9, 9.75715173935707939801e-03) (10, 3.66058462080004698436e-01) (0, 6.37291943739752730913e-01) (1, 1.83048798966347098238e-01) (2, 9.69466336845742071571e-02) (3, 1.92505865621506150331e-01) (4, 1.27805813479362678775e-01) (5, -7.39987962107018892866e-01) (6, -4.35613429475923297218e+00) (7, 8.06113515901103028227e-01) (8, 1.43333817228427218282e+00) (9, 6.55772016013230274467e+00) (10, 1.19046542162700302869e-01) (0, -2.91671822382668810558e-01) (1, 1.57397962532655859291e-02) (2, -1.34064429082257838277e-02) (3, -5.22238560594899883793e-02) (4, -5.21639161385877361821e-02) (5, 4.93292084053913115760e-01) (6, 3.27612577847618768878e-01) (7, -1.71354001564512720135e-01) (8, 6.28231453186695182334e-02) (9, -1.50540653179244872384e-02) (10, -1.21078594185833096630e-01) (11, 5.20111870973220757897e-01) (12, 4.26507136271256637183e-01) (13, -2.86744704797242067063e-01) (14, 4.93603930153575176654e-01) (15, 5.48215273031150696070e-01) (16, 5.17223912774136485027e-01) (17, 5.35021167223255211809e-01) (18, -1.33876396903444050412e-01) (19, 5.33897370376315016927e-01) (20, 4.98744613584334028911e-01) (21, 2.79168477621036748015e-01) 
