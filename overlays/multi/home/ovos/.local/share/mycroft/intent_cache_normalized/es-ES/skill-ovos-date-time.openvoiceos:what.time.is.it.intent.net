FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.30406831290037006355e-01) (1, 3.63243907236908636360e-01) (2, 3.12349333906029424934e-01) (3, 4.84502598190163225045e-01) (4, 4.30016003095959276070e-01) (5, -6.03797749638589564825e-01) (6, -1.40777224500003450558e+00) (7, -1.33713445062911784333e+00) (8, -1.25431945352230678203e+00) (9, -1.34135307992218266726e+00) (10, -1.35908966677868253692e-01) (0, 8.43743088434003696285e-01) (1, 4.06956025882524941029e-01) (2, 4.30967429204744789661e-01) (3, 4.08337691350740883411e-01) (4, 4.35953998490137550892e-01) (5, -5.94683654049488641391e-01) (6, 4.30996400798137901944e+00) (7, 3.79031465963546949638e+01) (8, 1.63426335911861393413e+01) (9, 4.40961056219518354737e+00) (10, -4.55555057162662924153e-01) (0, 8.08528276870511874996e-01) (1, 5.75602734543119387567e-01) (2, 5.58416986919675784051e-01) (3, 5.58672541834150271356e-01) (4, 5.46492422081266360223e-01) (5, -3.45911133321391395246e-01) (6, 4.43569088289912460965e+00) (7, 3.78728578404609876884e+01) (8, 1.62131546746602985820e+01) (9, 4.40864466892659834230e+00) (10, -6.37727922448212858342e-01) (0, 7.14873525092862838193e-01) (1, 4.31400978489679787220e-01) (2, 4.09904757066530678333e-01) (3, 3.02334660216135420185e-01) (4, 4.66295489593309908649e-01) (5, -5.81542200783344842563e-01) (6, 4.33401368866141556424e+00) (7, 3.77293680847708898796e+01) (8, 1.63138661395421955547e+01) (9, 4.26861346529901286573e+00) (10, -4.08022893602272562763e-01) (0, 7.65321555326246016904e-01) (1, 4.46323344035906288685e-01) (2, 3.78302083833975233418e-01) (3, 4.26430725975794289173e-01) (4, 2.97295027895731367451e-01) (5, -5.72389423290344812045e-01) (6, 4.11940555760468729574e+00) (7, 3.79168347732250410331e+01) (8, 1.63658698361030552348e+01) (9, 4.26715084927261134595e+00) (10, -3.54095412427803568622e-01) (0, 1.01223380290687217098e+01) (1, 1.11380990045662889298e-01) (2, 2.58953380780812314477e-01) (3, 1.24129358130093583878e-01) (4, 2.88079548078175540216e-01) (5, -5.81561216939008793214e-02) (6, -2.12737440545741263165e+00) (7, -9.94226604897281207585e-01) (8, -1.69575767502900287553e+00) (9, -8.22781338881229284965e-01) (10, 3.17355700723247535766e-01) (0, 1.82652181380216571682e+00) (1, 5.92764613981553223354e-01) (2, 5.89715776796647217495e-01) (3, 6.16408994252988007290e-01) (4, 6.67785962875195648891e-01) (5, -7.31372955822089276801e-01) (6, -2.94555947527397554708e+00) (7, -1.45916587782064599566e+00) (8, -1.24594203364216293828e+00) (9, -1.08689071766726441659e+00) (10, -3.81766580927781318255e-01) (0, 5.00423760797292560554e-01) (1, 4.14363078319882005562e-01) (2, 3.29376197898243627815e-01) (3, 3.66635895812367162971e-01) (4, 3.23136619770382604866e-01) (5, -7.74211683332952205205e-01) (6, -1.40157026995244615719e+00) (7, -1.43042744691646372956e+00) (8, -1.14975885888703488469e+00) (9, -1.36096209908245335818e+00) (10, -1.19742153006279369865e-01) (0, 5.10353089954168170905e-01) (1, 3.39783131623123846321e-01) (2, 3.68958521181200704842e-01) (3, 4.08556453430508226266e-01) (4, 3.98405074620102495064e-01) (5, -7.40099949312059690598e-01) (6, -1.41146721143546693966e+00) (7, -1.29896534974849497956e+00) (8, -1.02191613198806385832e+00) (9, -1.26900284612892400027e+00) (10, -8.62383747274516887504e-02) (0, 4.87670557136788529462e-01) (1, 4.49180124815897807711e-01) (2, 5.07326825078921128664e-01) (3, 4.12868736800150737398e-01) (4, 5.21451203640894700442e-01) (5, -8.37572921660117652998e-01) (6, -2.37247473923549501151e+00) (7, -1.90812901109396970689e+00) (8, -1.17640537945900569561e+00) (9, -1.44823960005222374114e+00) (10, 2.04348339987066085488e-01) (11, -1.87046835825589952140e-01) (12, 3.08257882334646182709e-01) (13, 3.95612117685731845551e-01) (14, 3.77806362547334628754e-01) (15, 3.49354368485864597016e-01) (16, -1.86312478825355620771e-01) (17, -2.75111215560798594293e-01) (18, -4.47493294313521583483e-01) (19, -4.73198768666015401330e-01) (20, -2.48908364733995024487e-01) (21, 3.84216097034868198090e-01) 
