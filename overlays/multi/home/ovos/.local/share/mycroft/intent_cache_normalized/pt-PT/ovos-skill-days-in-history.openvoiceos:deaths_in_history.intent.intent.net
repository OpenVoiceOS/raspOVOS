FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.50000000000000000000e+03) (1, 4.14721023591103354899e-01) (2, 4.11012854726853171794e-01) (3, 5.13599444480481115427e-01) (4, 3.84325612636151225932e-01) (5, -4.45775292409408052663e-01) (6, -4.50041331361433435632e+02) (7, 1.16303063273463180849e+00) (8, 4.63366258530517285408e+01) (9, -3.66020015293344069818e+00) (10, 1.71548466900970697502e-01) (0, 8.01557697531149804249e-01) (1, 5.17666879119717981972e-01) (2, 6.96523319008194863322e-01) (3, 5.48169693919265177406e-01) (4, 5.59665190996968098247e-01) (5, -2.90110066050468828891e+00) (6, 2.86015662090790101502e+00) (7, 1.25468374723228981438e+00) (8, 4.30131227279772421213e+00) (9, -4.40801691021659269865e+00) (10, 8.74153572007466128646e-01) (0, 1.32810906429063479983e+00) (1, 3.60817414686452175854e-01) (2, 2.48066180363665345077e-01) (3, 3.08991697594891812084e-01) (4, 3.87187358901273037670e-01) (5, -4.80121203454467959659e-01) (6, -5.23825880598038118308e+00) (7, 1.74837423588580320732e+00) (8, 4.56188324391640254163e+00) (9, 2.84687038052615504213e+00) (10, -2.54479683088855934692e-01) (0, 7.97913291252410203924e+02) (1, 6.19162308179783904549e-01) (2, 5.61692422383475165049e-01) (3, 5.89539209508347483712e-01) (4, 6.29495466255593383309e-01) (5, -6.47384796199003131889e-01) (6, -4.50041331361433435632e+02) (7, 5.01636939909592793896e-01) (8, 4.88685677991039497670e+01) (9, -4.35299425733757150425e+00) (10, 3.01174670442688474647e-01) (0, -2.62746431522659029678e-01) (1, -2.09685391096556678070e-01) (2, -3.75467179624521973924e-01) (3, -2.39042764811003755065e-01) (4, -2.89474582581007833237e-01) (5, -1.51381016089493969368e+00) (6, 7.74819058386443202835e+02) (7, -2.72242876383681275954e-01) (8, -2.40745681193742067094e+00) (9, 6.05262307234316287996e-01) (10, -1.59403267423263494962e-01) (0, 2.00984928736458942633e-02) (1, -1.49008547538586555437e-01) (2, -1.04863514775105470811e-01) (3, -9.91654373944482608882e-02) (4, -1.17660725140639313424e-01) (5, -6.01345611651552136578e+00) (6, 3.04095826735225616133e+00) (7, -1.77508758480341699570e-01) (8, -5.45143713316739941810e-01) (9, -8.80061080757577329781e-02) (10, 7.74834688492649359404e-02) (0, 8.00190195220089939276e-01) (1, 6.44384929241607951056e-01) (2, 5.45338646771382773437e-01) (3, 5.98939602525900172125e-01) (4, 6.14603863360355662238e-01) (5, -3.63143990174083830524e+00) (6, 2.87231397913706087976e+00) (7, 1.33657085425072597040e+00) (8, 5.50012026280414101365e+00) (9, -4.49647146228112593036e+00) (10, 8.42084386842523402450e-01) (0, -1.56280073606763103555e-01) (1, -8.56859716010428262845e-02) (2, -1.47023807687792912313e-01) (3, -1.18757988734278771559e-01) (4, -8.54619326424933267727e-02) (5, -5.75348691337144600766e+02) (6, 7.74819058386443202835e+02) (7, -2.89733215241610164714e-01) (8, -3.56299494186914145200e-01) (9, -2.23114392871855193867e-01) (10, -1.48696758722581590106e-01) (0, 1.67006666443431561753e+00) (1, 2.78335408759093461128e-01) (2, 4.11473923814272390942e-01) (3, 4.28618410122370119275e-01) (4, 2.94847832512831753426e-01) (5, 1.01929327399611691085e+00) (6, 7.74819058386443202835e+02) (7, 6.44005669594491934049e+01) (8, 5.89582145133883184229e+00) (9, 2.28355538532121915551e+00) (10, 4.34140537474689558461e-01) (0, 1.40643363496614347241e+03) (1, 7.01487325047422838331e-01) (2, 6.99187643741537523390e-01) (3, 6.28258235668112230421e-01) (4, 6.33855491792131853224e-01) (5, -1.23855654617202537437e+00) (6, 2.21889850537228605631e+00) (7, 1.36721521010163948873e+00) (8, 4.57239873549971846955e+01) (9, -5.31184224481738720414e+00) (10, 1.54588616548619461888e+00) (11, -1.38902336562875594606e-01) (12, -1.34436652039920556678e-01) (13, 4.95186057854583028259e-01) (14, -1.06953263486627522805e-01) (15, 5.20771814209978867183e-01) (16, -5.18992728218214183222e-01) (17, -1.42890995311774987586e-01) (18, 4.78434358523089298121e-01) (19, 3.54545630801145927880e-01) (20, -1.41777133097847757304e-01) (21, 3.00844528317858406208e-01) 
