FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 5.09727553317038464820e-02) (1, -1.07175240463297013344e-02) (2, -1.75945291465799402708e-02) (3, 6.02361580186803399761e-02) (4, 1.24235061758477868116e-02) (5, 3.95114816088724962739e+00) (6, 2.35036032328455740670e-01) (7, 2.86076875967747334961e-01) (8, 3.43702010169250754545e-02) (9, -1.52728789504059676574e+02) (10, 1.90012152571388981404e-01) (0, -1.77328669391016019441e-01) (1, -9.53574751473826483228e-03) (2, -2.76511785850660288255e-02) (3, -4.58739278183118784349e-02) (4, 5.02859326734407460768e-02) (5, 5.90850348211049647418e-01) (6, 6.39634259712252573049e-02) (7, 3.60926713574895308145e-01) (8, 2.33060350985314981842e-02) (9, 1.30456330754705929209e+00) (10, -6.77000873001766673154e-02) (0, 3.01265050422193647606e-01) (1, -3.59932952331039682226e-02) (2, -4.69442480014297738866e-02) (3, 2.26064524842764860701e-02) (4, -6.59887850986454194135e-02) (5, 8.35982483440218970827e-01) (6, -1.46498690506437090342e+00) (7, -1.02200472795834929052e+00) (8, -6.27764744531136842376e-01) (9, 1.31688676092688666808e-01) (10, 1.67571139216443260045e-01) (0, -1.24207795522550554224e-01) (1, -9.34225593663615301587e-03) (2, -2.81294090018407785814e-02) (3, -1.30350115643514785635e-01) (4, -1.02362993107809260862e-01) (5, 1.99549241022400791756e+00) (6, 8.61416934709105935886e-02) (7, 4.37466119583431289897e-01) (8, 1.65475896290551566237e-01) (9, -3.30030659207900201046e-01) (10, -1.94761574949258081668e-01) (0, 4.15113174266980955451e+00) (1, 3.48771594405106633019e-01) (2, 4.33523714482716537866e-01) (3, 3.59036743581227391076e-01) (4, 3.80342800110510914635e-01) (5, -1.05749815247279932429e+00) (6, 2.21368529583945122807e-01) (7, -6.47731837293028078761e+00) (8, 4.10345965757656638839e-01) (9, -3.14233107225848573574e-01) (10, 2.88450415019788852788e+00) (0, 8.92839916686068851837e-02) (1, -6.94845397864617964068e-02) (2, -2.54492317234315604257e-02) (3, -3.62685476337709158945e-02) (4, -3.68616287504472464609e-02) (5, 4.28223923445866994086e+00) (6, 1.52965584823935324410e-01) (7, 5.77222243769542786573e-01) (8, 7.29298613324386724166e-02) (9, -1.52689994494804267333e+02) (10, 2.27546375188225541430e-01) (0, -7.75032052019634098983e-02) (1, 2.49122502937181578009e-02) (2, -7.38187407113474573783e-03) (3, -4.55656920529764856986e-03) (4, -1.38809959338678512442e-01) (5, 1.93259667263560230843e+00) (6, 1.17579202232793375571e-01) (7, 3.98062263782802627787e-01) (8, -1.49811253390633031257e-01) (9, 3.22036488746382243420e-02) (10, -3.93293916921999275882e-02) (0, -1.47046313546041501619e-01) (1, -9.76231967069071844556e-03) (2, 3.90789183511598761656e-02) (3, 5.52727009199696986408e-03) (4, -2.62637709237498054651e-03) (5, 1.99754039601616795174e+00) (6, 2.55018218767195589680e-02) (7, 3.52296126242462204203e-01) (8, -1.22778989813648639617e-01) (9, -2.95410083601077122495e-01) (10, -2.85017162304308166454e-02) (0, -3.24904674537645987709e-01) (1, -1.19758430765585288658e-01) (2, -1.17783710257725104942e-01) (3, -4.62195313438934149430e-02) (4, -9.48496124610465340954e-02) (5, 5.64968824559730897761e+00) (6, 3.32603713573228376710e-01) (7, 9.10712781142867133788e-01) (8, 6.88074453241585004548e-02) (9, 5.50391247258649185525e-01) (10, -4.57436402735379132878e+00) (0, 8.19014341423002917297e-02) (1, -3.69094689284600943613e-02) (2, -1.24329925039891209249e-01) (3, 1.00163961852751028786e-02) (4, 3.08058406318388114387e-02) (5, 5.36659113768930406962e+00) (6, 2.86914454827158660333e-01) (7, 5.07289708478824818982e-01) (8, 9.80000964656097817151e-02) (9, -1.52778884145804767059e+02) (10, 1.34430798846401577951e-01) (11, -3.38114196323411719813e-01) (12, 3.64984125322402153113e-01) (13, -4.47499485924818807270e-02) (14, 3.68193140833565968340e-01) (15, -1.21689823809553218648e-01) (16, -2.50368996011256617074e-01) (17, 4.21006704353496186144e-01) (18, 5.01651047062544042099e-01) (19, 8.57191173034731745961e-02) (20, -2.76486685918807373330e-01) (21, 4.20659555530827344860e-01) 
