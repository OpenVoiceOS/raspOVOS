FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.36763237057056130652e+00) (1, 6.75040627266549675412e-01) (2, 6.45415643598222343869e-01) (3, 6.95633868123674004025e-01) (4, 6.57420372958564369625e-01) (5, -4.81852863738274805883e+00) (6, 7.77415162551997052098e-01) (7, 7.27118315446831142701e-01) (8, -5.79717010092306672675e+00) (9, -4.88983850508613704378e-01) (10, 1.61985670365947886928e+00) (0, -8.53062425816166425285e-01) (1, 1.60248859453484410187e-01) (2, 1.45303590822502964874e-01) (3, 2.67733907926365644059e-01) (4, 1.53218890208289021393e-01) (5, 1.59365864362330455606e+02) (6, -9.71243074116629467873e-02) (7, 3.60423605669330307522e-03) (8, 1.20511460834518890906e+00) (9, -2.31639047622610033672e-01) (10, -1.07905835787258375102e-02) (0, 2.80249895476583799070e-01) (1, -2.66169733638977924761e-02) (2, 8.86303416536090606093e-03) (3, -2.22116238708710590777e-02) (4, -1.43110626869938334371e-01) (5, 2.88500554612060717830e+01) (6, 3.94980333027354288400e-02) (7, 1.07579678453574731356e-01) (8, -7.12460997500479309430e-01) (9, 1.21576589283953845344e-01) (10, 3.33377772998519270042e-02) (0, 1.17626795690409080564e+00) (1, -3.87789214425720701884e-02) (2, -3.35284674935974608134e-02) (3, -5.28257059329189718189e-02) (4, -9.42241305881179741499e-02) (5, -1.56859062015560709824e+02) (6, 4.54396296522460274581e-01) (7, 1.03397831430733733171e+00) (8, 3.00320008760286139005e-01) (9, 7.34271683410343101173e-01) (10, 5.11925634308944998985e-01) (0, 2.38720857737388314490e+00) (1, 5.39425099642813932199e-01) (2, 5.82326224448025953073e-01) (3, 6.94010818751395586190e-01) (4, 5.95528206230939161081e-01) (5, -5.02425828941203178601e+00) (6, 8.75793033216102911886e-01) (7, 6.95119285049341395499e-01) (8, -5.62214229162670342532e+00) (9, -5.68303171127719886080e-01) (10, 2.24654046079054436902e+00) (0, 2.73610250972990298735e-01) (1, -2.94621058656932902076e-03) (2, -5.32985157604432338574e-02) (3, -6.83020255799508396732e-02) (4, -1.65908565306208094503e-01) (5, 6.51810480932864777515e+01) (6, 1.10149028080414851383e-01) (7, -5.43421728833812442416e-02) (8, -2.28587266722048099155e+00) (9, 6.51847093677627442698e-02) (10, -7.61819008084855670004e-02) (0, -5.56676634174613549355e-01) (1, 2.74604781260816332988e-01) (2, 1.80263987472383119259e-01) (3, 1.50682813217965672070e-01) (4, 2.05261698654023744259e-01) (5, 1.59191173027963657205e+02) (6, -5.91048609762516274913e-01) (7, -3.69425670831319219456e-01) (8, 8.44092710096816034593e+00) (9, 2.14681684123830707200e-01) (10, 7.10347434644713893181e-02) (0, -1.22363423615329175442e-01) (1, -9.69197505874579007257e-02) (2, -2.01697243170255863243e-01) (3, -1.33211118893140967323e-01) (4, -2.77778336567823837255e-02) (5, 2.26834387215691668516e+00) (6, 2.50977259861327461898e-02) (7, 1.69440955466299197907e-01) (8, 3.42782137450343360463e-01) (9, 1.35061427168964437451e-01) (10, -9.89199735730492413222e-02) (0, 1.49514330700220621218e+00) (1, -1.67217932993673872033e-02) (2, -1.01314105873753931841e-01) (3, -4.14563052708410845204e-02) (4, 9.09668646400666885965e-02) (5, -2.70713269877893125681e+00) (6, 5.21107089086662589317e-01) (7, 8.91971472654615404885e-01) (8, -2.87845954767518841244e-01) (9, 7.01451729284287450739e-01) (10, 4.43045642539154405082e-01) (0, 2.53446042511816030385e-02) (1, -1.58373520622074992481e-01) (2, -1.91603594372094065967e-01) (3, -3.38297196720246870494e-02) (4, -1.74672272900164515796e-01) (5, 2.07572043076445877929e+00) (6, 1.87263583837637970220e-01) (7, 1.21682404359459307025e-02) (8, 3.70743652542360302427e-01) (9, -3.33297671234095357895e-02) (10, -1.62198514541944144307e-01) (11, -9.00745251272324631842e-02) (12, 4.17510422999195385252e-01) (13, 2.16125671971553856565e-01) (14, -3.43330720003686279407e-02) (15, -1.01165936240971504612e-01) (16, 2.75988913316390749308e-01) (17, 4.17714061229418065491e-01) (18, 4.78046081031830494279e-01) (19, -1.04398814872426734524e-01) (20, 4.46570313092689075152e-01) (21, 4.29132536636103900651e-01) 
