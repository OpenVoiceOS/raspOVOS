FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.66832361340518042780e-01) (1, -1.36503106185588696242e-01) (2, -1.75274482158754192629e-02) (3, 1.56700441451932612202e-02) (4, -9.64120236912710537602e-03) (5, -4.98808227883196064756e-01) (6, 3.49688828083897007559e+00) (7, -7.44423144328924757396e-01) (8, 3.25409927647371836554e-03) (9, -8.54965217432059575164e-03) (10, -1.78357116191900394453e-01) (0, 1.35000545501704205531e-01) (1, -1.77312066940400921145e-02) (2, -1.18507662990484097243e-01) (3, -8.89677724031542188632e-02) (4, -1.22623456844482267569e-01) (5, -9.59627691760713941171e-01) (6, 3.78331128496744995005e+00) (7, -6.60262597564061137767e-01) (8, -9.94451656075213685781e-02) (9, 3.85218643395039000987e-02) (10, -9.01864400438133484439e-03) (0, 4.43841255903196715837e-02) (1, -6.31818493990038282382e-02) (2, -1.00746026424501644581e-02) (3, -6.39576113013361341464e-02) (4, -1.76550390401277401686e-01) (5, -4.35678527368253520891e-01) (6, 3.40668773985898054590e+00) (7, -5.96866764891478163158e-01) (8, -9.76001634331438994163e-02) (9, 1.89188949743234260947e-03) (10, -1.74994083689726021014e-01) (0, -8.63740099236436131847e-01) (1, 1.99599495532473358850e-01) (2, 2.55544489624460913202e-01) (3, 2.28155782821139130334e-01) (4, 1.52709251644572052697e-01) (5, 1.90767249940952687837e+00) (6, -2.32974415363971631976e+00) (7, 9.25678428327184832369e+00) (8, -2.18426487002452346387e-01) (9, -5.47819162718173185977e-02) (10, 7.37257096754324187105e-02) (0, 3.94590101127769321554e-01) (1, 4.86880897297889581044e-01) (2, 5.15667053624422000446e-01) (3, 5.16757662161619113483e-01) (4, 6.27318194820911445575e-01) (5, -1.88426040623732249379e+00) (6, 6.35621562760418612470e+00) (7, -1.55043858513534416055e-01) (8, 9.46611500195369104382e-01) (9, 4.43411745709203886090e-01) (10, 6.84190206789997312775e-01) (0, 2.48377134079079233686e+01) (1, 2.33849527691820852082e-01) (2, 1.64642235641697637361e-01) (3, 1.15603285287836754502e-01) (4, 1.72867620741347066682e-01) (5, -4.57326479237332961247e+00) (6, -9.93244180691575828668e-01) (7, -5.85131382055313897794e-01) (8, 2.45532196810593794645e-01) (9, -1.62998386143539980564e-01) (10, 3.42252494603163093245e-01) (0, 6.68803318005405489366e-01) (1, 5.98979914058272222555e-01) (2, 5.17021836210314500448e-01) (3, 5.47690895814720857260e-01) (4, 5.52569278944555986044e-01) (5, -3.50546162879632516862e+00) (6, -1.33798514791911782318e+00) (7, -5.41038873504091188948e-01) (8, 2.11488852582041003680e-01) (9, 3.49919055591819794238e-01) (10, 9.13791777347428890721e-01) (0, 2.86961708639892447426e-02) (1, -1.56573399573690097419e-01) (2, -3.90899885894123375052e-02) (3, -9.06561721564595052181e-02) (4, -2.62611914874378744866e-02) (5, -5.21949991987562533780e-01) (6, 3.32759530886015353346e+00) (7, -5.28713139970431567072e-01) (8, -5.75654198353997420545e-02) (9, -2.58618418660884197280e-01) (10, 1.13382042290331844225e-01) (0, -8.33910657307426084195e-01) (1, 1.37817847069045346098e-01) (2, 1.88962566192885622618e-01) (3, 4.61547919471465908914e-02) (4, 3.29686351974212490945e-02) (5, 5.88454891143841085643e-01) (6, -1.40793757090686244560e+00) (7, 8.63320878188776341666e-01) (8, 6.00988672971871085160e-02) (9, -7.33570947301733222723e-02) (10, -7.86435150933342125290e-02) (0, -3.48553146133666835738e-01) (1, 2.80638900177231309652e-01) (2, 2.39893746154060782949e-01) (3, 2.94159409181824205159e-01) (4, 1.77727591769447718084e-01) (5, 1.45942409075543917396e+01) (6, -7.27438950542991591419e+00) (7, 1.98240716623480004444e+01) (8, -2.69509166338421313203e-01) (9, 4.55567610914360598962e-01) (10, 1.43759359984651557962e-01) (11, 4.70310893633665128011e-01) (12, 3.92306019344629330892e-01) (13, 4.67612166681589169759e-01) (14, 4.45416606804934278596e-01) (15, -5.76909049867650944310e-02) (16, -2.89084541038058473461e-01) (17, -7.37678083431895503796e-02) (18, 4.27624680825771374959e-01) (19, 4.37870822689142957795e-01) (20, 3.84323030552473798860e-01) (21, 4.28320412942575778903e-01) 
