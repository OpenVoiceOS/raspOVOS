FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.81579828923105346394e-01) (1, 1.67550139622705290821e-01) (2, 1.33506276206987267718e-01) (3, 1.92192800836580107715e-01) (4, 1.76374055938737700489e-01) (5, -4.51086632161607603564e+00) (6, 9.68357582028296359766e-02) (7, 1.71361243050652967135e+00) (8, 6.53180005123930595978e-02) (9, 3.78122972865268138776e+02) (10, 1.35409362397218818730e-01) (0, 4.89036820610270961396e+00) (1, 5.20150599613153574730e-01) (2, 5.32324170305692900840e-01) (3, 5.24954488112890360618e-01) (4, 5.29335563614332316185e-01) (5, 1.31177684901948676810e+00) (6, -3.22823980433010582214e-01) (7, -3.43854246604243618535e+00) (8, 8.13873175995441289210e-01) (9, -7.17435826383032182463e+00) (10, 1.92153893100525685611e+00) (0, 4.26547328305745154786e-01) (1, 2.10112677568272093742e-01) (2, 2.31064708584621886223e-01) (3, 9.18154620171343499591e-02) (4, 1.52220429533794859855e-01) (5, -4.48607377253990247112e+00) (6, 1.51549650947168462745e-01) (7, 9.09262985044407123780e+00) (8, 3.85685763949520865612e-01) (9, 3.78113280937686113248e+02) (10, 1.59494888264180123549e-01) (0, 1.26319532292159114606e-01) (1, 2.28373059809048234436e-01) (2, 5.06422089336706487961e-02) (3, 9.56342535017324912872e-02) (4, 1.13276870727379436188e-01) (5, -4.84346739814175375471e+00) (6, 6.78339441885011384947e-02) (7, 6.54121914340771493102e-01) (8, 5.38091459204865368848e-02) (9, 1.65264725825595405695e+02) (10, 1.88206403278232292653e-01) (0, 1.11112774214668288408e+01) (1, 4.26057685283628972961e-01) (2, 5.99114652840105677001e-01) (3, 5.33669244615046167723e-01) (4, 6.13945242134539270751e-01) (5, -2.90117106188148587975e+00) (6, 1.38261850243223044510e-01) (7, -5.89683294140306291098e+00) (8, 9.76487279537659680351e-01) (9, -1.75115601957554822832e+00) (10, 2.62791642438589478914e+00) (0, -3.65690749152318694826e-01) (1, -1.21284407576630920422e-01) (2, -8.98815216627820712247e-02) (3, -1.72843699356625996044e-01) (4, -1.84778240522454700878e-01) (5, 7.79313514076288015531e+02) (6, -2.04754636140961260304e-01) (7, -2.35352040435864912071e-01) (8, -3.79442349892778418852e-01) (9, 1.45688732923880126524e-01) (10, 1.45873195569689251272e-01) (0, 3.66081133243542772293e-02) (1, 3.26731336782433504085e-02) (2, 1.72136334250428089943e-02) (3, 4.17474193046547814578e-02) (4, 1.09708665079877573656e-01) (5, 4.25321725258970388950e+02) (6, 1.11218163554052271169e-01) (7, -8.12798260819833195967e-01) (8, 8.32186237331874051337e-02) (9, -1.85861286890406951500e+00) (10, 8.55960303665736638168e-02) (0, 2.25038215277677616788e-01) (1, 2.83897851324583339050e-01) (2, 2.31761787748838710144e-01) (3, 2.54938480175043391540e-01) (4, 1.48206670499349824066e-01) (5, 2.43425290936256688568e+00) (6, -1.87640182400004157381e-01) (7, -1.71834108601494572888e+00) (8, 9.26357710836351655459e-01) (9, -4.91937547708885958286e+00) (10, 3.02751157959176508960e-01) (0, -5.15751444368063194368e-01) (1, -3.93050331022929688718e-02) (2, -9.65221989300440730286e-02) (3, -7.00981635000895442200e-02) (4, -1.59993434264917383514e-01) (5, 1.28964341264802628828e+00) (6, -4.94279327269832036151e-02) (7, 1.56891620786722751912e-01) (8, -8.61186246993667081640e-02) (9, 1.00892350509359740585e+00) (10, -2.42826527923060059289e-01) (0, 4.78988288737538462497e-03) (1, -3.75454561912674580904e-02) (2, 6.48760435974937155201e-02) (3, 4.77243078744750554021e-02) (4, 7.03712193167548849537e-02) (5, 1.04230380077012523543e+03) (6, -1.23001512212915622979e-01) (7, -1.49770818956174056602e+00) (8, -1.37675782484534461186e-02) (9, -2.38542753089601289318e+00) (10, -2.58884227850530758419e-02) (11, 3.49761256859907054828e-01) (12, -9.56171057809957619655e-02) (13, 2.65096800313600389210e-01) (14, 3.28352684066900157855e-01) (15, -1.10877922497411601777e-01) (16, 5.61384129243523988251e-01) (17, 2.50382176975063208690e-01) (18, -5.59203509191661674027e-02) (19, 6.04278416203569257448e-01) (20, 7.32386964978567200912e-02) (21, 2.04578142563006604160e-01) 
