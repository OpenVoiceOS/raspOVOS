FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=15 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (15, 6, 5.00000000000000000000e-01) (15, 6, 5.00000000000000000000e-01) (15, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.20539719085714480684e+00) (1, 1.40537996982092350251e+00) (2, -4.05916363724259099399e-02) (3, 2.28609274036453982148e+01) (4, -1.19991433553747151031e+00) (5, 2.82545704155038546190e+00) (6, 2.37873620973121191469e+00) (7, 1.38657566458638417117e+01) (8, -7.25919060876963451179e-01) (9, 2.86984504892300362755e+00) (10, 1.29651142444417266653e+00) (11, -1.32072738803990996637e+00) (12, 1.40103779063280331485e+01) (13, -1.07066982701118496024e+00) (14, -5.76832986066396213864e-01) (0, -2.25565097996984391671e+01) (1, 3.89951109026127706869e+00) (2, 7.98160620549206001328e-01) (3, 4.40674946895203767383e+00) (4, -1.82509456153495142061e+00) (5, 1.12302224378703252583e+00) (6, -2.84872688326402168091e-01) (7, 4.43429948988185440584e+00) (8, 6.38339410213574431907e-01) (9, 1.10494036268113271504e+00) (10, 3.68625154377455954702e+00) (11, -3.35914223693471181154e-01) (12, 4.43282504233346497102e+00) (13, -3.16101077608073621761e+00) (14, -2.64176601115176878931e-01) (0, -5.15164271216413638399e+00) (1, 1.33780545388216465241e+00) (2, -7.39358492918995718890e-02) (3, 2.27477229616807719026e+01) (4, -1.29987569086603138580e+00) (5, 2.81479084252189348803e+00) (6, 2.68193378312085695114e+00) (7, 1.39391059280093418948e+01) (8, 3.38224158325847179540e-02) (9, 2.90213249384235139061e+00) (10, 1.33465383734509734914e+00) (11, -1.18314244099029175494e+00) (12, 1.39516625777538525455e+01) (13, -1.05683478429611277249e+00) (14, -5.84117172846848942136e-01) (15, 9.87528077614081012392e-01) (16, 1.59578386743065987474e+01) (17, 9.92236650836241351747e-01) (18, -6.24876312392589339240e-01) 
