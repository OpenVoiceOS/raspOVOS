FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.66386178907941190142e-01) (1, 2.25699868777656798002e-02) (2, -5.97224879285430387821e-02) (3, -1.14695850035285724228e-02) (4, -4.95584729334449247684e-02) (5, 6.60329599139756640014e-01) (6, -6.00068629102046280366e-03) (7, -2.86592817039355608755e-02) (8, 3.84927292392926112541e-01) (9, -1.30389467335660147462e-01) (10, 1.50899605818194282314e-01) (0, -8.21108622665360776693e-01) (1, -2.20951774270248743615e-02) (2, -5.72099798471641871056e-02) (3, -1.43380206182689734495e-01) (4, -1.00211847677917534338e-01) (5, 3.15482452602309504375e-01) (6, -8.89377808249325191303e-02) (7, 5.56298910818941116929e-01) (8, 1.09397916975490839576e-03) (9, 6.54566997741177439979e-03) (10, 2.48240481629804048136e-02) (0, 9.03700174799658834113e-02) (1, -7.40902498475570214787e-02) (2, -1.38370543829459610730e-02) (3, -6.80656954757231646980e-02) (4, 1.80681422241669199791e-02) (5, -5.46802193767906563604e-02) (6, 4.71162256867928341819e-01) (7, -7.40460045724196458394e+00) (8, 6.78507945220393482444e-01) (9, -4.57336201939963285512e-02) (10, 3.43554857258432821965e-01) (0, -6.12667550937277649581e-01) (1, -2.71244289205860092395e-02) (2, -1.69232809715111121029e-01) (3, -3.63598185046423542921e-03) (4, 1.07996223642040350271e-02) (5, 4.00447964443388315647e-01) (6, -1.95368296471867014530e-01) (7, 5.07607692519673969755e-01) (8, 1.65707493223298946150e-01) (9, -3.27788706001971297255e-02) (10, 3.45327972711145791962e-02) (0, -2.61138792362998417573e-01) (1, 4.04568751790490635512e-02) (2, -1.03029449518683813292e-01) (3, 2.68501927592721609250e-02) (4, 9.96275148176823006907e-03) (5, 6.43958111416821132877e-01) (6, 1.52760274854142147349e-01) (7, -2.71678542823657401262e-02) (8, 2.50290150832474855846e-01) (9, -7.64249642262051082087e-02) (10, 1.77825621117415066941e-02) (0, 5.07286702930206434559e+00) (1, 2.01768730562680798091e-01) (2, 1.81680519860738437510e-01) (3, 2.13752132636064129390e-01) (4, 2.56130111199372789699e-01) (5, 1.31986051450259452622e+02) (6, 4.16075346903793719111e-01) (7, -6.18447442544706760259e+00) (8, 1.07389605406044204017e+01) (9, -4.55630220066781735366e-03) (10, -8.66779667588822522184e-01) (0, 8.69968661409531024420e-02) (1, -1.00264159934944835362e-01) (2, -2.91545744709441230491e-02) (3, -3.72834410242506852917e-02) (4, -3.71888037495085588269e-02) (5, 2.26628647060900201282e+00) (6, 7.51706918557051018137e-02) (7, -8.57574961463722651445e+00) (8, 1.72599617526395143763e+00) (9, 6.49188301733264888638e-02) (10, 1.55652072536042185291e-02) (0, 1.41089451850281794032e-01) (1, -5.99640368081810481993e-02) (2, -1.31788140402007369900e-01) (3, 2.92622388742682926210e-02) (4, -6.32790236331703809247e-02) (5, 1.75225520598726780541e+00) (6, 3.90646672378155701555e-02) (7, -9.32923195833466856186e+00) (8, 3.08011813168100845317e+00) (9, -5.91778494033298567423e-03) (10, 3.90516360453305871880e-02) (0, -2.62970782098363331514e-01) (1, 2.50299712159600881711e-02) (2, -7.54892612359556458568e-02) (3, -5.07857734820875497683e-02) (4, -1.29901329538854911488e-02) (5, 6.66669678149289568481e-01) (6, -5.13309194347869035546e-03) (7, -6.38569844521124918568e-02) (8, 3.54176151124835025819e-01) (9, 2.41871844163348732881e-02) (10, 2.05084614239546825232e-02) (0, -2.81904824432221161601e+01) (1, -2.58312846732227374691e-01) (2, -1.95588547003357066112e-01) (3, -3.15984099370328952450e-01) (4, -2.09916490328399790721e-01) (5, 5.03613555835616732992e+00) (6, -2.33939719220546460399e-01) (7, 1.59746570444769558916e-01) (8, 2.14371356581960964549e-01) (9, 2.12320716529330988376e-01) (10, -1.35115093660764906680e-01) (11, 3.93417401427139712400e-01) (12, 4.53920910984109027275e-01) (13, -2.50143142574419455326e-02) (14, 4.45102165029758023440e-01) (15, 3.62052796127790010861e-01) (16, 8.14028850087406552483e-01) (17, -2.74016689602881335919e-01) (18, -4.97935305010239304124e-01) (19, 4.68213068304351887061e-01) (20, 7.32969372605369873241e-01) (21, 3.26007140095248337808e-01) 
