FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=9 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 9.10817115875161453786e-01) (1, 1.04867789906794794952e-01) (2, 1.25424537820155390655e-01) (3, 4.97426279013702885012e-02) (4, -2.90180779869010363470e-02) (5, 1.39549033840110392468e+00) (6, 6.71674223039007034508e-01) (7, -9.64908334387922295150e+00) (8, -2.68644727641990099798e-02) (0, -1.09883552823121580233e-01) (1, 5.15968338607497134518e-01) (2, 4.62670211701340594601e-01) (3, 5.56716782062000925535e-01) (4, 6.47405755727238352293e-01) (5, 4.24522347600068183926e-01) (6, 4.25503341907055854243e-01) (7, -3.59966259609092631422e+00) (8, 8.69816105189002541920e-01) (0, -1.02981214882574681480e-01) (1, 2.76117158651912664169e-01) (2, 3.61371172190273148850e-01) (3, 3.22689009428584916428e-01) (4, 2.64628296316230748886e-01) (5, 1.25781574376456706243e-01) (6, -1.64828983115663657566e-02) (7, 1.14230826935343685591e+01) (8, 2.76321393301799833608e-01) (0, -4.28580999708724486030e-02) (1, 4.14445392107606724874e-01) (2, 5.70440999483705191153e-01) (3, 4.26332610910773956991e-01) (4, 5.05952685391545919913e-01) (5, 9.52892176682883784711e-02) (6, 6.01259425263177882393e-01) (7, -3.77952187667716899000e+00) (8, 8.18032988459994836461e-01) (0, -1.76256877841196674117e-01) (1, 3.03185885370338303879e-01) (2, 3.60532713652217728928e-01) (3, 3.05659850657546861008e-01) (4, 3.73471362114513261155e-01) (5, 7.24267689191572527907e-03) (6, -4.94951875681783060035e-03) (7, 1.14099828219585361921e+01) (8, 2.25139510681942017767e-01) (0, -2.14324751408539432296e-01) (1, 3.79242626548374039963e-01) (2, 2.11434809089267650162e-01) (3, 2.94808452547157262558e-01) (4, 3.52071491599643571213e-01) (5, 9.03477753480601136760e-02) (6, -4.50457644628992209812e-02) (7, 1.14266093513541164839e+01) (8, 1.97521318545654328558e-01) (0, -1.32271035049016494334e-01) (1, 4.16674441909433201925e-01) (2, 4.97768699442983242331e-01) (3, 5.18866635179162649649e-01) (4, 5.53985378956437735098e-01) (5, 9.36013929421836421430e-02) (6, 7.00216792504560481269e-01) (7, -3.79349231879342951856e+00) (8, 6.51137489452260131984e-01) (0, -1.30714817378575820506e-01) (1, 4.71783955602593341183e-01) (2, 5.25132079629845538449e-01) (3, 4.62485690622277179074e-01) (4, 5.71337153224892313474e-01) (5, 4.84309293181504341153e-01) (6, 5.06908378058465003413e-01) (7, -3.75762350644101061903e+00) (8, 8.46004010505878367709e-01) (0, 3.23289539806529524490e-01) (1, -1.18902876742200749871e-01) (2, -2.15069330997781638093e-01) (3, -2.33362588800506531861e-01) (4, -2.57417551942186295655e-01) (5, 6.71293468132858284214e-01) (6, 5.81171198298991775744e-01) (7, 4.63071248063527463046e+00) (8, -3.27054393332182835596e-01) (0, -1.74141174588258285105e-01) (1, 4.88845233826584624204e-01) (2, 6.03463125436253244871e-01) (3, 4.64628611712403216671e-01) (4, 5.36743243246025958371e-01) (5, 4.70506898910607429531e-01) (6, 4.40665295773537690760e-01) (7, -3.78463263969053187452e+00) (8, 8.07093651361667552280e-01) (9, -3.23562993733481740755e-01) (10, -2.41789695661246223368e-01) (11, 4.89637360793987785890e-01) (12, -2.59245511930167149561e-01) (13, 5.18006996137539310610e-01) (14, 5.06550297958294315492e-01) (15, -1.88984458189188936350e-01) (16, -1.84720818739115694163e-01) (17, 6.64079419776735013770e-01) (18, -1.98354367952525117991e-01) (19, 5.10952175480762882387e-01) 
