FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.09523949201186465707e+00) (1, 3.29233141552642480376e-02) (2, 1.57912637473268863131e-01) (3, 1.42254013003034945894e-01) (4, 1.42906221927328463961e-01) (5, -2.50313033502755355464e-01) (6, 2.29128890942606799896e+00) (7, -3.52918215345759511692e-01) (8, -4.05597653797201029846e-01) (9, 3.15290663279110705552e-01) (10, 1.20267175729681249918e-01) (0, 3.44730291824271339873e-01) (1, 1.60779548546068962178e-01) (2, 2.34473833551400001607e-01) (3, 2.74957695713036576191e-01) (4, 2.16492095699303443990e-01) (5, 3.58102226098786946551e+00) (6, -2.58270642558926954990e+00) (7, 1.20284221255201120848e-01) (8, 1.35644159393851798656e-01) (9, 1.70098508571666529043e+00) (10, 1.89395270396876508334e-01) (0, 6.56866700022670779724e-02) (1, -2.09193252768830439958e-01) (2, -2.05207445469216487322e-01) (3, -2.68666692819909236345e-01) (4, -3.83623585846738057725e-01) (5, 8.35507750911945179517e-01) (6, 5.29194838321303784312e+00) (7, -2.00014973837937981571e-01) (8, -5.97909491212699428786e-01) (9, 4.87067106538539240645e-01) (10, -3.58061128804890604904e-01) (0, 1.90850837203450862667e-01) (1, 5.68136796833104362747e-01) (2, 6.86487429321831710283e-01) (3, 5.77115208388394584915e-01) (4, 7.23377846659249312822e-01) (5, 1.65836394291210709184e+00) (6, 2.44940678556029878266e+00) (7, 1.50872301291027044456e+00) (8, 4.08918116985588575574e+00) (9, -6.29140884145882139933e+00) (10, 1.12020859004472450593e+00) (0, 1.56714741978391192134e-01) (1, 3.07132629630431308598e-01) (2, 2.50738462385996785375e-01) (3, 3.26607732055052890630e-01) (4, 2.08161065307721104833e-01) (5, -5.39161207511233281364e-02) (6, -5.19170916296112672228e+00) (7, 2.73778908641885099229e+00) (8, 1.18350929577240489010e+01) (9, 1.96818381091696430474e+01) (10, 2.66815875126974411113e-01) (0, 9.82341555188452564273e-01) (1, 8.32848949553780387722e-02) (2, 1.64050097596865201988e-01) (3, 2.26362499428015284675e-01) (4, 2.36159670186262660163e-01) (5, -3.61332405996379446567e-01) (6, 9.62312224638275259814e-01) (7, 1.64228739074069651771e-01) (8, -1.28277212205654550514e-01) (9, 3.82694724964119992183e-01) (10, 1.16356587839510056126e-01) (0, 2.09679738933393577938e+00) (1, 3.34225172751035359830e-01) (2, 2.07288401775445496611e-01) (3, 1.74837397985543735457e-01) (4, 1.78620542042340763045e-01) (5, -1.38034815761163121017e+00) (6, -5.25431695796120301623e+00) (7, 2.83592881204436597642e+00) (8, 5.43466357915510656085e+01) (9, 1.97206223225337673455e+01) (10, 2.07593328700810431098e-01) (0, 9.93798437369822762477e-01) (1, 1.21803809763930875887e-01) (2, 1.83140308280216150549e-02) (3, 1.89904537858508637438e-01) (4, 9.37173933406101056232e-02) (5, -1.20968770628073901685e-01) (6, 5.81675031515930029968e-01) (7, 4.42119254039732512140e-01) (8, 1.29470722833531625556e-02) (9, 4.54523188547762524436e-01) (10, 2.78131290566195205072e-02) (0, 1.71519625884328541510e+00) (1, -1.27844986564610602064e-01) (2, -1.66750562436078164641e-01) (3, -1.47778485960457894866e-01) (4, -3.06819277472239748672e-02) (5, -1.49723256005205107577e+00) (6, -3.98135529156936351924e+00) (7, -1.79658895163487902780e-01) (8, -5.77526537304074660817e-01) (9, -8.28848310031056523250e-01) (10, -1.23172948894061898284e-01) (0, -3.38251953729722565356e+00) (1, -3.33060263273342593848e-01) (2, -4.30994151708706196846e-01) (3, -3.36367188570125985603e-01) (4, -4.23754474696739491524e-01) (5, 9.15966255203232604387e-01) (6, 4.84604465375816051420e+00) (7, -2.50375001183807233129e-01) (8, -4.21348898758349743598e-01) (9, 1.21944060015991073698e+00) (10, -3.05476579808891202106e-01) (11, 1.81113174801553533655e-01) (12, 2.77715797325322366618e-01) (13, 6.52891422204079474589e-01) (14, -2.15132112848880230338e-01) (15, 3.94145475199323080950e-01) (16, 1.48296609829868142327e-01) (17, 3.54805321862798062060e-01) (18, 2.04535626183952157220e-01) (19, 1.20717261248341201085e-01) (20, 7.79861993653765295242e-01) (21, 2.05164056163821717504e-01) 
