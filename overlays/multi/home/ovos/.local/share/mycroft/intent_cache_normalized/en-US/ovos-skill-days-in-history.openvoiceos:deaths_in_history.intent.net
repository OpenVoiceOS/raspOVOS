FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -3.55338396288780666854e-01) (1, -5.32031974143004071642e-01) (2, -6.54311029559587642446e-01) (3, -6.17391551232552071582e-01) (4, -4.78819182461713621635e-01) (5, -5.35668977110613009351e-01) (6, 6.70224586903455360698e+02) (7, -5.70013591820879406136e-01) (8, 3.18465096530997582303e-01) (9, -1.02591762973722677721e+00) (10, -1.53059959588868026159e-02) (0, 6.34266543707689045561e+00) (1, 5.68307615585567749150e-01) (2, 6.77377685673476781503e-01) (3, 5.09692646332027821110e-01) (4, 6.68158814080001439706e-01) (5, 3.14349397398237861978e-01) (6, 3.06140216714237611839e+00) (7, -8.51747350654597779052e-02) (8, 6.75754761257241742811e+00) (9, -5.34808077179173668725e+00) (10, 6.11717043795427573016e-01) (0, 1.45256142791180975493e+00) (1, 4.30320069983088138166e-01) (2, 4.41287667348467471662e-01) (3, 3.54444322034203285199e-01) (4, 4.62292239859186770978e-01) (5, 6.88123986709395007999e-01) (6, -5.65771649485735217411e+00) (7, 1.50000000000000000000e+03) (8, 3.12906718110522730214e+00) (9, 1.18415958368568823289e+01) (10, 6.96577272931005186951e-02) (0, 2.48185390594820198817e-01) (1, 1.03787335448530290694e-01) (2, 6.86669190159111958449e-02) (3, 4.89740770986348897265e-02) (4, 1.60873751037385936957e-01) (5, -1.25661086392498289044e-01) (6, 1.22796070680663760299e+00) (7, -1.39346843418524518610e-01) (8, -2.62305278911547368192e-01) (9, -2.12070608457759579579e+00) (10, 6.52878546224041483859e-02) (0, 6.36770057773754644126e-02) (1, 8.69789929306347825433e-02) (2, -6.58662755133314261474e-03) (3, -2.59967609131495532016e-02) (4, 1.15213191620095536938e-01) (5, -1.68212503247069738999e-01) (6, 6.49067264357399949404e+02) (7, -1.12662454804540040909e-01) (8, -6.02090543715125492419e-01) (9, -8.58117220991297391386e-02) (10, -9.71591485913692598508e-02) (0, 1.54041047712821232984e+00) (1, 8.50591301267747179615e-01) (2, 8.22784751003389103197e-01) (3, 8.41337873881463749193e-01) (4, 7.47544995790128452562e-01) (5, 2.19933942405463850323e-01) (6, 2.74106102216956104556e+00) (7, -1.79488420845779289570e-01) (8, 6.98281136346135511417e+00) (9, -5.38509233856683966479e+00) (10, 6.64938951239581910180e-01) (0, 1.62573457827824818800e+00) (1, 8.07546130662564975999e-01) (2, 7.87434651797418339036e-01) (3, 8.36208096986417515062e-01) (4, 8.84878232424859523242e-01) (5, 3.02135915009261679653e-01) (6, 2.82674491419030582406e+00) (7, -1.17085077227863237903e-01) (8, 6.91174440276894941348e+00) (9, -5.25284418825137677800e+00) (10, 6.08336018726821636804e-01) (0, 7.61155061070189020711e+01) (1, 6.75622253208880141351e-01) (2, 7.06158733456854537103e-01) (3, 7.23202249496702642695e-01) (4, 7.92673161714796736810e-01) (5, -1.15561029478469512455e+00) (6, 1.64882653268775292155e+00) (7, -2.55865080436957903487e+00) (8, 7.12720434353931331373e+00) (9, -5.79918755400064167560e+00) (10, 1.88878881756530825342e+00) (0, 1.07273763987929493524e+00) (1, 5.03190077328016016622e-01) (2, 5.66997505211164209982e-01) (3, 5.36437995218564722677e-01) (4, 3.90200450503161822002e-01) (5, 1.18700287785913638494e+00) (6, -6.02283756452650109736e+00) (7, 1.50000000000000000000e+03) (8, 3.43763636523447679849e+00) (9, 1.16559040786823686631e+01) (10, 1.98390590193101123972e-01) (0, -1.90158266002006776096e-01) (1, -7.20360480978561867893e-02) (2, -5.36307476306694362600e-03) (3, 1.15754476950095802623e-02) (4, -6.82492337966698111446e-03) (5, 1.77481077639502382581e-01) (6, 9.53475914427795001593e-01) (7, 2.79896713735935040446e-02) (8, -2.94927559517063397898e-01) (9, -4.68781588781315497472e-02) (10, -9.91341910414616778180e-02) (11, 5.14856043338700497536e-01) (12, -1.56984754313134106107e-01) (13, 5.13215382948320697487e-01) (14, -4.31496931938816719043e-01) (15, 1.07053997105417941782e+00) (16, -1.18776597489022361920e-01) (17, -1.32744752277039440580e-01) (18, -1.67222485783432850415e-01) (19, 5.17849636629026943702e-01) (20, 6.89641975080176106161e-01) (21, 2.58806639968414742281e-01) 
