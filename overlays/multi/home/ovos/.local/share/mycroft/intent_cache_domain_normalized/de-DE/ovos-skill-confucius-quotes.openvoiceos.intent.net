FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=10 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (10, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.89560905633945564475e-01) (1, 4.03609821675797231233e-01) (2, 4.22798285363693959749e-01) (3, 4.16838983176728017366e-01) (4, 4.03560036896248586213e-01) (5, -5.04043639107319835224e-02) (6, -8.83987096901675123428e-02) (7, 1.85090814942600694337e+01) (8, -1.77352376079156048849e-01) (9, -7.95794517202946388723e-02) (0, -3.81304146353167339178e-01) (1, -1.44644352854709667966e-01) (2, -1.19956913651447366620e-01) (3, -5.99500865354347795000e-02) (4, -4.01382774724770227670e-02) (5, -3.86878938557434493006e-02) (6, -6.13144649374724667878e-02) (7, 6.78863020349536938625e+00) (8, -5.64321791421088791996e-02) (9, -2.32395342381454927949e-01) (0, 1.63995747475169817431e-01) (1, 3.04625442354460485017e-01) (2, 3.73856539009590871370e-01) (3, 4.01175135849495656526e-01) (4, 4.35866469620247609651e-01) (5, 8.02621081070330755214e-02) (6, -1.48881695961548876639e-01) (7, 1.86665691735329595247e+01) (8, -1.68898194825245984152e-01) (9, -1.98736045306517672415e-01) (0, 5.51380085783212514627e-01) (1, 3.84923313565499969346e-01) (2, 3.41134355224616714342e-01) (3, 4.91370611645228938524e-01) (4, 3.57265920199401565416e-01) (5, -5.21369710161873722853e-01) (6, -5.18447483732660599487e-02) (7, -9.60051282312244014250e-01) (8, -2.82474051600474340695e-01) (9, 9.06998317052992442111e-02) (0, -5.09307885173720054084e-01) (1, -8.36919278516579240312e-02) (2, -6.32173151387978166094e-02) (3, -1.12855988801937173749e-01) (4, -1.20138350190143655682e-01) (5, 1.86359866378974781131e-02) (6, -2.65281081809807563199e-02) (7, 7.51954130026442513213e+00) (8, -1.92377428490987739673e-01) (9, -8.86500466780438534098e-02) (0, 9.50865627096643351024e-01) (1, 7.62584487520215348333e-01) (2, 7.68275635860917405218e-01) (3, 7.46298472009656266302e-01) (4, 9.19263253532407009772e-01) (5, 4.08867757070260517960e-01) (6, 3.53295515242851410065e-01) (7, -7.46506083520991303715e+00) (8, 3.75559712711609039459e-01) (9, 4.89181822117443376730e-01) (0, 9.95563872920026682323e-01) (1, 3.48814395470284865919e-01) (2, 4.74502151114606307569e-01) (3, 4.81112723452710555616e-01) (4, 5.04423563582562795027e-01) (5, -1.62089878965655825871e-01) (6, 3.29420327854984207328e-01) (7, -3.91899561103916571625e+00) (8, 2.89285915889951172897e-01) (9, 4.79849862095708346033e-01) (0, 1.85164686189804306160e-01) (1, 6.27389938401316099581e-02) (2, -2.84017718566323719431e-02) (3, -5.85489778948689934546e-02) (4, -2.66133344901467762400e-02) (5, 7.93709857526696760388e-01) (6, 8.51027757691477093616e-02) (7, -1.86275158265712228456e+01) (8, 1.65811959560985350270e-01) (9, 1.26894930081281498424e-01) (0, 5.11386590298291032752e-01) (1, 1.74205235879334396909e-01) (2, 1.19245774786385497213e-01) (3, 1.43117170523318293363e-01) (4, 1.87070078890236801694e-01) (5, -1.70234859263537208207e-01) (6, 1.73392607054107750209e-01) (7, -2.04165055303668951936e+00) (8, 1.31984532447782798581e-01) (9, 2.31610918363207685111e-01) (0, 1.13362077666857952174e+00) (1, 2.44610629803021423223e-01) (2, 3.23965944176991482717e-01) (3, 3.22933666235287686330e-01) (4, 3.78186464136918087942e-01) (5, -5.58588599629960880222e-01) (6, 2.91407041965242419312e-01) (7, -4.03512325104809210785e+00) (8, 4.55749340581285689034e-02) (9, 5.38351342334512095178e-01) (10, 4.01758799432297475374e-01) (11, 4.55032001929197627010e-01) (12, 3.54486847399254567659e-01) (13, -2.37100303783226752652e-02) (14, 4.67415999369535761776e-01) (15, -6.91935450448799699297e-02) (16, -7.78135091676521867265e-02) (17, -4.39314430120762988707e-01) (18, -2.78983444585610175503e-02) (19, -7.93799639358330338945e-02) (20, 2.92708953945418126619e-01) 
