FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.83945769364836786419e-01) (1, -2.21148445736140897289e-01) (2, -2.44911908637256342525e-01) (3, -3.07362131308288322185e-01) (4, -2.50014729331702878490e-01) (5, 5.17350677928019586638e-01) (6, 5.86627692730411487432e-01) (7, 3.66910078743637790843e-01) (8, 2.86895429429997594895e+00) (9, 4.69932477555881966680e-01) (10, -4.90314766522926322873e-03) (0, 2.01731387815404700214e-01) (1, -1.09432064888747612730e-01) (2, -8.36841249444803397806e-02) (3, -1.98399172125132916555e-01) (4, -9.95904588678201835306e-02) (5, -1.18649247726174619788e-01) (6, -4.71722325254709640152e-01) (7, -3.51050008737766802702e-02) (8, 4.97614177860808037934e+00) (9, 6.49859213201861712905e-02) (10, -1.40012088855477584426e-01) (0, 2.00442403848174161007e-01) (1, 4.09474372813438403096e-02) (2, -7.98489227941299589686e-02) (3, 2.17295438000892696118e-02) (4, 5.71784376594773342123e-03) (5, 1.44421881706451205041e-01) (6, 4.26334893108379464621e-01) (7, 2.35850214907859859204e-02) (8, 8.30119211054985228770e+00) (9, 2.06964019664560239242e-01) (10, -2.94267982294822635914e-02) (0, -2.59590190637380535676e-01) (1, 2.79325217301848505169e-01) (2, 1.28364142323735302975e-01) (3, 2.06991516346934384396e-01) (4, 2.30387509162428921750e-01) (5, 2.04700653890228373433e-01) (6, 2.66646050671246026553e-01) (7, 2.47298656329379917818e-01) (8, 2.01588779343365906982e+01) (9, 1.25537344347114859788e-01) (10, 1.56189618149852854634e-01) (0, -3.38858430075914263124e-01) (1, 2.18610152895453518918e-01) (2, 2.67143816049101923138e-01) (3, 1.99450046117308682492e-01) (4, 1.52038879985812253048e-01) (5, 2.45369737485504307850e-01) (6, 2.41294526318218682803e-01) (7, 1.14182683005796273279e-01) (8, 2.01135901315211533813e+01) (9, 7.30652595864489384780e-02) (10, 1.24301092216825573300e-01) (0, 1.48984016756954523730e-01) (1, 6.64740221558497323961e-01) (2, 6.19037045149491205187e-01) (3, 7.88872232644245996447e-01) (4, 7.99345156161473169298e-01) (5, 6.20792926141900847092e-01) (6, 1.69408539850831402207e-01) (7, 5.56397714504880736008e-01) (8, -9.03522404308032811571e+00) (9, 2.85498606188996084665e-01) (10, 8.12786538913953582330e-01) (0, -1.99115474510461631930e-01) (1, 2.43428886229994839718e-01) (2, 1.44256737376454419186e-01) (3, 1.63969837422373837521e-01) (4, 2.29296297128203457882e-01) (5, 1.21057347843504026219e-01) (6, 2.52172031263020013370e-01) (7, 1.35774759873615091799e-01) (8, 2.01869063852428674011e+01) (9, 3.10391756927107187458e-01) (10, 8.38868594561577862745e-02) (0, -2.48823118198186810091e-01) (1, 1.83403961653712338498e-01) (2, 1.57649323637488431027e-01) (3, 1.54931277329924649289e-01) (4, 1.43919844473603314450e-01) (5, 2.01333960035896403218e-01) (6, 1.97845675805714105167e-01) (7, 2.12670316437339884663e-01) (8, 2.02352944715022324829e+01) (9, 1.65460731124017990101e-01) (10, 2.64542912582016065404e-01) (0, 2.02279800455513275592e-01) (1, 6.27895129828141107531e-01) (2, 7.85611664859936609240e-01) (3, 7.72300606577084436388e-01) (4, 7.72503754107640161486e-01) (5, 5.07083690064378256324e-01) (6, -8.28416743873384153751e-02) (7, 6.32685095855874846116e-01) (8, -9.13800609136533559251e+00) (9, 2.69979166014893301462e-01) (10, 8.53177589193094054743e-01) (0, -2.86004920292169451113e-01) (1, 1.84887759680750912716e-01) (2, 1.83556571896079129269e-01) (3, 1.93437084848883694699e-01) (4, 1.56281166071894711544e-01) (5, 2.48833288887596287831e-01) (6, 1.09667734066154945460e-01) (7, 1.33269370982503992940e-01) (8, 2.02423795564173936157e+01) (9, 1.28834002743835746019e-01) (10, 2.31431206205940348530e-01) (11, 6.61442117749140634508e-01) (12, 3.96601680423902591688e-01) (13, 3.19378888074863254154e-01) (14, 2.56874174172881220013e-01) (15, 2.03483231658938473752e-01) (16, -2.05192540574342552340e-01) (17, 2.25956395561698025753e-01) (18, 2.42506131823065823605e-01) (19, -2.18926046777040334002e-01) (20, 2.01208047862055844357e-01) (21, 2.57533639962676141888e-01) 
