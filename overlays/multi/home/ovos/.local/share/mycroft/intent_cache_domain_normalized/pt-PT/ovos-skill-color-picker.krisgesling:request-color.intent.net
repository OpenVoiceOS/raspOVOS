FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.29272879585248934298e-01) (1, -4.50576884216763506874e-01) (2, -4.63390899366833697304e-01) (3, -4.66703956491448412880e-01) (4, -4.11191826230980828072e-01) (5, 5.15961523753987516550e-01) (6, 2.56381105258618180454e-01) (7, 5.12798915768489571576e-01) (8, -3.26268715302347078744e-01) (9, 6.76865237998565660860e+00) (10, -6.65377869456356130229e-01) (0, 7.82376282236432463169e-02) (1, -7.50145781155489022041e-02) (2, -5.80826307948492104316e-02) (3, -3.67655474062822187897e-02) (4, -5.83741869147680336738e-02) (5, -2.45723810658245667149e-01) (6, 4.68286932718412984844e-01) (7, 2.32683258007630677788e-01) (8, -6.46492523547682051976e-02) (9, -1.05658425658390187074e+00) (10, 2.00557191477188007100e-01) (0, 8.81428099207627324674e-01) (1, 4.87777615365459105146e-01) (2, 4.88219926533176085126e-01) (3, 4.75794950899554869306e-01) (4, 4.56729481396152270367e-01) (5, 7.64184840688509892104e-01) (6, 1.00067021819268719973e+00) (7, 1.25866651668430007405e+00) (8, 4.84553008883146452135e-01) (9, -4.30212959426001706476e+00) (10, 2.31662542504617541272e-01) (0, 6.30286786332155068990e-01) (1, 3.16594179870699099499e-01) (2, 2.00145978631590198971e-01) (3, 1.98017928900335621334e-01) (4, 2.99393188359354134320e-01) (5, 4.82414999806509259273e-01) (6, 1.15081107425374917774e+00) (7, 3.06082666812624082375e-01) (8, 2.05576279537727474311e-01) (9, -2.51242303995470805944e+00) (10, 2.55523770859215959561e-01) (0, -7.41855304042103425033e-01) (1, 7.55751296382598192203e-01) (2, 7.77397401135615617740e-01) (3, 8.08964348834208757388e-01) (4, 8.56410704057864458072e-01) (5, 8.19544440073732727292e-01) (6, 2.93476641050986919446e-01) (7, 2.03104227499408512836e-01) (8, 7.10980592665414246056e-01) (9, 3.23451195771004984536e+00) (10, -7.73042790092085541076e-02) (0, 3.84574947918340792619e-01) (1, -2.79047875334430262040e-01) (2, -1.39288688679147287797e-01) (3, -1.38088608761239572953e-01) (4, -1.72956312675881906937e-01) (5, 2.78317457619377872646e-01) (6, 2.06457344577297918775e-01) (7, 2.67416235599015728042e-01) (8, 1.55634949323852755931e+00) (9, 7.74426967709371472104e-01) (10, -5.39780773706076688612e-01) (0, -6.55914436379673615463e-01) (1, 7.72715545695475847232e-01) (2, 7.55958340566806108463e-01) (3, 7.90403671067408830631e-01) (4, 7.62607149284533769595e-01) (5, 7.87511175436739319089e-01) (6, 3.58373336366824668531e-01) (7, 2.27029226438445780323e-01) (8, 6.18125712868909271691e-01) (9, 3.29176449144389460244e+00) (10, -2.03536967407179358869e-03) (0, -7.01627458730938569076e-01) (1, 8.18130965532089193815e-01) (2, 8.42418342084909399503e-01) (3, 8.98664760590816458219e-01) (4, 8.25162316859985311979e-01) (5, 8.04471617502932168087e-01) (6, 3.81714761130027335767e-01) (7, 2.18299679176254063373e-01) (8, 5.27530921992997670067e-01) (9, 3.17564094552900622048e+00) (10, 2.96630062662510823879e-02) (0, 1.95749590849623039679e-01) (1, -1.13468662578903034532e-01) (2, -2.36734307597501435205e-02) (3, 3.85627814342157718142e-02) (4, -2.91877169452438908065e-04) (5, -2.65078535312051666306e-01) (6, 3.23749412652597878814e-01) (7, -8.22482157790754009330e-02) (8, 3.32434658483842654375e-02) (9, -1.00945462181613088148e+00) (10, 2.69167982924382098719e-01) (0, 9.29841311001038284267e-01) (1, 1.98976126945298065207e-01) (2, 2.22454396522324432395e-01) (3, 2.29074349141400207541e-01) (4, 2.70904188490193154060e-01) (5, 3.48009374157690620777e-01) (6, 1.52712377687277944815e+00) (7, 3.30526468856959632081e-01) (8, 3.82682985634130112818e-01) (9, -2.16215805876222644244e+00) (10, 1.86408172639168867590e-01) (11, 1.18793261452874210349e+00) (12, -4.71043048838676148282e-01) (13, 5.18964604651821925785e-01) (14, 5.17380508872808064780e-01) (15, -2.33492709282102750556e-01) (16, 5.62083715543304163198e-01) (17, -1.85459859255018455482e-01) (18, -1.88609234574499351478e-01) (19, -2.06145611821463264901e-01) (20, 5.29643166157544698081e-01) (21, 3.43634405954746813272e-01) 
