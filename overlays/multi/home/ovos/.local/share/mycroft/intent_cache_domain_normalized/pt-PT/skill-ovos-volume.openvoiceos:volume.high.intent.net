FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.04810109723275690641e-01) (1, 4.50324342086222337289e-02) (2, -7.64086618588064175883e-02) (3, 3.86551842524912592225e-02) (4, -3.45977469847294979988e-02) (5, 1.24994500214768974877e-01) (6, -1.57305385002850850462e-01) (7, 5.72942132233182771506e-02) (8, -4.47685393165858924736e-02) (9, 2.59789931269345775178e+00) (10, -1.59778286832435945097e-01) (0, 3.56074571259070538076e+00) (1, 5.37299094610860117349e-01) (2, 5.49605486803700693521e-01) (3, 4.84391229116370447549e-01) (4, 5.69884052389313944254e-01) (5, 2.04192189426564973953e-01) (6, -3.93211522834651799885e-02) (7, 6.92143465872125673854e+00) (8, 3.44418496429781451074e-01) (9, -5.36323504514228321938e+00) (10, 5.71501639682380352392e-02) (0, 2.19913867824199300571e+01) (1, 2.02486164657953549018e-01) (2, 2.14273165701511669745e-01) (3, 2.62064374773386232764e-01) (4, 1.84394918172243349463e-01) (5, -5.92395557670512440396e-01) (6, 2.43327264029760209629e-01) (7, -3.42146235980021096168e+00) (8, -2.50540452205754549109e-01) (9, -9.86725561077841151203e-01) (10, 3.97573536049232401091e-01) (0, -2.90856423819217635440e+00) (1, -6.37598485334410630676e-02) (2, 2.38289385454163830935e-02) (3, 7.69280194656358079497e-02) (4, 3.44722712201931541509e-03) (5, 1.75208254318027667473e-01) (6, -1.84688757190465263625e-01) (7, 3.24881235466783657717e-01) (8, 3.00611061573067495090e-01) (9, 7.87035058078842997276e-01) (10, -2.60749992448305080439e-02) (0, 2.19423785545351606174e+01) (1, 6.39517543474594635233e-02) (2, 1.46222786382810304717e-01) (3, 2.12112353519574831084e-01) (4, 2.31045217589513601775e-01) (5, -4.11526231785342033742e-01) (6, 2.96727168174310551585e-01) (7, -3.58914005916344347469e+00) (8, 2.99103214578230414133e-01) (9, -8.64099543239775114145e-01) (10, 1.13453196799285649377e+00) (0, 8.18924339157444003412e-01) (1, 1.72425205699934330772e-01) (2, 8.16080075426238726877e-02) (3, 2.00295190207495060752e-01) (4, 2.08441818944944706749e-01) (5, -1.59757078977529665131e-01) (6, 2.58183584401776744510e-01) (7, -3.78428881866767041764e-01) (8, 1.53669293122847716582e-01) (9, 3.48871297787042566885e-01) (10, 1.89049676793718701706e-01) (0, 8.81647362516701882384e+00) (1, 4.53175211829110025086e-01) (2, 4.45434818548127053894e-01) (3, 3.45384518188401212768e-01) (4, 2.92998912316723814087e-01) (5, 1.13407512056269796363e-01) (6, -1.60235972865139769672e-01) (7, 7.55010029155757855790e+00) (8, 2.22297892510752215234e-01) (9, -5.23200630760919072060e+00) (10, 2.06068393785087122794e-02) (0, -9.68263310923016873311e-01) (1, 2.72463405417831927569e-01) (2, 1.77887940692337487292e-01) (3, 1.56817847775848895342e-01) (4, 1.75592409359844658923e-01) (5, 9.09428192701327686720e-01) (6, 3.08816151606472522051e-01) (7, 2.31557802306039839380e-01) (8, 1.15430125888230228348e+00) (9, -1.06134869373422180416e+01) (10, -2.66456781971059375458e-01) (0, 8.89366515682519143127e+00) (1, 1.06264023198482759902e+00) (2, 1.12588264508840807387e+00) (3, 1.14251119359133013198e+00) (4, 1.11274314387438089113e+00) (5, 2.93086713851805957809e-01) (6, 1.43381875878579062578e-01) (7, 7.51318340975788068192e+00) (8, 2.87724070787291008600e-01) (9, 6.53891432688514395721e+00) (10, 1.36785247643622515090e-01) (0, 4.22787719953721441435e-01) (1, 2.56097593465856657180e-02) (2, 6.40002743344707594741e-04) (3, -8.94327930053659125864e-02) (4, -1.28211593522686517765e-01) (5, 2.82623037172120783467e-01) (6, -2.06083731605798181619e-01) (7, 3.88321206405070062928e-02) (8, -2.69433785228390433597e-02) (9, 2.81801367436263427990e+00) (10, -1.84616466182335292601e-01) (11, 5.27525580498265034279e-01) (12, 4.37145642418961144493e-01) (13, -4.24734167868343226449e-01) (14, 6.08923415723656491494e-01) (15, -3.90956276175052797051e-01) (16, -3.54178128931514762945e-02) (17, 3.50979335088829613731e-01) (18, -1.05849139782803622345e-01) (19, 3.75396013480560675202e-01) (20, 5.30693083080338245949e-01) (21, 1.89795725268261439300e-01) 
