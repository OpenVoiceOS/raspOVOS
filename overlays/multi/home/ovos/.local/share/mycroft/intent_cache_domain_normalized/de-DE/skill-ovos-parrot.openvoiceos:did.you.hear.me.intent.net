FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.05423245020844746911e+00) (1, 4.37785805405924988154e-02) (2, 2.04100058616946411494e-02) (3, 1.71149355286533633169e-01) (4, 1.52739894503528872427e-01) (5, -6.44599608561967490905e-02) (6, -5.21464164774292221471e-02) (7, -3.37038504595514876172e-01) (8, 1.43241117701217918201e-01) (9, -1.36652166398665353420e+00) (10, 2.40497664026362134582e-01) (0, 9.04360792304814808951e-01) (1, -9.26773772801404993604e-02) (2, -1.20583213981533640680e-01) (3, 5.02468346391672093798e-02) (4, -9.09847990836149256300e-02) (5, 1.51935514034092999935e-01) (6, -8.01666393280206501792e-02) (7, -2.06622041730179437091e-02) (8, 7.30215441771151219097e-02) (9, -1.13477056856166358401e+00) (10, 2.72341281584841554686e-01) (0, 1.19800321449300706256e+00) (1, 1.13387022501587747536e-01) (2, 8.45534544054268727420e-02) (3, 1.38756554907917939445e-01) (4, 1.80847313589215241691e-01) (5, 1.19743946911156751156e-01) (6, -3.34973432519067582103e-01) (7, -8.96456305879327675079e-01) (8, 6.52750078924300147776e-02) (9, -6.04237674875103492056e-01) (10, 6.79770290277224853437e-02) (0, 8.99868034220626777930e-01) (1, 3.32515790256893428722e-02) (2, -1.46980182138049808582e-02) (3, 1.19556534210149056907e-01) (4, 7.35599204096233777461e-02) (5, -5.01348383539760744654e-01) (6, 9.69961479150239225255e-02) (7, -1.67122686089019145683e+00) (8, 1.03804214497578456422e-01) (9, -8.18213171299588459817e+00) (10, 6.44438511365081484472e-02) (0, 2.64661903566276013322e+00) (1, 3.24303735640584789301e-01) (2, 3.35395362821161113764e-01) (3, 3.08246721175252758051e-01) (4, 3.82499252107679210688e-01) (5, 6.61932782999833069226e+00) (6, 2.90140188654955555325e-01) (7, -3.43206308792062042556e+00) (8, -4.87505971690775141703e-01) (9, 1.43181879869263948279e+01) (10, -3.61857181263560423456e-01) (0, -4.78241471485408686792e-01) (1, -1.25737654085424532813e-01) (2, -7.75647629454403325644e-02) (3, -2.36038798506525288046e-01) (4, -2.37743685062196979940e-01) (5, -3.33018927678289777816e-01) (6, 2.40787510024349171722e-01) (7, 1.54867670248158906254e+00) (8, 3.14330746233447533289e-01) (9, 9.40052235561832594257e-01) (10, -1.46178138297773263510e-01) (0, 2.57921597993527829118e+00) (1, 3.38051234033643566157e-01) (2, 3.20767652836381755854e-01) (3, 3.32187977281152568843e-01) (4, 3.89722262170850597407e-01) (5, 6.57113176862786918520e+00) (6, 2.10713236948546073224e-01) (7, -3.41633262495227096878e+00) (8, -4.66506331464410950449e-01) (9, 1.43630633367758573371e+01) (10, -3.38159588945502409540e-01) (0, 1.34272211432075252446e+01) (1, 4.26775478098122496018e-01) (2, 2.85932399604050535569e-01) (3, 3.48955437812535185227e-01) (4, 3.89997288856236357102e-01) (5, -3.23724962003766680851e+00) (6, -4.08072235720979004103e-01) (7, -3.39646686209933568179e+00) (8, -2.04703686422188635419e+00) (9, -7.58921299836261997740e-01) (10, 4.86615357945272164475e-01) (0, 1.33347500279164066228e+01) (1, 3.77462558735955633349e-01) (2, 3.84901710142243780322e-01) (3, 4.18575674583066381640e-01) (4, 4.59092334200490392870e-01) (5, -4.20938699258281712190e+00) (6, -1.74958398840014600628e-01) (7, -2.66888698538465840571e+00) (8, -1.98346563464482206207e+00) (9, -1.71631908258765553121e+00) (10, 5.04862470891049630595e-01) (0, -5.25362595073845528582e-01) (1, -7.28998906574533367397e-02) (2, -2.48602253910093246025e-01) (3, -1.96475361999063485863e-01) (4, -1.89105016704587930443e-01) (5, -2.02704719111147041444e-01) (6, 9.67956902091947762878e-02) (7, 1.69955086242839170652e+00) (8, 3.83319601894611672321e-01) (9, 8.96604405778869795185e-01) (10, -2.76466821175790689047e-01) (11, -2.62462865320742845532e-02) (12, -5.11047949472077933653e-03) (13, -6.78847799944919116921e-02) (14, 4.53897763721960584293e-02) (15, 7.10707286096735502134e-01) (16, 6.79354654617654651894e-01) (17, 5.24874086075536872009e-01) (18, -4.24481076489592212830e-01) (19, -3.39780575706604637265e-01) (20, 6.88051300115930408730e-01) (21, 4.36989056169538148122e-01) 
