FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 7.47186842148966579025e-01) (1, 3.44071026639612975728e-01) (2, 4.61516692982586684835e-01) (3, 3.56901056812676253926e-01) (4, 4.74507555113705459249e-01) (5, 3.28564572490877560007e-01) (6, 8.27126022521588288861e-01) (7, -4.26566678967127896338e-02) (8, 7.67384057415258724610e-01) (9, 2.05709317309635597582e-01) (10, 3.27936705791385230135e-01) (0, 8.43408697071578838944e-01) (1, 5.20350500819791506046e-01) (2, 5.60273434845317552799e-01) (3, 5.46859986958612154240e-01) (4, 6.40998795507062624210e-01) (5, -1.65764443295369379161e-01) (6, -2.88901949446729222615e+00) (7, -1.60666348252723217849e-01) (8, -3.02790456783107275740e+00) (9, -2.25974433992805867621e-01) (10, 4.19150067108976731589e-01) (0, -3.46004710321534347806e-01) (1, 5.88013826485595941795e-02) (2, 7.96821700296022721044e-03) (3, 2.03422306652984857811e-02) (4, -3.95333780088803939112e-03) (5, 3.23866191254103874542e-01) (6, 4.22247915204992718685e+00) (7, -2.96376017995632790036e-01) (8, 2.74582295578960744464e+00) (9, 2.53457442340179639473e-01) (10, -1.42315304239810730413e-01) (0, 8.95815239074733593583e-01) (1, 5.53862370429147432560e-01) (2, 5.84334455309022615666e-01) (3, 6.01468793926347444767e-01) (4, 5.24178393063653658146e-01) (5, -3.82813888787144618320e-01) (6, -2.91172013204625645955e+00) (7, -8.22855996231531966467e-02) (8, -2.90126880031159917195e+00) (9, -3.05039011810722682227e-01) (10, 5.15407746273386258018e-01) (0, 7.19401446805807309737e-01) (1, 5.83446375904191683048e-01) (2, 6.27222001431096742863e-01) (3, 6.27541683492768953556e-01) (4, 5.57631846395839403385e-01) (5, -2.56382599192033044933e-01) (6, -2.91411135118535558064e+00) (7, -1.02372424517104612662e-01) (8, -2.95852550637057820637e+00) (9, -2.35540681456032185981e-01) (10, 4.24145651596891770652e-01) (0, 9.06800123637385158126e-01) (1, 2.98032507048519912374e-01) (2, 4.62613329039486709249e-01) (3, 2.98874132083329024923e-01) (4, 4.46565747009190383565e-01) (5, 1.87953251756853484400e-01) (6, 8.31675540749165498333e-01) (7, 6.91895704779973141907e-02) (8, 8.04279041958582241456e-01) (9, 1.48891726119297435060e-01) (10, 3.73329518533463411423e-01) (0, 1.15764261305449389639e+00) (1, 9.35662721779696726632e-02) (2, 2.73794721582209410471e-01) (3, 1.53236113258873679621e-01) (4, 2.45051111796175641322e-01) (5, 6.05077226048936545055e-01) (6, 2.87051056212249078303e-01) (7, 6.72203078325922742664e-01) (8, 4.11517870312991362880e-01) (9, 5.14198995586569673755e-01) (10, 1.51335395642792441828e-01) (0, -1.99778122362989440930e+00) (1, -1.37396875729096368257e-01) (2, -1.79604720284474328462e-01) (3, -2.06996943761837914888e-01) (4, -1.88303258230221703950e-01) (5, 1.12874860563227819021e-01) (6, 1.12664751118683104103e+00) (7, 1.91629082997126887200e-01) (8, 1.00076327464994152550e+00) (9, 2.19900800534436391409e-01) (10, -1.10544516632049893978e-01) (0, 3.39802074902717632376e-01) (1, 4.09845861071500450201e-01) (2, 3.91415702575597435064e-01) (3, 3.71121095889959007330e-01) (4, 3.84711401933583885260e-01) (5, 3.92794440434420799590e-01) (6, 4.10808136549225277889e+00) (7, -2.78903832801140449948e-01) (8, 4.12738756325474209774e+00) (9, -1.52411702125425790877e-01) (10, -9.86103787429524203390e-02) (0, 7.51291136031813411300e-01) (1, 4.58064024036137995566e-01) (2, 4.04031981413571883444e-01) (3, 5.86626958732812342490e-01) (4, 4.79763989036290583456e-01) (5, 3.77630251802629879343e-01) (6, 7.96684440318676911907e-01) (7, 5.64167942915310971497e-02) (8, 7.60760539693367321412e-01) (9, 2.36032852513569313402e-01) (10, 2.64397161252016954514e-01) (11, 2.13867615805538785478e-01) (12, -3.34231565604743319220e-01) (13, 8.25833839862612495430e-01) (14, -3.87932310710486727423e-01) (15, -3.55197868208226519293e-01) (16, 2.08120863781841886064e-01) (17, 6.89774282791297405071e-02) (18, 7.78198073732707107375e-01) (19, 8.27151460140017280587e-01) (20, 1.55265409396561221422e-01) (21, 5.48680721273929972170e-01) 
