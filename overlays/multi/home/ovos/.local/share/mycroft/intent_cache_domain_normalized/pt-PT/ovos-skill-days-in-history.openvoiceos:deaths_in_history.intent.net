FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.07720430124424249030e-02) (1, -9.05790924420194765165e-02) (2, -8.91669764509039064482e-02) (3, -6.65244012465316114646e-02) (4, -3.94524111738043525843e-02) (5, -1.50000000000000000000e+03) (6, 2.76949674886468022450e+00) (7, 9.18105524057646026070e-03) (8, 2.40081422324627841158e-01) (9, -1.07937460426927067658e+00) (10, 6.53996643724848003565e-01) (0, 1.40161174245977293573e-01) (1, -1.70859269003236197149e-01) (2, -1.69451288084351919805e-01) (3, -3.90354461472867952931e-02) (4, -7.14562038743396044441e-04) (5, -5.14856397178811597826e+00) (6, 8.72027693620185928580e+00) (7, 1.02056913042068458930e-01) (8, -9.49024509000838123463e-02) (9, -1.73686666415244617134e+00) (10, -1.08993559829900603375e-03) (0, 2.37211922405859587659e-01) (1, -4.96904762002699784684e-02) (2, -2.75482819887869836983e-02) (3, 1.28959505346543570198e-02) (4, 2.17195735247204370702e-03) (5, -1.96095958794097224853e+00) (6, 1.50000000000000000000e+03) (7, -8.72267853027089701268e-02) (8, -5.98326964768344359696e-03) (9, -4.84758015171963332901e-02) (10, -1.22601279280324063081e-01) (0, 1.95813210903307499677e-01) (1, 1.35075265077836282474e-02) (2, -1.17085393378328683767e-01) (3, 3.28091198114639839845e-02) (4, 4.96001183073590222461e-03) (5, -6.64927853598971552174e-01) (6, 1.50000000000000000000e+03) (7, 3.29362502623596481444e-02) (8, 3.21871532161893030358e-03) (9, -4.06413703369792808640e-02) (10, -6.41965505100679373340e-02) (0, 3.57914173860491235946e-01) (1, 5.78875234637264868276e-01) (2, 5.59128618154530077078e-01) (3, 5.56217735919956712820e-01) (4, 4.65971859458689685596e-01) (5, 1.50000000000000000000e+03) (6, -5.44209147809241677862e+00) (7, 3.13188488281523405021e+02) (8, 1.02869569788089029316e+03) (9, 5.33362109696677411108e+00) (10, 6.65696550559166100314e-01) (0, 1.29145569358970591090e+00) (1, 7.80595106483037426415e-02) (2, 1.40319207072311213924e-01) (3, 1.45537191391044345989e-01) (4, 6.56957862377699070011e-02) (5, -1.50000000000000000000e+03) (6, 1.35511876457856228484e+00) (7, 5.64727948850461070052e-01) (8, -3.81457529828131206018e-02) (9, 2.72778907540669968146e-01) (10, -3.89787169340734873657e-01) (0, 1.27761020934112545788e+00) (1, 5.85428888102121139370e-02) (2, 3.65678632755823337930e-02) (3, 3.61950809259958469766e-02) (4, 1.84980724205548313632e-01) (5, -1.50000000000000000000e+03) (6, 1.27098106421472656358e+00) (7, 3.30694930116544674736e-02) (8, 3.82878003140620648548e-02) (9, 1.07555384070323944989e+00) (10, 4.76373609153448673847e-01) (0, 5.20675200053862252680e-01) (1, -5.42280852798226048472e-02) (2, -7.00764089826349245715e-02) (3, -6.09960287812951335029e-02) (4, 6.31138503547903906288e-02) (5, -2.42287811228359251103e+00) (6, 2.60946285157547475109e-01) (7, -2.77340460321453208259e-01) (8, 4.58842706647811793275e-02) (9, -8.45286501291666464430e-02) (10, -1.13602818069595587480e-01) (0, -7.37516739649657537825e+00) (1, -3.63950914549941162424e-01) (2, -3.53746636617297272043e-01) (3, -3.96150871026629436411e-01) (4, -2.27425089883917991296e-01) (5, -6.40177922807867116894e-01) (6, -8.81051518800412880594e-01) (7, -6.90724006601054707488e-01) (8, -5.44125337387040186599e+00) (9, 5.46900660075653544112e+00) (10, -1.37270890555080549156e+00) (0, 9.70808213936296543478e-02) (1, 8.42195075822569044592e-04) (2, -1.30497883933710451254e-01) (3, 2.58532875002152051636e-02) (4, 3.78234051883942767880e-02) (5, -1.64864531630448674093e-02) (6, 1.50000000000000000000e+03) (7, -7.27213335236409491591e-02) (8, 1.28503736799294776549e-01) (9, 1.33036016680783408805e-01) (10, -1.39013295790935770269e-01) (11, -4.80801450826592313703e-01) (12, 4.07238289957752797044e-01) (13, 4.31530603663747791554e-01) (14, 5.77003297985735352782e-01) (15, 5.88454627527990936642e-01) (16, -2.58742335174218657246e-02) (17, -6.37333650107900495652e-02) (18, -1.25934370184432181672e-01) (19, 7.07238995418925542324e-01) (20, 3.81898818768909964483e-01) (21, 4.43550972447824987110e-01) 
