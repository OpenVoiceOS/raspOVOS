FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.02356608406736313555e+01) (1, 6.39834101883245187636e-01) (2, 5.52724625555348847428e-01) (3, 6.74685310212445932265e-01) (4, 6.10990848032308075943e-01) (5, 8.03649121312305503295e+00) (6, -9.58246341075691399070e-03) (7, -5.02800666923643468920e+00) (8, -4.51108795080798208321e-02) (9, 3.58653159425998624421e-01) (10, -2.60008199478267643112e-01) (0, -1.87380723325115461364e+00) (1, 1.15088723490149660855e-01) (2, 1.22627213487536551595e-01) (3, 6.55787139033385951814e-02) (4, 1.03971373180062456876e-01) (5, 3.32158703863644100274e-01) (6, 1.67001512161912502030e-01) (7, 3.11545060244541716088e+00) (8, 2.87810647589028301141e-01) (9, 4.48565298725457084483e-01) (10, 4.25298884935250254280e-01) (0, 4.48686951108013765133e-01) (1, -6.38291639789156650675e-02) (2, -8.02508396609835361613e-02) (3, -2.49500525697283273563e-02) (4, 7.09604742423482204305e-02) (5, 5.97536636268108489745e-01) (6, 3.41279011080277971946e-01) (7, 4.14029074625287873967e-01) (8, 3.21810162980086023055e-01) (9, 3.25124840727863295697e-01) (10, -5.93218203206914429515e-02) (0, -1.71533644503910265300e+00) (1, -2.30371040682966418678e-02) (2, -1.01859750358360479083e-01) (3, 2.18658166546647735051e-02) (4, -1.19664257524507711139e-01) (5, 4.24058953706308028320e-01) (6, 8.66824834066665245169e-02) (7, 1.14938340307731867718e+00) (8, -6.23797816732979404364e-03) (9, -1.51870883303119624808e-01) (10, -2.67343584176150894649e-01) (0, 1.93740530306541067773e+01) (1, 2.77894507234004328744e-01) (2, 2.64983299260047322488e-01) (3, 4.47749334459212611215e-01) (4, 4.55827253942397425668e-01) (5, -3.85124049589211114863e+00) (6, 1.67928546872126882583e-01) (7, -6.92899657736533169938e-01) (8, -1.49299902740740564333e+00) (9, 3.88688743155611138302e-01) (10, 6.10683872293443563706e-01) (0, 8.65350036596030491864e-01) (1, 6.91752738216032669882e-02) (2, 5.23847794988264656157e-02) (3, 8.96694964625944224146e-02) (4, -7.61752271196732833047e-02) (5, 3.50491344536550064603e-01) (6, 1.53110969173213984362e-01) (7, 1.22959232301969767498e+00) (8, -1.96141795682508668364e-01) (9, -1.59879150720393437002e-01) (10, -1.09243928394002529081e-01) (0, -1.74433309889394960734e+00) (1, -4.44866550545866407118e-02) (2, -2.83151549678022605716e-02) (3, -1.02374835072057912555e-01) (4, -5.10913041811163343153e-02) (5, 2.77734175724465681689e-01) (6, 1.99453595387241444659e-01) (7, 1.17276855529804469036e+00) (8, 1.28818472795690586175e-01) (9, -1.88078261213733638479e-01) (10, -2.97015737470237350948e-01) (0, -1.67353184080679140422e+00) (1, -1.19856280665571451188e-02) (2, -4.86763325910742200575e-02) (3, -9.17421487550909020570e-02) (4, -8.92695611526320154105e-03) (5, 3.15773906592412612060e-01) (6, 4.71655128914153926045e-02) (7, 1.52282155426833853973e+00) (8, -1.19090242730603521060e-01) (9, -2.90844272124006208990e-01) (10, -2.25870981943295828787e-01) (0, -9.59246786103535309387e-01) (1, -1.50615655366364481482e-01) (2, -1.57570839408818247351e-01) (3, -8.61935508586797738406e-02) (4, -1.27278056476298334632e-01) (5, 5.68228597111000044428e-01) (6, 2.35633527265949327401e+00) (7, -8.40118475391660801677e+00) (8, 6.39694903774861378665e-01) (9, 8.76438891125441266183e-01) (10, 4.85804133836449381079e-01) (0, -4.52883992225179055069e-01) (1, -5.67293370017983231146e-02) (2, -1.09654685283658653860e-01) (3, -1.06935797060487419730e-01) (4, 1.29508992781661204374e-02) (5, 2.81609678068691415831e-01) (6, 2.67538659705380421627e+00) (7, -9.15217443630131555210e+00) (8, 5.87562151982191971378e-01) (9, 1.14465823670665178824e+00) (10, 4.17779137168349024023e-01) (11, 1.05212363721540524963e+00) (12, -4.21403073689028545568e-03) (13, 2.76466887645188841294e-01) (14, 4.21980040388163002429e-01) (15, -6.80841769260629336991e-01) (16, 5.59754549757176256275e-02) (17, 4.85099553667771854482e-01) (18, 4.73970790044775713845e-01) (19, -2.62397262600824077161e-01) (20, -1.92649372852742140072e-01) (21, 4.53899824246485172896e-01) 
