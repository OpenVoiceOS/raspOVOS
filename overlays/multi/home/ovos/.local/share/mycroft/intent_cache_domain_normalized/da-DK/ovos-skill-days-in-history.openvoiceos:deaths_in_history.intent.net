FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 9.15747113697456538439e-01) (1, 6.80473870354688536999e-01) (2, 6.56196432876384627697e-01) (3, 7.90924052464759719250e-01) (4, 7.86443988549507033703e-01) (5, 7.52362924824573342519e-01) (6, 7.09607461092203739561e-01) (7, 1.81496592791071353901e+00) (8, 8.03114389356843760837e-01) (9, -4.32115755732685169477e+00) (10, 7.99272872415460522433e-01) (0, -9.67968220163582859783e-01) (1, -1.14366454882794421621e+00) (2, -1.02684305114918772439e+00) (3, -1.13922593249255221792e+00) (4, -1.16151418788605753640e+00) (5, 1.89033982892126650732e+00) (6, 2.52511224972194403549e+00) (7, -1.15057939666318476135e+00) (8, -3.39089379681128866295e+00) (9, -4.48839534073244237788e+02) (10, -1.19236648957476813748e-02) (0, -9.30123241195436600837e-02) (1, -1.29653702104868706613e-01) (2, -1.42048748756232079415e-01) (3, -2.21277269149603633691e-01) (4, -1.28229411865057763009e-01) (5, 9.10015378461776158758e-01) (6, 1.53070262308354054781e+00) (7, -8.11261347372610797102e-01) (8, -5.07187652669728539223e-01) (9, -6.85709886413465824262e+02) (10, 3.43249981461055397869e-01) (0, 5.88654111189346629374e-01) (1, 7.40426350592630844183e-01) (2, 7.18056652902620773382e-01) (3, 6.72420773862856369085e-01) (4, 6.88037414311426842595e-01) (5, 1.08842063711812597759e+00) (6, 6.81644559480916734096e-01) (7, 1.90167210436004197049e+00) (8, 7.36334703828118053437e-01) (9, -4.85146097467485404309e+00) (10, 1.21648182082496525425e+00) (0, 4.35787395184085366018e-02) (1, 7.42845428004379249032e-01) (2, 6.80391331031436896737e-01) (3, 7.90385116651172614510e-01) (4, 7.80519095077629065926e-01) (5, 1.01021067955896337587e+00) (6, 6.53819740392934667383e-01) (7, 2.03885877204775534111e+00) (8, 7.97813552867958075687e-01) (9, -5.24097851450028340281e+00) (10, 1.14193754846832340455e+00) (0, -1.41480206097101740559e-01) (1, -2.10283360034067695610e-01) (2, -1.57881731688816723258e-01) (3, -8.46374207733466138448e-02) (4, -8.90131914614035596456e-02) (5, 8.99043944047389409491e-01) (6, 1.41726558144325887056e+00) (7, -7.70431830425341357405e-01) (8, -5.40604993037761105690e-01) (9, -6.85729725797127571241e+02) (10, -1.72660111511819258645e-02) (0, -1.22935375723996087594e+00) (1, -6.28996940520013314568e-01) (2, -5.54272340800965546137e-01) (3, -6.62933426168168526971e-01) (4, -6.53772348728383523309e-01) (5, 1.84572361968996245807e+00) (6, 7.31065299573140414680e+00) (7, -1.05283055823048976585e+00) (8, -4.05722531257222268630e-02) (9, 2.04087831771862315833e+00) (10, -1.85292954887283478627e-01) (0, 2.68714626039253401402e+00) (1, 6.31577281566378045952e-01) (2, 6.63106033863541055595e-01) (3, 7.75012227060314695670e-01) (4, 7.78977038623806405937e-01) (5, 1.47117352580258681227e-01) (6, -3.29668928543757022709e+00) (7, 4.23068238447974120930e+00) (8, 4.95523143845519375983e-01) (9, 4.46381359972359234689e+02) (10, 3.10913892867453300894e-01) (0, -6.51634909539546169910e-01) (1, -3.22631183795233020262e-02) (2, -1.42606127612997873877e-01) (3, 7.28425032685274835217e-04) (4, -2.44300100735922141870e-02) (5, -5.36099428249577972250e-02) (6, 6.88888503805691016169e+02) (7, -8.08464363461228074925e-01) (8, -4.44771380844715458203e-01) (9, -3.29017529187636881272e-01) (10, -2.89165266696814748926e-01) (0, 2.56051698977934050205e+00) (1, 6.88606793345924783623e-01) (2, 7.82885598303791452324e-01) (3, 7.33302446367260496451e-01) (4, 6.12933660211082753655e-01) (5, -1.55397832507782218681e-01) (6, -3.35233007977436603397e+00) (7, 1.03320114750756464339e+00) (8, 4.68847644867464441454e-01) (9, 2.15531649815693754135e+02) (10, 1.77945045921800520938e-01) (11, -2.31023636885552052789e-01) (12, -2.17169440801724483725e-01) (13, -2.49471825236729172204e-01) (14, -2.63639790244965366295e-01) (15, -2.41172683664230974587e-01) (16, -2.24395108098438766930e-01) (17, 7.13160829863807799178e-01) (18, 4.90959519554771572913e-01) (19, 6.96078223324124278903e-01) (20, 5.05594672669567257728e-01) (21, 3.56808270058831145111e-01) 
