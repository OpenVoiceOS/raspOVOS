FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.05666033477438903176e+00) (1, 4.73168584267595837556e-01) (2, 9.69897564863277605873e-01) (3, 7.11594087863311752784e+00) (4, -1.33485315710927565647e+00) (5, -2.95063185070722688863e+00) (6, -4.12952796178695236140e-01) (7, 7.32051883897005817303e-01) (8, 1.30452149487849089482e+00) (9, 1.04495175620895608404e+00) (10, -2.47734004008659258744e+00) (0, -2.44325145859011927385e+00) (1, -3.26642654544852506504e+00) (2, -1.13909163949957692452e+00) (3, -2.25259111432381287088e+00) (4, -3.23364269118068925479e+00) (5, -2.93977560880429056311e+00) (6, 4.12151422439375636486e+00) (7, -1.58838118335565647854e+00) (8, -2.98686071928120711050e+00) (9, -2.94846524741030879824e+00) (10, 4.52326095598813093801e+00) (0, -1.78237192363356028446e-01) (1, -7.05828767622609265509e-01) (2, -3.42353323428672751128e-01) (3, -1.44032100281446262535e+00) (4, 3.97950188179572894587e-01) (5, -3.58830952789533697000e+00) (6, 4.40210607999204195373e-01) (7, -1.73703917387393569349e+00) (8, -1.00765518509118190416e+00) (9, -3.95530827250648808757e-01) (10, 1.87135143524543523519e+00) (11, 4.67947772875726919484e-01) (12, -2.15198390969912196624e+02) (13, -7.14187925181976357436e+00) (14, -2.29654777363482098806e+00) 
