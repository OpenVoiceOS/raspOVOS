FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -9.90489809123991232775e-02) (1, 9.79625784446557651419e-02) (2, 2.06161318032916429654e-01) (3, 1.06487651972945587819e-01) (4, 5.11888206590971200916e-02) (5, 1.23320699708035341757e+01) (6, 2.09450422243747408357e-01) (7, -8.69356346737905349986e-02) (8, 1.68390629499133470670e-01) (9, 5.31559965341890539037e-01) (10, 1.19974165424026210647e-01) (0, -5.02158425010081066464e-02) (1, 1.13074918894942672010e-01) (2, 1.11996201484378249402e-01) (3, 1.11963433830912978406e-01) (4, 3.65968846370538919421e-02) (5, 1.22987768725922457236e+01) (6, 2.86094656603011754381e-01) (7, -4.64330417525335367723e-02) (8, 2.47329898764452049686e-02) (9, 6.07016077965105260716e-01) (10, 2.22067139311946604163e-01) (0, 1.07600657331246529758e+00) (1, 3.55868537154138742107e-01) (2, 3.31526734110058007854e-01) (3, 3.35767522629440484661e-01) (4, 4.05750900682151971477e-01) (5, -2.37945345388612405912e+00) (6, 1.46922803525450174078e-01) (7, 3.69060228404983481365e-01) (8, 3.31887034871835795435e-01) (9, -2.55026065902671472685e+00) (10, 4.58857948669291970667e-01) (0, -1.44886588147115524805e-01) (1, 1.09914963751014144178e-01) (2, 1.37186703889544847623e-01) (3, 1.97380287258799913541e-01) (4, 1.47544570853075096561e-02) (5, 1.21504204423597208518e+01) (6, 2.52643322961005833971e-01) (7, -8.45309225332303848521e-02) (8, 9.20638271262010782214e-02) (9, 5.44661528199518407689e-01) (10, 2.22805849476970407874e-01) (0, 5.54921555678747971396e-02) (1, 1.97133732645686510221e-01) (2, 1.87564768483003685429e-02) (3, 1.47413877456363084928e-01) (4, 1.78617445438083055631e-01) (5, 1.22724928186705461997e+01) (6, 2.59019276516112950670e-01) (7, -4.52118245374723490282e-02) (8, 1.16229166894133956189e-01) (9, 5.19517476439083303319e-01) (10, 1.99567428990520212562e-01) (0, 7.98839145609243073132e-01) (1, 1.81598387522041382347e-01) (2, 1.50476682794630112205e-01) (3, 1.98925778133213104759e-01) (4, 1.36193260413944305931e-01) (5, -1.13558468851859384330e+00) (6, -1.38128943746461285702e-01) (7, 1.07894853803385828983e-01) (8, -4.54495260781661358784e-01) (9, -7.35771547983130158599e-01) (10, 2.40666292471229614769e-01) (0, 1.03206021400589142978e+00) (1, 3.98052325349224522810e-01) (2, 4.48469413321388565841e-01) (3, 3.67481132846249058943e-01) (4, 3.46084715675486043196e-01) (5, -3.34417796911427434026e+00) (6, 1.86800312251674704989e-01) (7, -1.03918628397315582212e-01) (8, 3.03358308374503060634e-01) (9, -6.06024012332810224635e-01) (10, 3.26276593326424668717e-01) (0, 9.86915898196879970428e-01) (1, 3.75541834037674382429e-01) (2, 3.89267673592937901716e-01) (3, 4.44409189622772538009e-01) (4, 3.29950181644333317976e-01) (5, -3.36406060339638646184e+00) (6, 8.78937169917698712940e-02) (7, -1.45197214188903411802e-01) (8, 3.35674608883842484630e-01) (9, -5.66585898762596906764e-01) (10, 3.78306903361176560807e-01) (0, 6.88143515297277352794e-01) (1, 1.81359093049120873031e-01) (2, 1.13741035917353600082e-01) (3, 1.69292754837822884140e-01) (4, 2.85962636926722579833e-01) (5, -1.13757023844535165580e+00) (6, -4.80515584401028789108e-02) (7, 1.53894365879763683758e-01) (8, -4.01837418145552960347e-01) (9, -7.87017453442057313140e-01) (10, 2.81729496878921625846e-01) (0, -5.48153392232294811581e-02) (1, 1.30759937374766710416e-01) (2, 3.47155236293634622546e-02) (3, 1.65888098685916307584e-01) (4, 7.94927083541712015125e-02) (5, 1.23017121182253710288e+01) (6, 2.68269768373641637194e-01) (7, -1.56807119907097419764e-01) (8, 8.16041581918558328601e-02) (9, 6.39677307337129796849e-01) (10, 1.19922905429519374709e-01) (11, 4.38097553666038641573e-01) (12, 4.00690237398548254610e-01) (13, -3.20069885031068090964e-01) (14, 4.42695336754722723605e-01) (15, 4.50516404922409186007e-01) (16, -2.51535912410276174533e-01) (17, -2.99177399054848858206e-01) (18, -2.80382862464272741843e-01) (19, -2.81929036036985158908e-01) (20, 4.20184882696075567843e-01) (21, 2.60563932033492928753e-01) 
