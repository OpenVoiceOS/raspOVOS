FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.98580965523089014724e-01) (1, 6.95747220140759386336e-03) (2, 1.23313673697120346207e-02) (3, -1.53191319767414901865e-01) (4, -7.40338183082931422518e-02) (5, 7.39583053137604622407e+01) (6, -1.12686320590657140794e-01) (7, -5.59210257423768064555e-01) (8, 3.00605660238616456681e-01) (9, -1.89514127209704091870e-01) (10, -3.88281216758829816893e-01) (0, -1.16146819713467117086e-01) (1, 1.81002797803936271714e-01) (2, 1.78371051571426658677e-01) (3, 2.09843332192955173143e-01) (4, 1.19874717912731604263e-01) (5, 1.50955290518856943827e+00) (6, 8.08960323322260865808e-01) (7, -3.63745868177860698367e-01) (8, 7.40259296423471879756e-01) (9, 1.15718276830651209397e-01) (10, 6.43698645282127168699e-01) (0, -3.35828965711373161884e-01) (1, 3.94205743901457739131e-01) (2, 2.21081885032381963985e-01) (3, 1.97969699732985393581e-01) (4, 2.09245669178690807399e-01) (5, -3.19934652164663235752e+00) (6, 3.66487285551141295237e-01) (7, 2.77462824867755353253e+00) (8, 6.06189483049413002647e-01) (9, 7.48966806310403399038e+01) (10, 2.36610353609241430561e-01) (0, -2.34976330819481354917e-01) (1, 1.25616261980591253922e-01) (2, 2.16963457188663638719e-01) (3, 2.35779488704261908083e-01) (4, 2.94393094501552765596e-01) (5, 1.44080533370158025974e+00) (6, 7.40824517894232759119e-01) (7, -3.30865465076416453982e-01) (8, 6.94932709610737275874e-01) (9, 1.64814429017044961068e-01) (10, 6.69792180407382753415e-01) (0, -4.29802897336413691765e-01) (1, 2.93667698419846467939e-01) (2, 3.23494115985192232099e-01) (3, 2.74402314937866986799e-01) (4, 3.76570547140397005048e-01) (5, -3.06730178311492807097e+00) (6, 3.15126395281861815256e-01) (7, 3.06845274163859649263e+00) (8, 5.59173160436650440452e-01) (9, 8.48045024116807333314e+00) (10, 3.55043069144405532001e-01) (0, 8.16386641178757227477e-02) (1, 3.38719466590549855844e-01) (2, 3.52201858424808889048e-01) (3, 4.08111313247349172251e-01) (4, 3.27916534506943135874e-01) (5, -3.04009436383869058318e+00) (6, 4.20243367674470125195e-01) (7, 3.05105179144519134127e+00) (8, 8.31417774316465796325e-01) (9, 7.74612543907698736234e+00) (10, 3.80372879904350369440e-01) (0, -4.43139150648086899142e-01) (1, 1.83311960313366634212e-01) (2, 2.21062800500439499141e-01) (3, 2.65064625654267083110e-01) (4, 1.98079003605889009121e-01) (5, -2.82321161747604154613e+00) (6, 2.74653313232550377876e-01) (7, 2.36682263372146550751e+00) (8, 4.93529534543619211040e-02) (9, 8.40215146010094038331e+00) (10, 3.68438026353877567942e-01) (0, -1.50222944573364269871e-01) (1, 4.86204071836146334284e-01) (2, 5.18585844831141451472e-01) (3, 5.07434978084238985652e-01) (4, 4.24040256933841064324e-01) (5, 1.45665415141883713979e+00) (6, 4.47002669972611454963e-01) (7, -4.93141295749506491930e-01) (8, 6.12077440252396187681e-01) (9, 4.20719538310033724926e-01) (10, 1.11713352275102395517e+00) (0, 2.48726119041938614762e-01) (1, -3.36728650163641674009e-01) (2, -3.47711044382086442717e-01) (3, -2.53387893568506994413e-01) (4, -2.57917064260474004911e-01) (5, 4.02785601190680164763e+00) (6, 2.29291008477339380844e-01) (7, 1.44704927147022988088e-01) (8, -7.66894050156621043257e-01) (9, 1.16289230079169980669e+00) (10, -4.95819774173757388258e-01) (0, 2.47004885819146230430e-01) (1, -3.42277716305511181538e-01) (2, -4.64232485201613864767e-01) (3, -3.52010052111404070363e-01) (4, -3.77598855104701647267e-01) (5, 6.54671580013045151247e+00) (6, 5.04712681716388056330e-01) (7, -6.24555065074610671338e-01) (8, -7.61145090384060640787e-01) (9, 3.30431588173930090857e-01) (10, -6.59243954744682669755e-01) (11, 5.46017422435158383820e-01) (12, -2.58090417619383671877e-01) (13, 4.15450967696747996971e-01) (14, -2.93634082909262517092e-01) (15, 4.10003545776353117702e-01) (16, 3.98647697011456747518e-01) (17, 4.11838782616886722732e-01) (18, -2.37014736767447248278e-01) (19, 5.88154810922600690404e-01) (20, 5.98522135274546895900e-01) (21, 4.86670559771424537931e-01) 
