FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=15 4 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (15, 6, 5.00000000000000000000e-01) (15, 6, 5.00000000000000000000e-01) (15, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (0, 4, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.08143017376663852502e+00) (1, -1.99938393327177132974e+00) (2, 6.49954632380753016818e-01) (3, -1.83875463236820491097e+00) (4, -2.20000533864912295456e+00) (5, -1.01617707959069258905e+00) (6, -1.31985570624163872644e+00) (7, 2.32806184390311132759e+00) (8, -3.07144674632979395312e+01) (9, 2.10909746741627657229e+00) (10, 1.72868838507166877605e+00) (11, -3.06857809994650843066e+01) (12, -1.85369144893231707627e+00) (13, -1.97656704427806295676e+00) (14, 1.99922984774115275464e+00) (0, -2.56834547550393788029e+00) (1, 2.70575575417636349229e+00) (2, 1.01848821299966724174e-01) (3, 1.21051185057008203749e+00) (4, 7.94008652977058471123e-01) (5, 7.45741384589629019430e-01) (6, 1.26586860737376105668e+00) (7, -1.02556510095524111570e+00) (8, 5.44651774529153499316e+02) (9, 4.71247254823235905974e+00) (10, 3.78235081830357078303e+00) (11, 5.44672318558925780962e+02) (12, 2.34831366851643030813e+00) (13, 2.32816641287304326724e+00) (14, -2.75068193509295344512e+00) (0, -5.35012762355162041494e-02) (1, 6.30211263472852722956e-01) (2, 8.26974102574744990291e-02) (3, 1.30207478114035857253e+00) (4, 2.32679573116792326459e-01) (5, 5.37315332723459104791e-01) (6, 6.09809073431873516213e-01) (7, 1.90569061775618005250e-02) (8, 5.31483390897652370199e+01) (9, 1.22152637460425794735e+00) (10, 1.96490473068748916496e+00) (11, 5.31448177218577129111e+01) (12, 1.45978545296158856992e+00) (13, 8.01001089041014191494e-01) (14, -2.92834869340457881748e-01) (15, -6.23464563638230551135e+00) (16, 1.10513246835803879975e+01) (17, 4.62952539769387383384e-01) (18, -1.21336912231194715694e+00) 
