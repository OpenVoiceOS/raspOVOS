FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.29648532123272097749e+00) (1, 4.37615058740374984758e-01) (2, 3.14406953176257553118e-01) (3, 4.80175323208568038957e-01) (4, 4.17767382582423629778e-01) (5, 5.09160214950265554990e+00) (6, 4.67331066028631170184e+01) (7, 9.74455922950975717001e-01) (8, 4.37067558327642491367e-01) (9, 2.30454629055439452756e-01) (10, 9.52402080353647706445e-01) (0, -6.06200605578854689881e-01) (1, -4.07842383458117407891e-02) (2, -4.87147406413058203789e-02) (3, -7.25631103589037679269e-02) (4, -6.84667885972956441476e-02) (5, -1.80328004259938695641e-01) (6, 7.20023210640795774395e-01) (7, 1.25962644116834732166e+00) (8, -1.08436614353180083015e-02) (9, 1.13958462095282903048e-01) (10, -4.64368493453306718788e-01) (0, 2.97111724701580826036e+00) (1, 3.71780618449447097795e-01) (2, 4.53494823893306198137e-01) (3, 4.57424424012896957414e-01) (4, 4.18313882669207992571e-01) (5, 5.19788836975516943539e+00) (6, 4.68291736812746961505e+01) (7, 1.02586843829321106014e+00) (8, 3.56925529697862675693e-01) (9, 1.68985990575729772001e-01) (10, 9.52368999775797364649e-01) (0, 2.07975830520414836644e-02) (1, -1.01648179697829382784e-01) (2, 8.23258032323540001052e-02) (3, 5.34232129575431813340e-02) (4, -2.98873538850127906663e-02) (5, -1.09917619161813959749e+01) (6, 3.85992560908228421468e+00) (7, 2.20057662132562104063e+00) (8, 2.99006177514538279763e-03) (9, 8.56449458717277317676e-02) (10, -1.03177472732896707841e-03) (0, -2.87569608281228683566e-01) (1, -9.57299078582106999225e-02) (2, 8.04477204801262169021e-02) (3, -6.65840933798133444155e-02) (4, -9.00392736075698307818e-02) (5, -1.13167685263929236328e+01) (6, 4.18868967032263128658e+00) (7, 2.50375675895811022897e+00) (8, 9.12178596976971900467e-02) (9, 8.82696811078956117269e-02) (10, -9.21241705077858741291e-02) (0, -3.87658884636674727631e+01) (1, -2.76382685882641032649e-01) (2, -2.08981217128826363139e-01) (3, -2.68780672235084783228e-01) (4, -3.08554780913187276514e-01) (5, -1.14367587752743027352e+00) (6, 7.59153785128681324323e+00) (7, 1.18102635098148311954e+00) (8, 1.60132166313109280953e-01) (9, 5.12316592916355562082e-01) (10, -2.32816095855439320061e+00) (0, -8.87040127652086174415e-01) (1, -2.47771831236500400530e-01) (2, -1.76492111764568804233e-01) (3, -1.53971316300053145953e-01) (4, -3.02712673328537684725e-01) (5, 1.17365878281666446981e+00) (6, 9.50025232273835729124e-01) (7, 1.51945536562310135054e+00) (8, -1.03721321059403376319e-01) (9, 5.42762735199834978062e-01) (10, -8.98808581013499585666e-01) (0, 2.29267420378867337050e+00) (1, 4.93830451691792571278e-01) (2, 4.89854926193402373524e-01) (3, 3.29430306280301177235e-01) (4, 4.60493648374722563954e-01) (5, 5.17539394189538626279e+00) (6, 2.14764258273810391131e+01) (7, 4.89004952827072170152e+00) (8, 4.05859238043464243262e-01) (9, 7.18911717398008187629e-01) (10, 8.98420187225216992921e-01) (0, 2.59882622272375485295e+00) (1, 5.75322972710413549358e-01) (2, 5.30326543475670542094e-01) (3, 6.91572724010033113018e-01) (4, 5.70662526513619039470e-01) (5, -4.58801289778916931983e-01) (6, -4.95255391762155205981e+00) (7, -1.20353379124832038372e+00) (8, 7.50239194111301466172e-01) (9, -4.08055811543428958998e-01) (10, 1.05492999727671743493e+00) (0, 3.57724258424313878990e+00) (1, 7.42716954573030552211e-01) (2, 6.25503511651392174464e-01) (3, 5.64494737907285992762e-01) (4, 6.40100517793531498256e-01) (5, 4.35705100574972636895e-01) (6, -4.92427422963441152604e+00) (7, -7.58140157647185924938e-02) (8, 8.69269477114439914622e-01) (9, -1.10245032184777880246e+00) (10, 6.09975084796597788461e-01) (11, 4.52252561404267339196e-01) (12, 4.71380797558628383825e-01) (13, 4.53734228565255193200e-01) (14, -4.15244081693934397048e-01) (15, -3.59738528708348603846e-01) (16, 5.95124676368169458485e-01) (17, 5.81577702660032502813e-01) (18, 4.77449085798641537082e-01) (19, -2.07564087754487314186e-01) (20, -2.66551360188176489352e-01) (21, 3.26079348042511374395e-01) 
